--- d:\nupic\src\python\python27\examples\bindings\sparse_matrix_how_to.py	(original)
+++ d:\nupic\src\python\python27\examples\bindings\sparse_matrix_how_to.py	(refactored)
@@ -53,7 +53,7 @@
 # With NTA_DOUBLE_PRECISION or NTA_QUAD_PRECISION set at compile time, NuPIC can 
 # use 32 bits to represent floating point values. The global epsilon can 
 # then be set to smaller values via the variable nupic::Epsilon in nupic/math/math.hpp
-print '\nGlobal epsilon :', getGlobalEpsilon()
+print('\nGlobal epsilon :', getGlobalEpsilon())
 
 
 # 3. Creation of sparse matrices:
@@ -63,37 +63,37 @@
 # You can create a SparseMatrix by passing it a 2D array:
 
 s = SparseMatrix([[1,2],[3,4]], dtype='Float32')
-print '\nFrom array 32\n', s
+print('\nFrom array 32\n', s)
 
 # ... or by passing it a numpy.array:
 
 s = SparseMatrix(numpy.array([[1,2],[3,4]]),dtype='Float32')
-print '\nFrom numpy array 32\n', s
+print('\nFrom numpy array 32\n', s)
 
 # ... or by using one of the shortcuts: SM32, SM32:
 
 s = SM32([[1,2],[3,4]])
-print '\nWith shortcut 32\n', s
+print('\nWith shortcut 32\n', s)
 
 # It is also possible to create an empty SparseMatrix, or a copy of another
 # SparseMatrix, or a SparseMatrix from a string in CSR format:
 
 s_empty = SM32()
-print '\nEmpty sparse matrix\n', s_empty
+print('\nEmpty sparse matrix\n', s_empty)
 
 s_string = SM32('sm_csr_1.5 26 2 2 4 2 0 1 1 2 2 0 3 1 4')
-print '\nSparse matrix from string\n', s_string
+print('\nSparse matrix from string\n', s_string)
 
 # A sparse matrix can be converted to a dense one via toDense:
 
 a = numpy.array(s_string.toDense())
-print '\ntoDense\n', a
+print('\ntoDense\n', a)
 
 # To set a sparse matrix from a dense one, one can use fromDense:
 
 s = SM32()
 s.fromDense(numpy.random.random((4,4)))
-print '\nfromDense\n', s
+print('\nfromDense\n', s)
 
 # A sparse matrix can be serialized:
 schema = SM32.getSchema()
@@ -110,7 +110,7 @@
   s2 = SM32()
   s2.read(proto2)
 
-print '\nSerializing\n', s2
+print('\nSerializing\n', s2)
 
 # 4. Simple queries:
 # =================
@@ -125,39 +125,39 @@
 s = SM32(numpy.random.random((4,4)))
 s.threshold(.5)
 
-print '\nPrint\n', s
-print '\nNumber of rows ', s.nRows()
-print 'Number of columns ', s.nCols()
-print 'Is matrix zero? ', s.isZero()
-print 'Total number of non zeros ', s.nNonZeros()
-print 'Sum of all values ', s.sum()
-print 'Prod of non-zeros ', s.prod()
-print 'Maximum value and its location ', s.max()
-print 'Minimum value and its location ', s.min()
-
-print 'Number of non-zeros on row 0 ', s.nNonZerosOnRow(0)
-print 'If first row zero? ', s.isRowZero(0)
-print 'Number of non-zeros on each row ', s.nNonZerosPerRow() 
-print 'Minimum on row 0 ', s.rowMin(0)
-print 'Minimum values and locations for all rows', s.rowMin() 
-print 'Maximum on row 0 ', s.rowMax(0)
-print 'Maximum values and locations for all rows', s.rowMax() 
-print 'Sum of values on row 0 ', s.rowSum(0)
-print 'Sum of each row ', s.rowSums()
-print 'Product of non-zeros on row 1', s.rowProd(1)
-print 'Product of each row ', s.rowProds()
-
-print 'Number of non-zeros on col 0 ', s.nNonZerosOnCol(0)
-print 'If first col zero? ', s.isColZero(0)
-print 'Number of non-zeros on each col ', s.nNonZerosPerCol() 
-print 'Minimum on col 0 ', s.colMin(0)
-print 'Minimum values and locations for all cols', s.colMin() 
-print 'Maximum on col 0 ', s.colMax(0)
-print 'Maximum values and locations for all cols', s.colMax() 
-print 'Sum of values on col 0 ', s.colSum(0)
-print 'Sum of each col ', s.colSums()
-print 'Product of non-zeros on col 1', s.colProd(1)
-print 'Product of each col ', s.colProds()
+print('\nPrint\n', s)
+print('\nNumber of rows ', s.nRows())
+print('Number of columns ', s.nCols())
+print('Is matrix zero? ', s.isZero())
+print('Total number of non zeros ', s.nNonZeros())
+print('Sum of all values ', s.sum())
+print('Prod of non-zeros ', s.prod())
+print('Maximum value and its location ', s.max())
+print('Minimum value and its location ', s.min())
+
+print('Number of non-zeros on row 0 ', s.nNonZerosOnRow(0))
+print('If first row zero? ', s.isRowZero(0))
+print('Number of non-zeros on each row ', s.nNonZerosPerRow()) 
+print('Minimum on row 0 ', s.rowMin(0))
+print('Minimum values and locations for all rows', s.rowMin()) 
+print('Maximum on row 0 ', s.rowMax(0))
+print('Maximum values and locations for all rows', s.rowMax()) 
+print('Sum of values on row 0 ', s.rowSum(0))
+print('Sum of each row ', s.rowSums())
+print('Product of non-zeros on row 1', s.rowProd(1))
+print('Product of each row ', s.rowProds())
+
+print('Number of non-zeros on col 0 ', s.nNonZerosOnCol(0))
+print('If first col zero? ', s.isColZero(0))
+print('Number of non-zeros on each col ', s.nNonZerosPerCol()) 
+print('Minimum on col 0 ', s.colMin(0))
+print('Minimum values and locations for all cols', s.colMin()) 
+print('Maximum on col 0 ', s.colMax(0))
+print('Maximum values and locations for all cols', s.colMax()) 
+print('Sum of values on col 0 ', s.colSum(0))
+print('Sum of each col ', s.colSums())
+print('Product of non-zeros on col 1', s.colProd(1))
+print('Product of each col ', s.colProds())
 
 
 # 5. Element access and slicing:
@@ -165,41 +165,41 @@
 
 # It is very easy to access individual elements:
 
-print '\n', s
-print '\ns[0,0] = ', s[0,0], 's[1,1] = ', s[1,1]
+print('\n', s)
+print('\ns[0,0] = ', s[0,0], 's[1,1] = ', s[1,1])
 
 s[0,0] = 3.5
-print 'Set [0,0] to 3.5 ', s[0,0]
+print('Set [0,0] to 3.5 ', s[0,0])
 
 # There are powerful slicing operations:
 
-print '\ngetOuter\n', s.getOuter([0,2],[0,2])
+print('\ngetOuter\n', s.getOuter([0,2],[0,2]))
 s.setOuter([0,2],[0,2],[[1,2],[3,4]])
-print '\nsetOuter\n', s 
+print('\nsetOuter\n', s) 
 
 s.setElements([0,1,2],[0,1,2],[1,1,1])
-print '\nsetElements\n', s
-print '\ngetElements\n', s.getElements([0,1,2],[0,1,2])
+print('\nsetElements\n', s)
+print('\ngetElements\n', s.getElements([0,1,2],[0,1,2]))
 
 s2 = s.getSlice(0,2,0,3)
-print '\ngetSlice\n', s2
+print('\ngetSlice\n', s2)
 s.setSlice(1,1, s2)
-print '\nsetSlice\n', s
+print('\nsetSlice\n', s)
 
 # A whole row or col can be set to zero with one call:
 
 s.setRowToZero(1)
-print '\nsetRowToZero\n', s
+print('\nsetRowToZero\n', s)
 
 s.setColToZero(1)
-print '\nsetColToZero\n', s
+print('\nsetColToZero\n', s)
 
 # Individual rows and cols can be retrieved as sparse or dense vectors:
 
-print '\nrowNonZeros ', s.rowNonZeros(0)
-print 'colNonZeros ', s.colNonZeros(0)
-print 'getRow ', s.getRow(0)
-print 'getCol ', s.getCol(0)
+print('\nrowNonZeros ', s.rowNonZeros(0))
+print('colNonZeros ', s.colNonZeros(0))
+print('getRow ', s.getRow(0))
+print('getCol ', s.getCol(0))
 
 
 # 6. Dynamic features:
@@ -208,49 +208,49 @@
 # SparseMatrix is very dynamic. Rows and columns can be added and deleted. 
 # A sparse matrix can also be resized and reshaped.
 
-print '\n', s
+print('\n', s)
 s.reshape(2,8)
-print '\nreshape 2 8\n', s
+print('\nreshape 2 8\n', s)
 s.reshape(8,2)
-print '\nreshape 8 2\n', s
+print('\nreshape 8 2\n', s)
 s.reshape(1,16)
-print '\nreshape 1 16\n', s
+print('\nreshape 1 16\n', s)
 s.reshape(4,4)
-print '\nreshape 4 4\n', s
+print('\nreshape 4 4\n', s)
 
 s.resize(5,5)
-print '\nresize 5 5\n', s
+print('\nresize 5 5\n', s)
 s.resize(3,3)
-print '\nresize 3 3\n', s
+print('\nresize 3 3\n', s)
 s.resize(4,4)
-print '\nresize 4 4\n', s
+print('\nresize 4 4\n', s)
 
 s.deleteRows([3])
-print '\ndelete row 3\n', s
+print('\ndelete row 3\n', s)
 s.deleteCols([1])
-print '\ndelete col 1\n', s
+print('\ndelete col 1\n', s)
 
 s.addRow([1,2,3])
-print '\nadd row 1 2 3\n', s
+print('\nadd row 1 2 3\n', s)
 s.addCol([1,2,3,4])
-print '\nadd col 1 2 3 4\n', s
+print('\nadd col 1 2 3 4\n', s)
 
 s.deleteRows([0,3])
-print '\ndelete rows 0 and 3\n', s
+print('\ndelete rows 0 and 3\n', s)
 s.deleteCols([1,2])
-print '\ndelete cols 1 and 2\n', s
+print('\ndelete cols 1 and 2\n', s)
 
 # It is also possible to threshold a row, column or whole sparse matrix.
 # This operation usually introduces zeros.
 
 s.normalize()
-print '\n', s
+print('\n', s)
 s.thresholdRow(0, .1)
-print '\nthreshold row 0 .1\n', s
+print('\nthreshold row 0 .1\n', s)
 s.thresholdCol(1, .1)
-print '\nthreshold col 1 .1\n', s
+print('\nthreshold col 1 .1\n', s)
 s.threshold(.1)
-print '\nthreshold .1\n', s
+print('\nthreshold .1\n', s)
 
 
 # 7. Element wise operations:
@@ -260,48 +260,48 @@
 # column-oriented and whole matrix element-wise operations.
 
 s = SM32(numpy.random.random((4,4)))
-print '\n', s
+print('\n', s)
 
 s.elementNZInverse()
-print '\nelementNZInverse\n', s
+print('\nelementNZInverse\n', s)
 
 s.elementNZLog()
-print '\nelementNZLog\n', s
+print('\nelementNZLog\n', s)
 
 s = abs(s)
-print '\nabs\n', s
+print('\nabs\n', s)
 
 s.elementSqrt()
-print '\nelementSqrt\n', s
+print('\nelementSqrt\n', s)
 
 s.add(4)
-print '\nadd 4\n', s
+print('\nadd 4\n', s)
 
 s.normalizeRow(1, 10)
-print '\nnormalizeRow 1 10\n', s
-print 'sum row 1 = ', s.rowSum(1)
+print('\nnormalizeRow 1 10\n', s)
+print('sum row 1 = ', s.rowSum(1))
 
 s.normalizeCol(0, 3)
-print '\nnormalizeCol 0 3\n', s
-print 'sum col 0 = ', s.colSum(0)
+print('\nnormalizeCol 0 3\n', s)
+print('sum col 0 = ', s.colSum(0))
 
 s.normalize(5)
-print '\nnormalize to 5\n', s
-print 'sum = ', s.sum()
+print('\nnormalize to 5\n', s)
+print('sum = ', s.sum())
 
 s.normalize()
-print '\nnormalize\n', s
-print 'sum = ', s.sum()
+print('\nnormalize\n', s)
+print('sum = ', s.sum())
 
 s.transpose()
-print '\ntranspose\n', s
+print('\ntranspose\n', s)
 
 s2 = SM32(numpy.random.random((3,4)))
-print '\n', s2
+print('\n', s2)
 s2.transpose()
-print '\ntranspose rectangular\n', s2
+print('\ntranspose rectangular\n', s2)
 s2.transpose()
-print '\ntranspose rectangular again\n', s2
+print('\ntranspose rectangular again\n', s2)
 
 
 # 8. Matrix vector and matrix matrix operations:
@@ -312,35 +312,35 @@
 # of the SparseMatrix.
 
 x = numpy.array([1,2,3,4])
-print '\nx = ', x
-print 'Product on the right:\n', s.rightVecProd(x)
-print 'Product on the left:\n', s.leftVecProd(x)
-print 'Product of x elements corresponding to nz on each row:\n', s.rightVecProdAtNZ(x)
-print 'Product of x elements and nz:\n', s.rowVecProd(x)
-print 'Max of x elements corresponding to nz:\n', s.vecMaxAtNZ(x)
-print 'Max of products of x elements and nz:\n', s.vecMaxProd(x)
-print 'Max of elements of x corresponding to nz:\n', s.vecMaxAtNZ(x)
+print('\nx = ', x)
+print('Product on the right:\n', s.rightVecProd(x))
+print('Product on the left:\n', s.leftVecProd(x))
+print('Product of x elements corresponding to nz on each row:\n', s.rightVecProdAtNZ(x))
+print('Product of x elements and nz:\n', s.rowVecProd(x))
+print('Max of x elements corresponding to nz:\n', s.vecMaxAtNZ(x))
+print('Max of products of x elements and nz:\n', s.vecMaxProd(x))
+print('Max of elements of x corresponding to nz:\n', s.vecMaxAtNZ(x))
 
 # axby computes linear combinations of rows and vectors
 
 s.axby(0, 1.5, 1.5, x)
-print '\naxby 0 1.5 1.5\n', s
+print('\naxby 0 1.5 1.5\n', s)
 s.axby(1.5, 1.5, x)
-print '\naxby 1.5 1.5\n', s
+print('\naxby 1.5 1.5\n', s)
 
 # The multiplication operator can be used both for inner and outer product,
 # depending on the shape of its operands, when using SparseMatrix instances:
 
 s_row = SM32([[1,2,3,4]])
 s_col = SM32([[1],[2],[3],[4]])
-print '\nInner product: ', s_row * s_col
-print '\nOuter product:\n', s_col * s_row
+print('\nInner product: ', s_row * s_col)
+print('\nOuter product:\n', s_col * s_row)
 
 # SparseMatrix supports matrix matrix multiplication:
 s1 = SM32(numpy.random.random((4,4)))
 s2 = SM32(numpy.random.random((4,4)))
 
-print '\nmatrix matrix multiplication\n', s1 * s2
+print('\nmatrix matrix multiplication\n', s1 * s2)
 
 # The block matrix vector multiplication treats the matrix as if it were
 # a collection of narrower matrices. The following multiplies a1 by x and then a2 by x,
@@ -350,15 +350,15 @@
 
 a = SM32([[1,2,3,4],[5,6,7,8],[9,10,11,12],[13,14,15,16]])
 x = [1,2,3,4]
-print a.blockRightVecProd(2, x)
+print(a.blockRightVecProd(2, x))
 
 # To do an element multiplication of two matrices, do:
 
-print a
+print(a)
 b = SM32(numpy.random.randint(0,2,(4,4)))
-print b
+print(b)
 a.elementNZMultiply(b)
-print a
+print(a)
 
 # In general, the "element..." operations implement element by element operations.
 
@@ -368,30 +368,30 @@
 
 # It is possible to use all 4 arithmetic operators, with scalars or matrices:
 
-print '\ns + 3\n', s + 3
-print '\n3 + s\n', 3 + s
-print '\ns - 1\n', s - 1
-print '\n1 - s\n', 1 - s
-print '\ns + s\n', s + s
-print '\ns * 3\n', s * 3
-print '\n3 * s\n', 3 * s
-print '\ns * s\n', s * s
-print '\ns / 3.1\n', s / 3.1
+print('\ns + 3\n', s + 3)
+print('\n3 + s\n', 3 + s)
+print('\ns - 1\n', s - 1)
+print('\n1 - s\n', 1 - s)
+print('\ns + s\n', s + s)
+print('\ns * 3\n', s * 3)
+print('\n3 * s\n', 3 * s)
+print('\ns * s\n', s * s)
+print('\ns / 3.1\n', s / 3.1)
 
 # ... and to write arbitrarily linear combinations of sparse matrices:
 
-print '\ns1 + 2 * s - s2 / 3.1\n', s1 + 2 * s - s2 / 3.1
+print('\ns1 + 2 * s - s2 / 3.1\n', s1 + 2 * s - s2 / 3.1)
 
 # In place operators are supported:
 
 s += 3.5
-print '\n+= 3.5\n', s
+print('\n+= 3.5\n', s)
 s -= 3.2
-print '\n-= 3.2\n', s
+print('\n-= 3.2\n', s)
 s *= 3.1
-print '\n*= 3.1\n', s
+print('\n*= 3.1\n', s)
 s /= -1.5
-print '\n/= -1.5\n', s
+print('\n/= -1.5\n', s)
 
 
 # 10. Count/find:
@@ -405,14 +405,14 @@
 
 s = SM32(numpy.random.randint(0,3,(5,5)))
 
-print '\nThe matrix is now:\n', s
-print '\nNumber of elements equal to 0=', s.countWhereEqual(0,5,0,5,0)
-print 'Number of elements equal to 1=', s.countWhereEqual(0,5,0,5,1)
-print 'Number of elements equal to 2=', s.countWhereEqual(0,5,0,5,2)
-print '\nIndices of the elements == 0:', s.whereEqual(0,5,0,5,0)
-print '\nIndices of the elements == 1:', s.whereEqual(0,5,0,5,1)
-print '\nIndices of the elements == 2:', s.whereEqual(0,5,0,5,2)
+print('\nThe matrix is now:\n', s)
+print('\nNumber of elements equal to 0=', s.countWhereEqual(0,5,0,5,0))
+print('Number of elements equal to 1=', s.countWhereEqual(0,5,0,5,1))
+print('Number of elements equal to 2=', s.countWhereEqual(0,5,0,5,2))
+print('\nIndices of the elements == 0:', s.whereEqual(0,5,0,5,0))
+print('\nIndices of the elements == 1:', s.whereEqual(0,5,0,5,1))
+print('\nIndices of the elements == 2:', s.whereEqual(0,5,0,5,2))
 
 
 # ... and there is even more:
-print '\nAll ' + str(len(dir(s))) + ' methods:\n', dir(s)
+print('\nAll ' + str(len(dir(s))) + ' methods:\n', dir(s))
--- d:\nupic\src\python\python27\examples\bindings\svm_how_to.py	(original)
+++ d:\nupic\src\python\python27\examples\bindings\svm_how_to.py	(refactored)
@@ -32,7 +32,7 @@
 
 def simple():
     
-    print "Simple"
+    print("Simple")
     numpy.random.seed(42)
     n_dims = 2
     n_class = 4
@@ -41,7 +41,7 @@
     samples = numpy.zeros((size, n_dims), dtype=type)
     do_plot = False
 
-    print "Generating data"
+    print("Generating data")
     centers = numpy.array([[0,0],[0,1],[1,0],[1,1]])
     for i in range(0, size):
         t = 6.28 * numpy.random.random_sample()
@@ -50,19 +50,19 @@
 
     classifier = algo.svm_dense(0, n_dims, probability=True, seed=42)
     
-    print "Adding sample vectors"
+    print("Adding sample vectors")
     for y, x_list in zip(labels, samples):
         x = numpy.array(x_list, dtype=type)
         classifier.add_sample(float(y), x)
 
-    print "Displaying problem"
+    print("Displaying problem")
     problem = classifier.get_problem()
-    print "Problem size:", problem.size()
-    print "Problem dimensionality:", problem.n_dims()
-    print "Problem samples:"
+    print("Problem size:", problem.size())
+    print("Problem dimensionality:", problem.n_dims())
+    print("Problem samples:")
     s = numpy.zeros((problem.size(), problem.n_dims()+1), dtype=type)
     problem.get_samples(s)
-    print s
+    print(s)
 
     if do_plot:
         pylab.ion()
@@ -71,30 +71,30 @@
         pylab.plot(s[s[:,0]==2,1], s[s[:,0]==2,2], '^', color='g')
         pylab.plot(s[s[:,0]==3,1], s[s[:,0]==3,2], 'v', color='g')
 
-    print "Training"
+    print("Training")
     classifier.train(gamma = 1./3., C = 100, eps=1e-1)
 
-    print "Displaying model"
+    print("Displaying model")
     model = classifier.get_model()
-    print "Number of support vectors:", model.size()
-    print "Number of classes:", model.n_class()
-    print "Number of dimensions: ", model.n_dims()
-    print "Support vectors:"
+    print("Number of support vectors:", model.size())
+    print("Number of classes:", model.n_class())
+    print("Number of dimensions: ", model.n_dims())
+    print("Support vectors:")
     sv = numpy.zeros((model.size(), model.n_dims()), dtype=type)
     model.get_support_vectors(sv)
-    print sv
+    print(sv)
     
     if do_plot:
         pylab.plot(sv[:,0], sv[:,1], 'o', color='g')
 
-    print "Support vector coefficients:"
+    print("Support vector coefficients:")
     svc = numpy.zeros((model.n_class()-1, model.size()), dtype=type)
     model.get_support_vector_coefficients(svc)
-    print svc
-
-    print "Hyperplanes (for linear kernel only):"
+    print(svc)
+
+    print("Hyperplanes (for linear kernel only):")
     h = model.get_hyperplanes()
-    print h
+    print(h)
 
     if do_plot:
         xmin = numpy.min(samples[:,0])
@@ -116,35 +116,35 @@
                 points[i,j] = proba[0]
         pylab.contour(X,Y,points)
 
-    print "Cross-validation"
-    print classifier.cross_validate(2, gamma = .5, C = 10, eps = 1e-3)
-
-    print "Predicting"
+    print("Cross-validation")
+    print(classifier.cross_validate(2, gamma = .5, C = 10, eps = 1e-3))
+
+    print("Predicting")
     for y, x_list in zip(labels, samples):
         x = numpy.array(x_list, dtype=type)
         proba = numpy.zeros(model.n_class(), dtype=type)
-        print x, ': real=', y,
-        print 'p1=', classifier.predict(x),
-        print 'p2=', classifier.predict_probability(x, proba),
-        print 'proba=', proba
-
-    print "Discarding problem"
+        print(x, ': real=', y, end=' ')
+        print('p1=', classifier.predict(x), end=' ')
+        print('p2=', classifier.predict_probability(x, proba), end=' ')
+        print('proba=', proba)
+
+    print("Discarding problem")
     classifier.discard_problem()
 
-    print "Predicting after discarding the problem"
+    print("Predicting after discarding the problem")
     for y, x_list in zip(labels, samples):
         x = numpy.array(x_list, dtype=type)
         proba = numpy.zeros(model.n_class(), dtype=type)
-        print x, ': real=', y,
-        print 'p1=', classifier.predict(x),
-        print 'p2=', classifier.predict_probability(x, proba),
-        print 'proba=', proba
+        print(x, ': real=', y, end=' ')
+        print('p1=', classifier.predict(x), end=' ')
+        print('p2=', classifier.predict_probability(x, proba), end=' ')
+        print('proba=', proba)
 
 
 
 def persistence():
     
-    print "Persistence"
+    print("Persistence")
     numpy.random.seed(42)
     n_dims = 2
     n_class = 12
@@ -152,21 +152,21 @@
     labels = numpy.random.random_integers(0, 256, size)
     samples = numpy.zeros((size, n_dims), dtype=type)
 
-    print "Generating data"
+    print("Generating data")
     for i in range(0, size):
         t = 6.28 * numpy.random.random_sample()
         samples[i][0] = 2 * labels[i] + 1.5 * numpy.cos(t)
         samples[i][1] = 2 * labels[i] + 1.5 * numpy.sin(t)
 
-    print "Creating dense classifier"
+    print("Creating dense classifier")
     classifier = algo.svm_dense(0, n_dims = n_dims, seed=42)
     
-    print "Adding sample vectors to dense classifier"
+    print("Adding sample vectors to dense classifier")
     for y, x_list in zip(labels, samples):
         x = numpy.array(x_list, dtype=type)
         classifier.add_sample(float(y), x)
 
-    print "Serializing dense classifier"
+    print("Serializing dense classifier")
 
     schema = classifier.getSchema()
     with open("test", "w+b") as f:
@@ -180,24 +180,24 @@
         proto2 = schema.read(f)
         classifier = algo.svm_dense.read(proto2)
 
-    print "Training dense classifier"
+    print("Training dense classifier")
     classifier.train(gamma = 1, C = 10, eps=1e-1)
 
-    print "Predicting with dense classifier"
-    print classifier.predict(samples[0])
-
-    print "Creating 0/1 classifier"
+    print("Predicting with dense classifier")
+    print(classifier.predict(samples[0]))
+
+    print("Creating 0/1 classifier")
     classifier01 = algo.svm_01(n_dims = n_dims, seed=42)
 
-    print "Adding sample vectors to 0/1 classifier"
+    print("Adding sample vectors to 0/1 classifier")
     for y, x_list in zip(labels, samples):
         x = numpy.array(x_list, dtype=type)
         classifier01.add_sample(float(y), x)
 
-    print "Training 0/1 classifier"
+    print("Training 0/1 classifier")
     classifier01.train(gamma = 1./3., C = 100, eps=1e-1)
 
-    print "Serializing 0/1 classifier"
+    print("Serializing 0/1 classifier")
     schema = classifier01.getSchema()
     with open("test", "w+b") as f:
         # Save
@@ -210,20 +210,20 @@
         proto2 = schema.read(f)
         classifier01 = algo.svm_01.read(proto2)
 
-    print "Predicting with 0/1 classifier"
-    print classifier01.predict(numpy.array(samples[0], dtype=type))
+    print("Predicting with 0/1 classifier")
+    print(classifier01.predict(numpy.array(samples[0], dtype=type)))
 
 
 
 def cross_validation():
     return
-    print "Cross validation"
+    print("Cross validation")
     numpy.random.seed(42)
     labels = [0, 1, 1, 2, 1, 2]
     samples = [[0, 0, 0], [0, 1, 0], [1, 0, 1], [1, 1, 1], [1, 1, 0], [0, 1, 1]]
     classifier = algo.svm_dense(0, n_dims = 3, seed=42)
     
-    print "Adding sample vectors"
+    print("Adding sample vectors")
     for y, x_list in zip(labels, samples):
         x = numpy.array(x_list, dtype=type)
         classifier.add_sample(float(y), x)
@@ -231,11 +231,11 @@
     cPickle.dump(classifier, open('test', 'wb'))
     classifier = cPickle.load(open('test', 'rb'))
 
-    print "Training"
+    print("Training")
     classifier.train(gamma = 1./3., C = 100, eps=1e-1)
 
-    print "Cross validation =", 
-    print classifier.cross_validate(3, gamma = .5, C = 10, eps = 1e-3)
+    print("Cross validation =", end=' ') 
+    print(classifier.cross_validate(3, gamma = .5, C = 10, eps = 1e-3))
 
 #--------------------------------------------------------------------------------
 simple()
--- d:\nupic\src\python\python27\examples\network\core_encoders_demo.py	(original)
+++ d:\nupic\src\python\python27\examples\network\core_encoders_demo.py	(refactored)
@@ -121,9 +121,9 @@
   filename = resource_filename("nupic.datafiles",
                                "extra/hotgym/rec-center-hourly.csv")
   csvReader = csv.reader(open(filename, 'r'))
-  csvReader.next()
-  csvReader.next()
-  csvReader.next()
+  next(csvReader)
+  next(csvReader)
+  next(csvReader)
   for row in csvReader:
     timestampStr, consumptionStr = row
 
@@ -138,7 +138,7 @@
     network.run(1)
 
     anomalyScore = tmRegion.getOutputData('anomalyScore')[0]
-    print "Consumption: %s, Anomaly score: %f" % (consumptionStr, anomalyScore)
+    print("Consumption: %s, Anomaly score: %f" % (consumptionStr, anomalyScore))
 
 if __name__ == "__main__":
   network = createNetwork()
--- d:\nupic\src\python\python27\examples\network\custom_region_demo.py	(original)
+++ d:\nupic\src\python\python27\examples\network\custom_region_demo.py	(refactored)
@@ -111,7 +111,7 @@
   """
   identityRegion = network.regions["identityRegion"]
 
-  for i in xrange(_NUM_RECORDS):
+  for i in range(_NUM_RECORDS):
     # Run the network for a single iteration
     network.run(1)
 
@@ -128,5 +128,5 @@
   outputPath = os.path.join(os.path.dirname(__file__), _OUTPUT_PATH)
   with open(outputPath, "w") as outputFile:
     writer = csv.writer(outputFile)
-    print "Writing output to %s" % outputPath
+    print("Writing output to %s" % outputPath)
     runNetwork(network, writer)
--- d:\nupic\src\python\python27\examples\network\hierarchy_network_demo.py	(original)
+++ d:\nupic\src\python\python27\examples\network\hierarchy_network_demo.py	(refactored)
@@ -112,17 +112,17 @@
   """
   encoder = MultiEncoder()
   encoder.addMultipleEncoders({
-      "consumption": {"fieldname": u"consumption",
+      "consumption": {"fieldname": "consumption",
                       "type": "ScalarEncoder",
-                      "name": u"consumption",
+                      "name": "consumption",
                       "minval": 0.0,
                       "maxval": 100.0,
                       "clipInput": True,
                       "w": 21,
                       "n": 500},
-      "timestamp_timeOfDay": {"fieldname": u"timestamp",
+      "timestamp_timeOfDay": {"fieldname": "timestamp",
                               "type": "DateEncoder",
-                              "name": u"timestamp_timeOfDay",
+                              "name": "timestamp_timeOfDay",
                               "timeOfDay": (21, 9.5)}
   })
   return encoder
@@ -291,7 +291,7 @@
   l2PreviousPrediction = None
   l1ErrorSum = 0.0
   l2ErrorSum = 0.0
-  for record in xrange(numRecords):
+  for record in range(numRecords):
     # Run the network for a single iteration
     network.run(1)
 
@@ -326,23 +326,23 @@
 
   # Output absolute average error for each level
   if numRecords > 1:
-    print "L1 ave abs class. error: %f" % (l1ErrorSum / (numRecords - 1))
-    print "L2 ave abs class. error: %f" % (l2ErrorSum / (numRecords - 1))
+    print("L1 ave abs class. error: %f" % (l1ErrorSum / (numRecords - 1)))
+    print("L2 ave abs class. error: %f" % (l2ErrorSum / (numRecords - 1)))
 
 
 
 def runDemo():
   dataSource = FileRecordStream(streamID=_INPUT_FILE_PATH)
   numRecords = dataSource.getDataRowCount()
-  print "Creating network"
+  print("Creating network")
   network = createNetwork(dataSource)
   outputPath = os.path.join(os.path.dirname(__file__), _OUTPUT_FILE_NAME)
   with open(outputPath, "w") as outputFile:
     writer = csv.writer(outputFile)
-    print "Running network"
-    print "Writing output to: %s" % outputPath
+    print("Running network")
+    print("Writing output to: %s" % outputPath)
     runNetwork(network, numRecords, writer)
-  print "Hierarchy demo finished"
+  print("Hierarchy demo finished")
 
 
 
--- d:\nupic\src\python\python27\examples\network\network_api_demo.py	(original)
+++ d:\nupic\src\python\python27\examples\network\network_api_demo.py	(refactored)
@@ -185,7 +185,7 @@
 
   prevPredictedColumns = []
 
-  for i in xrange(_NUM_RECORDS):
+  for i in range(_NUM_RECORDS):
     # Run the network for a single iteration
     network.run(1)
 
@@ -205,20 +205,20 @@
 
   spRegion = network.getRegionsByType(SPRegion)[0]
   sp = spRegion.getSelf().getAlgorithmInstance()
-  print "spatial pooler region inputs: {0}".format(spRegion.getInputNames())
-  print "spatial pooler region outputs: {0}".format(spRegion.getOutputNames())
-  print "# spatial pooler columns: {0}".format(sp.getNumColumns())
-  print
+  print("spatial pooler region inputs: {0}".format(spRegion.getInputNames()))
+  print("spatial pooler region outputs: {0}".format(spRegion.getOutputNames()))
+  print("# spatial pooler columns: {0}".format(sp.getNumColumns()))
+  print()
 
   tmRegion = network.getRegionsByType(TMRegion)[0]
   tm = tmRegion.getSelf().getAlgorithmInstance()
-  print "temporal memory region inputs: {0}".format(tmRegion.getInputNames())
-  print "temporal memory region outputs: {0}".format(tmRegion.getOutputNames())
-  print "# temporal memory columns: {0}".format(tm.numberOfCols)
-  print
+  print("temporal memory region inputs: {0}".format(tmRegion.getInputNames()))
+  print("temporal memory region outputs: {0}".format(tmRegion.getOutputNames()))
+  print("# temporal memory columns: {0}".format(tm.numberOfCols))
+  print()
 
   outputPath = os.path.join(os.path.dirname(__file__), _OUTPUT_PATH)
   with open(outputPath, "w") as outputFile:
     writer = csv.writer(outputFile)
-    print "Writing output to %s" % outputPath
+    print("Writing output to %s" % outputPath)
     runNetwork(network, writer)
--- d:\nupic\src\python\python27\examples\network\temporal_anomaly_network_demo.py	(original)
+++ d:\nupic\src\python\python27\examples\network\temporal_anomaly_network_demo.py	(refactored)
@@ -171,7 +171,7 @@
   sensorRegion = network.regions["sensor"]
   temporalPoolerRegion = network.regions["temporalPoolerRegion"]
 
-  for i in xrange(_NUM_RECORDS):
+  for i in range(_NUM_RECORDS):
     # Run the network for a single iteration
     network.run(1)
 
@@ -223,5 +223,5 @@
   outputPath = os.path.join(os.path.dirname(__file__), _OUTPUT_PATH)
   with open(outputPath, "w") as outputFile:
     writer = csv.writer(outputFile)
-    print "Writing output to %s" % outputPath
+    print("Writing output to %s" % outputPath)
     runNetwork(network, writer)
--- d:\nupic\src\python\python27\examples\opf\clients\cpu\cpu.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\cpu\cpu.py	(refactored)
@@ -55,8 +55,8 @@
   predHistory = deque([0.0] * WINDOW, maxlen=60)
 
   # Initialize the plot lines that we will update with each new record.
-  actline, = plt.plot(range(WINDOW), actHistory)
-  predline, = plt.plot(range(WINDOW), predHistory)
+  actline, = plt.plot(list(range(WINDOW)), actHistory)
+  predline, = plt.plot(list(range(WINDOW)), predHistory)
   # Set the y-axis range.
   actline.axes.set_ylim(0, 100)
   predline.axes.set_ylim(0, 100)
--- d:\nupic\src\python\python27\examples\opf\clients\cpu\model_params.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\cpu\model_params.py	(refactored)
@@ -43,9 +43,9 @@
             # CPU usage encoder.
             'encoders': {
                 'cpu': {
-                    'fieldname': u'cpu',
+                    'fieldname': 'cpu',
                     'n': 200,
-                    'name': u'cpu',
+                    'name': 'cpu',
                     'type': 'ScalarEncoder',
                     'minval': 0.0,
                     'maxval': 100.0,
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\hotgym_anomaly.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\hotgym_anomaly.py	(refactored)
@@ -56,11 +56,11 @@
     reader = csv.reader(fin)
     csvWriter = csv.writer(open(_OUTPUT_PATH,"wb"))
     csvWriter.writerow(["timestamp", "consumption", "anomaly_score"])
-    headers = reader.next()
-    reader.next()
-    reader.next()
+    headers = next(reader)
+    next(reader)
+    next(reader)
     for i, record in enumerate(reader, start=1):
-      modelInput = dict(zip(headers, record))
+      modelInput = dict(list(zip(headers, record)))
       modelInput["consumption"] = float(modelInput["consumption"])
       modelInput["timestamp"] = datetime.datetime.strptime(
           modelInput["timestamp"], "%m/%d/%y %H:%M")
@@ -72,7 +72,7 @@
         _LOGGER.info("Anomaly detected at [%s]. Anomaly score: %f.",
                       result.rawInput["timestamp"], anomalyScore)
 
-  print "Anomaly scores have been written to",_OUTPUT_PATH
+  print("Anomaly scores have been written to",_OUTPUT_PATH)
 
 if __name__ == "__main__":
   logging.basicConfig(level=logging.INFO)
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\model_params.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\model_params.py	(refactored)
@@ -29,7 +29,7 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {  'days': 0,
-        'fields': [(u'c1', 'sum'), (u'c0', 'first')],
+        'fields': [('c1', 'sum'), ('c0', 'first')],
         'hours': 1,
         'microseconds': 0,
         'milliseconds': 0,
@@ -60,21 +60,21 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'timestamp_timeOfDay': {
-                        'fieldname': u'timestamp',
-                        'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {
+                        'fieldname': 'timestamp',
+                        'name': 'timestamp_timeOfDay',
                         'timeOfDay': (21, 9.5),
                         'type': 'DateEncoder'
                 },
-                u'timestamp_dayOfWeek': None,
-                u'timestamp_weekend': None,
-                u'consumption':    {
+                'timestamp_dayOfWeek': None,
+                'timestamp_weekend': None,
+                'consumption':    {
                     'clipInput': True,
-                    'fieldname': u'consumption',
+                    'fieldname': 'consumption',
                     'maxval': 100.0,
                     'minval': 0.0,
                     'n': 50,
-                    'name': u'consumption',
+                    'name': 'consumption',
                     'type': 'ScalarEncoder',
                     'w': 21
                 },
@@ -232,9 +232,9 @@
         'clEnable': False,
         'clParams': None,
 
-        'anomalyParams': {  u'anomalyCacheRecords': None,
-    u'autoDetectThreshold': None,
-    u'autoDetectWaitRecords': 2184},
+        'anomalyParams': {  'anomalyCacheRecords': None,
+    'autoDetectThreshold': None,
+    'autoDetectWaitRecords': 2184},
 
         'trainSPNetOnlyIfRequested': False,
     },
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\nupic_anomaly_output.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\nupic_anomaly_output.py	(refactored)
@@ -44,10 +44,7 @@
 ANOMALY_THRESHOLD = 0.9
 
 
-class NuPICOutput(object):
-
-  __metaclass__ = ABCMeta
-
+class NuPICOutput(object, metaclass=ABCMeta):
 
   def __init__(self, name):
     self.name = name
@@ -79,7 +76,7 @@
       'anomaly_score', 'anomaly_likelihood'
     ]
     outputFileName = "%s_out.csv" % self.name
-    print "Preparing to output %s data to %s" % (self.name, outputFileName)
+    print("Preparing to output %s data to %s" % (self.name, outputFileName))
     self.outputFile = open(outputFileName, "w")
     self.outputWriter = csv.writer(self.outputFile)
     self.outputWriter.writerow(headerRow)
@@ -100,7 +97,7 @@
 
   def close(self):
     self.outputFile.close()
-    print "Done. Wrote %i data lines to %s." % (self.lineCount, self.name)
+    print("Done. Wrote %i data lines to %s." % (self.lineCount, self.name))
 
 
 
@@ -200,7 +197,7 @@
 
 
   def initializeLines(self, timestamp):
-    print "initializing %s" % self.name
+    print("initializing %s" % self.name)
     anomalyRange = (0.0, 1.0)
     self.dates = deque([timestamp] * WINDOW, maxlen=WINDOW)
     self.convertedDates = deque(
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\remove_tuesdays.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\remove_tuesdays.py	(refactored)
@@ -23,9 +23,9 @@
   with open(ORIGINAL, 'rb') as inputFile:
     reader = csv.reader(inputFile)
     outputCache = ""
-    headers = reader.next()
-    types = reader.next()
-    flags = reader.next()
+    headers = next(reader)
+    types = next(reader)
+    flags = next(reader)
 
     for row in [headers, types, flags]:
       outputCache += ",".join(row) + "\n"
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\run.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\run.py	(refactored)
@@ -69,7 +69,7 @@
   importName = "model_params.%s_model_params" % (
     gymName.replace(" ", "_").replace("-", "_")
   )
-  print "Importing model params from %s" % importName
+  print("Importing model params from %s" % importName)
   try:
     importedModelParams = importlib.import_module(importName).MODEL_PARAMS
   except ImportError:
@@ -92,9 +92,9 @@
   inputFile = open(inputData, "rb")
   csvReader = csv.reader(inputFile)
   # skip header rows
-  csvReader.next()
-  csvReader.next()
-  csvReader.next()
+  next(csvReader)
+  next(csvReader)
+  next(csvReader)
 
   shifter = InferenceShifter()
   if plot:
@@ -106,7 +106,7 @@
   for row in csvReader:
     counter += 1
     if (counter % 100 == 0):
-      print "Read %i lines..." % counter
+      print("Read %i lines..." % counter)
     timestamp = datetime.datetime.strptime(row[0], DATE_FORMAT)
     consumption = float(row[1])
     result = model.run({
@@ -135,7 +135,7 @@
   :param plot: Plot in matplotlib? Don't use this unless matplotlib is
   installed.
   """
-  print "Creating model from %s..." % gymName
+  print("Creating model from %s..." % gymName)
   model = createModel(getModelParamsFromName(gymName))
   inputData = "%s/%s.csv" % (DATA_DIR, gymName.replace(" ", "_"))
   runIoThroughNupic(inputData, model, gymName, plot)
@@ -143,7 +143,7 @@
 
 
 if __name__ == "__main__":
-  print DESCRIPTION
+  print(DESCRIPTION)
   plot = False
   args = sys.argv[1:]
   if "--plot" in args:
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\model_params\rec_center_hourly_model_params.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\anomaly\one_gym\model_params\rec_center_hourly_model_params.py	(refactored)
@@ -10,9 +10,9 @@
                        'weeks': 0,
                        'years': 0},
   'model': 'HTMPrediction',
-  'modelParams': { 'anomalyParams': { u'anomalyCacheRecords': None,
-                                      u'autoDetectThreshold': None,
-                                      u'autoDetectWaitRecords': None},
+  'modelParams': { 'anomalyParams': { 'anomalyCacheRecords': None,
+                                      'autoDetectThreshold': None,
+                                      'autoDetectWaitRecords': None},
                    'clParams': { 'alpha': 0.01962508905154251,
                                  'verbosity': 0,
                                  'regionName': 'SDRClassifierRegion',
@@ -27,7 +27,7 @@
                                                                          'name': '_classifierInput',
                                                                          'type': 'ScalarEncoder',
                                                                          'w': 21},
-                                                   u'kw_energy_consumption': { 'clipInput': True,
+                                                   'kw_energy_consumption': { 'clipInput': True,
                                                                                'fieldname': 'kw_energy_consumption',
                                                                                'maxval': 53.0,
                                                                                'minval': 0.0,
@@ -35,13 +35,13 @@
                                                                                'name': 'kw_energy_consumption',
                                                                                'type': 'ScalarEncoder',
                                                                                'w': 21},
-                                                   u'timestamp_dayOfWeek': None,
-                                                   u'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                   'timestamp_dayOfWeek': None,
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
                                                                              'name': 'timestamp',
                                                                              'timeOfDay': ( 21,
                                                                                             6.090344152692538),
                                                                              'type': 'DateEncoder'},
-                                                   u'timestamp_weekend': { 'fieldname': 'timestamp',
+                                                   'timestamp_weekend': { 'fieldname': 'timestamp',
                                                                            'name': 'timestamp',
                                                                            'type': 'DateEncoder',
                                                                            'weekend': ( 21,
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\cleanup.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\cleanup.py	(refactored)
@@ -34,7 +34,7 @@
     for f in os.listdir(directory):
       if re.search(r"_out\.csv$", f)\
       or re.search(r"\.pyc$", f):
-        print "Removing %s" % f
+        print("Removing %s" % f)
         os.remove(os.path.join(directory, f))
 
 
@@ -51,11 +51,11 @@
     for doomed in workingDirs:
       doomedPath = os.path.join(directory, doomed)
       if os.path.exists(doomedPath):
-        print "Removing %s" % doomedPath
+        print("Removing %s" % doomedPath)
         shutil.rmtree(doomedPath)
 
 
 
 if __name__ == "__main__":
-  print DESCRIPTION
+  print(DESCRIPTION)
   cleanUp(workingDirs=["swarm"])
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\nupic_output.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\nupic_output.py	(refactored)
@@ -39,10 +39,7 @@
 WINDOW = 100
 
 
-class NuPICOutput(object):
-
-  __metaclass__ = ABCMeta
-
+class NuPICOutput(object, metaclass=ABCMeta):
 
   def __init__(self, names, showAnomalyScore=False):
     self.names = names
@@ -73,7 +70,7 @@
     for name in self.names:
       self.lineCounts.append(0)
       outputFileName = "%s_out.csv" % name
-      print "Preparing to output %s data to %s" % (name, outputFileName)
+      print("Preparing to output %s data to %s" % (name, outputFileName))
       outputFile = open(outputFileName, "w")
       self.outputFiles.append(outputFile)
       outputWriter = csv.writer(outputFile)
@@ -103,7 +100,7 @@
   def close(self):
     for index, name in enumerate(self.names):
       self.outputFiles[index].close()
-      print "Done. Wrote %i data lines to %s." % (self.lineCounts[index], name)
+      print("Done. Wrote %i data lines to %s." % (self.lineCounts[index], name))
 
 
 
@@ -137,7 +134,7 @@
 
   def initializeLines(self, timestamps):
     for index in range(len(self.names)):
-      print "initializing %s" % self.names[index]
+      print("initializing %s" % self.names[index])
       # graph = self.graphs[index]
       self.dates.append(deque([timestamps[index]] * WINDOW, maxlen=WINDOW))
       self.convertedDates.append(deque(
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\run.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\run.py	(refactored)
@@ -75,7 +75,7 @@
   importName = "model_params.%s_model_params" % (
     gymName.replace(" ", "_").replace("-", "_")
   )
-  print "Importing model params from %s" % importName
+  print("Importing model params from %s" % importName)
   try:
     importedModelParams = importlib.import_module(importName).MODEL_PARAMS
   except ImportError:
@@ -89,9 +89,9 @@
   inputFile = open(inputData, "rb")
   csvReader = csv.reader(inputFile)
   # skip header rows
-  csvReader.next()
-  csvReader.next()
-  csvReader.next()
+  next(csvReader)
+  next(csvReader)
+  next(csvReader)
 
   shifter = InferenceShifter()
   if plot:
@@ -114,11 +114,11 @@
     result.metrics = metricsManager.update(result)
 
     if counter % 100 == 0:
-      print "Read %i lines..." % counter
-      print ("After %i records, 1-step altMAPE=%f" % (counter,
+      print("Read %i lines..." % counter)
+      print(("After %i records, 1-step altMAPE=%f" % (counter,
               result.metrics["multiStepBestPredictions:multiStep:"
                              "errorMetric='altMAPE':steps=1:window=1000:"
-                             "field=kw_energy_consumption"]))
+                             "field=kw_energy_consumption"])))
 
     if plot:
       result = shifter.shift(result)
@@ -135,7 +135,7 @@
 
 
 def runModel(gymName, plot=False):
-  print "Creating model from %s..." % gymName
+  print("Creating model from %s..." % gymName)
   model = createModel(getModelParamsFromName(gymName))
   inputData = "%s/%s.csv" % (DATA_DIR, gymName.replace(" ", "_"))
   runIoThroughNupic(inputData, model, gymName, plot)
@@ -143,7 +143,7 @@
 
 
 if __name__ == "__main__":
-  print DESCRIPTION
+  print(DESCRIPTION)
   plot = False
   args = sys.argv[1:]
   if "--plot" in args:
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\swarm.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\prediction\one_gym\swarm.py	(refactored)
@@ -86,26 +86,26 @@
 
 def printSwarmSizeWarning(size):
   if size is "small":
-    print "= THIS IS A DEBUG SWARM. DON'T EXPECT YOUR MODEL RESULTS TO BE GOOD."
+    print("= THIS IS A DEBUG SWARM. DON'T EXPECT YOUR MODEL RESULTS TO BE GOOD.")
   elif size is "medium":
-    print "= Medium swarm. Sit back and relax, this could take awhile."
+    print("= Medium swarm. Sit back and relax, this could take awhile.")
   else:
-    print "= LARGE SWARM! Might as well load up the Star Wars Trilogy."
+    print("= LARGE SWARM! Might as well load up the Star Wars Trilogy.")
 
 
 
 def swarm(filePath):
   name = os.path.splitext(os.path.basename(filePath))[0]
-  print "================================================="
-  print "= Swarming on %s data..." % name
+  print("=================================================")
+  print("= Swarming on %s data..." % name)
   printSwarmSizeWarning(SWARM_DESCRIPTION["swarmSize"])
-  print "================================================="
+  print("=================================================")
   modelParams = swarmForBestModelParams(SWARM_DESCRIPTION, name)
-  print "\nWrote the following model param files:"
-  print "\t%s" % modelParams
+  print("\nWrote the following model param files:")
+  print("\t%s" % modelParams)
 
 
 
 if __name__ == "__main__":
-  print DESCRIPTION
+  print(DESCRIPTION)
   swarm(INPUT_FILE)
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\simple\hotgym.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\simple\hotgym.py	(refactored)
@@ -70,11 +70,11 @@
                                   model.getInferenceType())
   with open (_INPUT_FILE_PATH) as fin:
     reader = csv.reader(fin)
-    headers = reader.next()
-    reader.next()
-    reader.next()
+    headers = next(reader)
+    next(reader)
+    next(reader)
     for i, record in enumerate(reader, start=1):
-      modelInput = dict(zip(headers, record))
+      modelInput = dict(list(zip(headers, record)))
       modelInput["consumption"] = float(modelInput["consumption"])
       modelInput["timestamp"] = datetime.datetime.strptime(
           modelInput["timestamp"], "%m/%d/%y %H:%M")
--- d:\nupic\src\python\python27\examples\opf\clients\hotgym\simple\model_params.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\hotgym\simple\model_params.py	(refactored)
@@ -55,20 +55,20 @@
 
             # Include the encoders we use
             'encoders': {
-                u'consumption':    {
-                    'fieldname': u'consumption',
+                'consumption':    {
+                    'fieldname': 'consumption',
                     'resolution': 0.88,
                     'seed': 1,
-                    'name': u'consumption',
+                    'name': 'consumption',
                     'type': 'RandomDistributedScalarEncoder',
                     },
 
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (21, 1),
                                            'type': 'DateEncoder'},
-                'timestamp_weekend': {   'fieldname': u'timestamp',
-                                         'name': u'timestamp_weekend',
+                'timestamp_weekend': {   'fieldname': 'timestamp',
+                                         'name': 'timestamp_weekend',
                                          'type': 'DateEncoder',
                                          'weekend': 21}
             },
--- d:\nupic\src\python\python27\examples\opf\clients\nyctaxi\nyctaxi_anomaly.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\clients\nyctaxi\nyctaxi_anomaly.py	(refactored)
@@ -82,9 +82,9 @@
     reader = csv.reader(fin)
     csvWriter = csv.writer(open(_OUTPUT_PATH,"wb"))
     csvWriter.writerow(["timestamp", "value", "anomaly_score"])
-    headers = reader.next()
+    headers = next(reader)
     for i, record in enumerate(reader, start=1):
-      modelInput = dict(zip(headers, record))
+      modelInput = dict(list(zip(headers, record)))
       modelInput["value"] = float(modelInput["value"])
       modelInput["timestamp"] = datetime.datetime.strptime(
           modelInput["timestamp"], "%Y-%m-%d %H:%M:%S")
@@ -96,7 +96,7 @@
         _LOGGER.info("Anomaly detected at [%s]. Anomaly score: %f.",
                       result.rawInput["timestamp"], anomalyScore)
 
-  print "Anomaly scores have been written to",_OUTPUT_PATH
+  print("Anomaly scores have been written to",_OUTPUT_PATH)
 
 if __name__ == "__main__":
   logging.basicConfig(level=logging.INFO)
--- d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\hotgym\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\hotgym\description.py	(refactored)
@@ -126,21 +126,21 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {   'consumption': {   'clipInput': True,
-                                   'fieldname': u'consumption',
+                                   'fieldname': 'consumption',
                                    'n': 100,
-                                   'name': u'consumption',
+                                   'name': 'consumption',
                                    'type': 'AdaptiveScalarEncoder',
                                    'w': 21},
                 'timestamp_dayOfWeek': {   'dayOfWeek': (21, 1),
-                                           'fieldname': u'timestamp',
-                                           'name': u'timestamp_dayOfWeek',
+                                           'fieldname': 'timestamp',
+                                           'name': 'timestamp_dayOfWeek',
                                            'type': 'DateEncoder'},
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (21, 1),
                                            'type': 'DateEncoder'},
-                'timestamp_weekend': {   'fieldname': u'timestamp',
-                                         'name': u'timestamp_weekend',
+                'timestamp_weekend': {   'fieldname': 'timestamp',
+                                         'name': 'timestamp_weekend',
                                          'type': 'DateEncoder',
                                          'weekend': 21}},
 
@@ -355,13 +355,13 @@
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
   'dataset' : {
-        u'info': u'test_hotgym',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'hotGym.csv',
-                            u'last_record': config['numRecords'],
-                            u'source': u'file://extra/hotgym/hotgym.csv'}],
+        'info': 'test_hotgym',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'hotGym.csv',
+                            'last_record': config['numRecords'],
+                            'source': 'file://extra/hotgym/hotgym.csv'}],
          'aggregation': config['aggregationInfo'],
-        u'version': 1},
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
--- d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\saw_200\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\saw_200\description.py	(refactored)
@@ -126,11 +126,11 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {   'f': {   'clipInput': True,
-                         'fieldname': u'f',
+                         'fieldname': 'f',
                          'maxval': 200,
                          'minval': 0,
                          'n': 513,
-                         'name': u'f',
+                         'name': 'f',
                          'type': 'ScalarEncoder',
                          'w': 21}},
 
@@ -336,13 +336,13 @@
 
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'Artificial Data',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'blah',
-                            u'source': u'file://'+os.path.join(os.path.dirname(__file__), 'data.csv'),
+  'dataset' : {   'info': 'Artificial Data',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'blah',
+                            'source': 'file://'+os.path.join(os.path.dirname(__file__), 'data.csv'),
                         }
                      ],
-        u'version': 1},
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -355,12 +355,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'f', 'predictionSteps': [1]},
+  "inferenceArgs":{'predictedField': 'f', 'predictionSteps': [1]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'f', metric='aae', inferenceElement='prediction', params={'window': 100}),
+    MetricSpec(field='f', metric='aae', inferenceElement='prediction', params={'window': 100}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\saw_big\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\saw_big\description.py	(refactored)
@@ -126,11 +126,11 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {   'f': {   'clipInput': True,
-                         'fieldname': u'f',
+                         'fieldname': 'f',
                          'maxval': 100,
                          'minval': 0,
                          'n': 100,
-                         'name': u'f',
+                         'name': 'f',
                          'type': 'AdaptiveScalarEncoder',
                          'w': 21}},
 
@@ -337,12 +337,12 @@
 
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'Artificial Data',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'blah',
-                            u'source': u'file://'+os.path.join(os.path.dirname(__file__), 'data.csv'),
+  'dataset' : {   'info': 'Artificial Data',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'blah',
+                            'source': 'file://'+os.path.join(os.path.dirname(__file__), 'data.csv'),
                         }],
-        u'version': 1},
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -355,12 +355,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'f', 'predictionSteps': [1]},
+  "inferenceArgs":{'predictedField': 'f', 'predictionSteps': [1]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'f', metric='aae', inferenceElement='prediction', params={'window': 1000}),
+    MetricSpec(field='f', metric='aae', inferenceElement='prediction', params={'window': 1000}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\simple\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\anomaly\temporal\simple\description.py	(refactored)
@@ -125,11 +125,11 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'f':     {
+                'f':     {
 			'clipInput': True,
-			'fieldname': u'f',
+			'fieldname': 'f',
     			'n': 100,
-    			'name': u'f',
+    			'name': 'f',
     			'minval': 0,
     			'maxval': 5,
     			'type': 'ScalarEncoder',
@@ -344,11 +344,11 @@
 
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'cerebro_dummy',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'hotGym.csv',
-                            u'source': u'file://'+os.path.join(os.path.dirname(__file__), 'data.csv')}],
-        u'version': 1},
+  'dataset' : {   'info': 'cerebro_dummy',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'hotGym.csv',
+                            'source': 'file://'+os.path.join(os.path.dirname(__file__), 'data.csv')}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -361,12 +361,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'f', u'predictionSteps': [1]},
+  "inferenceArgs":{'predictedField': 'f', 'predictionSteps': [1]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'f', metric='passThruPrediction', inferenceElement='anomalyScore', params={'window': 1000}),
+    MetricSpec(field='f', metric='passThruPrediction', inferenceElement='anomalyScore', params={'window': 1000}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\examples\opf\experiments\classification\makeDatasets.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\classification\makeDatasets.py	(refactored)
@@ -52,7 +52,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('reset', 'int', 'R'), ('category', 'int', 'C'),
             ('field1', 'string', '')]  
   outFile = FileRecordStream(pathname, write=True, fields=fields)
@@ -66,7 +66,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for i in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   for seqIdx in seqIdxs:
@@ -100,7 +100,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('reset', 'int', 'R'), ('category', 'int', 'C'),
             ('field1', 'float', '')]  
   outFile = FileRecordStream(pathname, write=True, fields=fields)
@@ -114,7 +114,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for i in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   for seqIdx in seqIdxs:
@@ -153,7 +153,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('reset', 'int', 'R'), ('category', 'int', 'C'),
             ('field1', 'string', '')]  
   outFile = FileRecordStream(pathname, write=True, fields=fields)
@@ -180,7 +180,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for _ in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   for seqIdx in seqIdxs:
--- d:\nupic\src\python\python27\examples\opf\experiments\classification\base_category\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\classification\base_category\description.py	(refactored)
@@ -125,9 +125,9 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   'field1': {   'fieldname': u'field1',
+            'encoders': {   'field1': {   'fieldname': 'field1',
                               'n': 100,
-                              'name': u'field1',
+                              'name': 'field1',
                               'type': 'SDRCategoryEncoder',
                               'w': 21}},
 
@@ -358,13 +358,13 @@
 
       # Input stream specification per py/nupic/cluster/database/StreamDef.json.
       #
-      'dataset' : {   u'info': u'test_NoProviders',
-        u'streams': [   {
-              u'columns': [u'*'],
-              u'info': u'simple.csv',
+      'dataset' : {   'info': 'test_NoProviders',
+        'streams': [   {
+              'columns': ['*'],
+              'info': 'simple.csv',
               'source':  config['dataSource'],
           }],
-        u'version': 1},
+        'version': 1},
 
       # Iteration count: maximum number of iterations.  Each iteration corresponds
       # to one record from the (possibly aggregated) dataset.  The task is
--- d:\nupic\src\python\python27\examples\opf\experiments\classification\base_scalar\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\classification\base_scalar\description.py	(refactored)
@@ -126,9 +126,9 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {   'field1': {   'clipInput': True,
-                              'fieldname': u'field1',
+                              'fieldname': 'field1',
                               'n': 438,
-                              'name': u'field1',
+                              'name': 'field1',
                               'type': 'AdaptiveScalarEncoder',
                               'w': 7,
 			      'forced': True}},
@@ -360,13 +360,13 @@
 
       # Input stream specification per py/nupic/cluster/database/StreamDef.json.
       #
-      'dataset' : {   u'info': u'test_NoProviders',
-        u'streams': [   {
-              u'columns': [u'*'],
-              u'info': u'simple.csv',
+      'dataset' : {   'info': 'test_NoProviders',
+        'streams': [   {
+              'columns': ['*'],
+              'info': 'simple.csv',
               'source':  config['dataSource'],
           }],
-        u'version': 1},
+        'version': 1},
 
       # Iteration count: maximum number of iterations.  Each iteration corresponds
       # to one record from the (possibly aggregated) dataset.  The task is
--- d:\nupic\src\python\python27\examples\opf\experiments\classification\scalar_encoder_0\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\classification\scalar_encoder_0\description.py	(refactored)
@@ -32,11 +32,11 @@
   'modelParams': { 'clParams': { 'verbosity': 0},
                    'inferenceType': 'NontemporalClassification',
                    'sensorParams': { 'encoders': { 'field1': { 'clipInput': True,
-                                                               'fieldname': u'field1',
+                                                               'fieldname': 'field1',
                                                                'maxval': 0.10000000000000001,
                                                                'minval': 0.0,
                                                                'n': 11,
-                                                               'name': u'field1',
+                                                               'name': 'field1',
                                                                'type': 'AdaptiveScalarEncoder',
                                                                'w': 7}},
                                      'verbosity': 0},
--- d:\nupic\src\python\python27\examples\opf\experiments\missing_record\make_datasets.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\missing_record\make_datasets.py	(refactored)
@@ -53,7 +53,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('timestamp', 'datetime', 'T'), 
             ('field1', 'string', ''),  
             ('field2', 'float', '')]  
@@ -69,7 +69,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for i in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   # Put 1 hour between each record
--- d:\nupic\src\python\python27\examples\opf\experiments\missing_record\base\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\missing_record\base\description.py	(refactored)
@@ -130,26 +130,26 @@
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
                 'timestamp_timeOfDay':     {
-                              'fieldname': u'timestamp',
-                              'name': u'timestamp_timeOfDay',
+                              'fieldname': 'timestamp',
+                              'name': 'timestamp_timeOfDay',
                               'timeOfDay': (21, 1),
                               'type': 'DateEncoder'},
                 'timestamp_dayOfWeek':     {
                               'dayOfWeek': (21, 1),
-                              'fieldname': u'timestamp',
-                              'name': u'timestamp_dayOfWeek',
+                              'fieldname': 'timestamp',
+                              'name': 'timestamp_dayOfWeek',
                               'type': 'DateEncoder'},
-                'field1': {   'fieldname': u'field1',
+                'field1': {   'fieldname': 'field1',
                               'n': 100,
-                              'name': u'field1',
+                              'name': 'field1',
                               'type': 'SDRCategoryEncoder',
                               'w': 21},
                 'field2': {   'clipInput': True,
-                              'fieldname': u'field2',
+                              'fieldname': 'field2',
                               'maxval': 50,
                               'minval': 0,
                               'n': 500,
-                              'name': u'field2',
+                              'name': 'field2',
                               'type': 'ScalarEncoder',
                               'w': 21}},
 
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\make_datasets.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\make_datasets.py	(refactored)
@@ -51,7 +51,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('reset', 'int', 'R'), 
             ('field1', 'string', ''),  
             ('field2', 'float', '')]  
@@ -66,7 +66,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for i in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   for seqIdx in seqIdxs:
@@ -105,7 +105,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('reset', 'int', 'R'), 
             ('field1', 'string', ''),  
             ('field2', 'float', '')]  
@@ -133,7 +133,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for _ in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   for seqIdx in seqIdxs:
@@ -284,7 +284,7 @@
   """
   
   # Create the file
-  print "Creating %s..." % (filename)
+  print("Creating %s..." % (filename))
   fields = [('reset', 'int', 'R'), 
             ('field1', 'string', ''),
             ('field2', 'float', '')]
@@ -298,12 +298,12 @@
   initCumProb = initProb.cumsum()
   
   firstOrderCumProb = dict()
-  for (key,value) in firstOrderProb.iteritems():
+  for (key,value) in firstOrderProb.items():
     firstOrderCumProb[key] = value.cumsum()
     
   if secondOrderProb is not None:
     secondOrderCumProb = dict()
-    for (key,value) in secondOrderProb.iteritems():
+    for (key,value) in secondOrderProb.items():
       secondOrderCumProb[key] = value.cumsum()
   else:
     secondOrderCumProb = None
@@ -314,7 +314,7 @@
   elementsInSeq = []
   numElementsSinceReset = 0
   maxCatIdx = len(categoryList) - 1
-  for _ in xrange(numRecords):
+  for _ in range(numRecords):
 
     # Generate a reset?
     if numElementsSinceReset == 0:
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\base\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\base\description.py	(refactored)
@@ -125,17 +125,17 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   'field1': {   'fieldname': u'field1',
+            'encoders': {   'field1': {   'fieldname': 'field1',
                               'n': 100,
-                              'name': u'field1',
+                              'name': 'field1',
                               'type': 'SDRCategoryEncoder',
                               'w': 21},
                 'field2': {   'clipInput': True,
-                              'fieldname': u'field2',
+                              'fieldname': 'field2',
                               'maxval': 50,
                               'minval': 0,
                               'n': 500,
-                              'name': u'field2',
+                              'name': 'field2',
                               'type': 'ScalarEncoder',
                               'w': 21}},
 
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\base\permutations_simple_3.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\base\permutations_simple_3.py	(refactored)
@@ -1,106 +1 @@
-# ----------------------------------------------------------------------
-# Numenta Platform for Intelligent Computing (NuPIC)
-# Copyright (C) 2013, Numenta, Inc.  Unless you have an agreement
-# with Numenta, Inc., for a separate license for this software code, the
-# following terms and conditions apply:
-#
-# This program is free software: you can redistribute it and/or modify
-# it under the terms of the GNU Affero Public License version 3 as
-# published by the Free Software Foundation.
-#
-# This program is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.
-# See the GNU Affero Public License for more details.
-#
-# You should have received a copy of the GNU Affero Public License
-# along with this program.  If not, see http://www.gnu.org/licenses.
-#
-# http://numenta.org/licenses/
-# ----------------------------------------------------------------------
-"""
-Template file used by ExpGenerator to generate the actual
-permutations.py file by replacing $XXXXXXXX tokens with desired values.
-
-This permutations.py file was generated by:
-'~/nupic/eng/lib/python2.6/site-packages/nupic/frameworks/opf/expGenerator/ExpGenerator.pyc'
-"""
-import os
-from pkg_resources import resources_filename
-from nupic.swarming.permutation_helpers import *
-
-# The name of the field being predicted.  Any allowed permutation MUST contain
-# the prediction field.
-# (generated from PREDICTION_FIELD)
-predictedField = 'field2'
-
-permutations = {
-  # Encoder permutation choices
-  # Example:
-  #
-  #  '__gym_encoder' : PermuteEncoder('gym', 'SDRCategoryEncoder', w=7,
-  #        n=100),
-  #
-  #  '__address_encoder' : PermuteEncoder('address', 'SDRCategoryEncoder',
-  #        w=7, n=100),
-  #
-  #  '__timestamp_timeOfDay_encoder' : PermuteEncoder('timestamp',
-  #        'DateEncoder.timeOfDay', w=7, radius=PermuteChoices([1, 8])),
-  #
-  #  '__timestamp_dayOfWeek_encoder' : PermuteEncoder('timestamp',
-  #        'DateEncoder.dayOfWeek', w=7, radius=PermuteChoices([1, 3])),
-  #
-  #  '__consumption_encoder' : PermuteEncoder('consumption', 'ScalarEncoder',
-  #        w=7, n=PermuteInt(13, 500, 20), minval=0,
-  #        maxval=PermuteInt(100, 300, 25)),
-  #
-  #  (generated from PERM_ENCODER_CHOICES)
-  'predictedField': 'field2',
-  'predictionSteps': [1,3],
-
-  relativePath = os.path.join("examples", "opf", "experiments", "multistep",
-                              "datasets", "simple_3.csv")
-  'dataSource': 'file://%s' % (resource_filename("nupic", relativePath)),
-
-  '__field2_encoder' : PermuteEncoder(fieldName='field2',
-              clipInput=True, minval = 0, maxval=50,
-              encoderClass='ScalarEncoder',
-              w=21, n=PermuteChoices([500])),
-
-}
-
-
-# Fields selected for final hypersearch report;
-# NOTE: These values are used as regular expressions by RunPermutations.py's
-#       report generator
-# (fieldname values generated from PERM_PREDICTED_FIELD_NAME)
-report = [
-          '.*field2.*',
-         ]
-
-# Permutation optimization setting: either minimize or maximize metric
-# used by RunPermutations.
-# NOTE: The value is used as a regular expressions by RunPermutations.py's
-#       report generator
-# (generated from minimize = 'prediction:aae:window=1000:field=consumption')
-minimize = "multiStepBestPredictions:multiStep:errorMetric='aae':steps=3:window=200:field=field2"
-
-
-
-def permutationFilter(perm):
-  """ This function can be used to selectively filter out specific permutation
-  combinations. It is called by RunPermutations for every possible permutation
-  of the variables in the permutations dict. It should return True for valid a
-  combination of permutation values and False for an invalid one.
-
-  Parameters:
-  ---------------------------------------------------------
-  perm: dict of one possible combination of name:value
-        pairs chosen from permutations.
-  """
-
-  # An example of how to use this
-  #if perm['__consumption_encoder']['maxval'] > 300:
-  #  return False;
-  #
-  return True
+Non
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym\description.py	(refactored)
@@ -126,21 +126,21 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {   'consumption': {   'clipInput': True,
-                                   'fieldname': u'consumption',
+                                   'fieldname': 'consumption',
                                    'n': 100,
-                                   'name': u'consumption',
+                                   'name': 'consumption',
                                    'type': 'AdaptiveScalarEncoder',
                                    'w': 21},
                 'timestamp_dayOfWeek': {   'dayOfWeek': (21, 1),
-                                           'fieldname': u'timestamp',
-                                           'name': u'timestamp_dayOfWeek',
+                                           'fieldname': 'timestamp',
+                                           'name': 'timestamp_dayOfWeek',
                                            'type': 'DateEncoder'},
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (21, 1),
                                            'type': 'DateEncoder'},
-                'timestamp_weekend': {   'fieldname': u'timestamp',
-                                         'name': u'timestamp_weekend',
+                'timestamp_weekend': {   'fieldname': 'timestamp',
+                                         'name': 'timestamp_weekend',
                                          'type': 'DateEncoder',
                                          'weekend': 21}},
 
@@ -360,13 +360,13 @@
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
   'dataset' : {
-        u'info': u'test_hotgym',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'hotGym.csv',
-                            u'last_record': config['numRecords'],
-                            u'source': u'file://extra/hotgym/hotgym.csv'}],
+        'info': 'test_hotgym',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'hotGym.csv',
+                            'last_record': config['numRecords'],
+                            'source': 'file://extra/hotgym/hotgym.csv'}],
          'aggregation': config['aggregationInfo'],
-        u'version': 1},
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_enc\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_enc\description.py	(refactored)
@@ -29,23 +29,23 @@
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'inferenceType': 'NontemporalMultiStep',
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': { 'dayOfWeek': ( 21,
                                                                                            1),
-                                                                            'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_dayOfWeek',
+                                                                            'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_dayOfWeek',
                                                                             'type': 'DateEncoder'},
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            1),
                                                                             'type': 'DateEncoder'},
-                                                   'timestamp_weekend': { 'fieldname': u'timestamp',
-                                                                          'name': u'timestamp_weekend',
+                                                   'timestamp_weekend': { 'fieldname': 'timestamp',
+                                                                          'name': 'timestamp_weekend',
                                                                           'type': 'DateEncoder',
                                                                           'weekend': 21}},
                                      'verbosity': 0},
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp\description.py	(refactored)
@@ -29,14 +29,14 @@
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'inferenceType': 'NontemporalMultiStep',
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': None,
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            8),
                                                                             'type': 'DateEncoder'},
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp_16K\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp_16K\description.py	(refactored)
@@ -29,14 +29,14 @@
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'inferenceType': 'NontemporalMultiStep',
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': None,
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            8),
                                                                             'type': 'DateEncoder'},
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp_5step\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp_5step\description.py	(refactored)
@@ -29,18 +29,18 @@
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'inferenceType': 'NontemporalMultiStep',
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': { 'dayOfWeek': ( 21,
                                                                                            3),
-                                                                            'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_dayOfWeek',
+                                                                            'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_dayOfWeek',
                                                                             'type': 'DateEncoder'},
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            1),
                                                                             'type': 'DateEncoder'},
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp_5step_16K\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_sp_5step_16K\description.py	(refactored)
@@ -29,18 +29,18 @@
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'inferenceType': 'NontemporalMultiStep',
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': { 'dayOfWeek': ( 21,
                                                                                            3),
-                                                                            'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_dayOfWeek',
+                                                                            'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_dayOfWeek',
                                                                             'type': 'DateEncoder'},
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            1),
                                                                             'type': 'DateEncoder'},
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_tp\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_tp\description.py	(refactored)
@@ -28,14 +28,14 @@
 config = \
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': None,
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            8),
                                                                             'type': 'DateEncoder'},
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_tp_16K\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_tp_16K\description.py	(refactored)
@@ -28,14 +28,14 @@
 config = \
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': None,
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            8),
                                                                             'type': 'DateEncoder'},
--- d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_tp_5step\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\multistep\hotgym_best_tp_5step\description.py	(refactored)
@@ -28,18 +28,18 @@
 config = \
 { 'modelParams': { 'clParams': { 'verbosity': 0},
                    'sensorParams': { 'encoders': { 'consumption': { 'clipInput': True,
-                                                                    'fieldname': u'consumption',
+                                                                    'fieldname': 'consumption',
                                                                     'n': 28,
-                                                                    'name': u'consumption',
+                                                                    'name': 'consumption',
                                                                     'type': 'AdaptiveScalarEncoder',
                                                                     'w': 21},
                                                    'timestamp_dayOfWeek': { 'dayOfWeek': ( 21,
                                                                                            3),
-                                                                            'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_dayOfWeek',
+                                                                            'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_dayOfWeek',
                                                                             'type': 'DateEncoder'},
-                                                   'timestamp_timeOfDay': { 'fieldname': u'timestamp',
-                                                                            'name': u'timestamp_timeOfDay',
+                                                   'timestamp_timeOfDay': { 'fieldname': 'timestamp',
+                                                                            'name': 'timestamp_timeOfDay',
                                                                             'timeOfDay': ( 21,
                                                                                            1),
                                                                             'type': 'DateEncoder'},
--- d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\base.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\base.py	(refactored)
@@ -99,7 +99,7 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-    'fields': [(u'c1', 'first'), (u'c0', 'first')],
+    'fields': [('c1', 'first'), ('c0', 'first')],
     'hours': 1,
     'microseconds': 0,
     'milliseconds': 0,
@@ -129,22 +129,22 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'c0_timeOfDay':     {   'fieldname': u'c0',
-    'name': u'c0_timeOfDay',
+                'c0_timeOfDay':     {   'fieldname': 'c0',
+    'name': 'c0_timeOfDay',
     'timeOfDay': (21, 1),
     'type': 'DateEncoder'},
-  u'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
-    'fieldname': u'c0',
-    'name': u'c0_dayOfWeek',
+  'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
+    'fieldname': 'c0',
+    'name': 'c0_dayOfWeek',
     'type': 'DateEncoder'},
-  u'c0_weekend':     {   'fieldname': u'c0',
-    'name': u'c0_weekend',
+  'c0_weekend':     {   'fieldname': 'c0',
+    'name': 'c0_weekend',
     'type': 'DateEncoder',
     'weekend': 21},
-  u'c1':     {   'clipInput': True,
-    'fieldname': u'c1',
+  'c1':     {   'clipInput': True,
+    'fieldname': 'c1',
     'n': 100,
-    'name': u'c1',
+    'name': 'c1',
     'type': 'AdaptiveScalarEncoder',
     'w': 21},
             },
@@ -348,13 +348,13 @@
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
   'dataset' : {   'aggregation': config['aggregationInfo'],
-        u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-        u'streams': [   {   u'columns': [u'c0', u'c1'],
-                            u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-                            u'source': 'file://%s' % (os.path.abspath(config['dataPath'])),
-                            u'types': [u'datetime', u'float']}],
-        u'timeField': u'c0',
-        u'version': 1},
+        'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+        'streams': [   {   'columns': ['c0', 'c1'],
+                            'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+                            'source': 'file://%s' % (os.path.abspath(config['dataPath'])),
+                            'types': ['datetime', 'float']}],
+        'timeField': 'c0',
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -367,12 +367,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'c1', u'predictionSteps': [24]},
+  "inferenceArgs":{'predictedField': 'c1', 'predictionSteps': [24]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
+    MetricSpec(field='c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\a\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\a\description.py	(refactored)
@@ -34,8 +34,8 @@
 
 # the sub-experiment configuration
 config ={
-  'aggregationInfo' : {'seconds': 0, 'fields': [(u'c1', 'first'), (u'c0', 'first')], 'months': 0, 'days': 0, 'years': 0, 'hours': 1, 'microseconds': 0, 'weeks': 0, 'minutes': 0, 'milliseconds': 0},
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'aggregationInfo' : {'seconds': 0, 'fields': [('c1', 'first'), ('c0', 'first')], 'months': 0, 'days': 0, 'years': 0, 'hours': 1, 'microseconds': 0, 'weeks': 0, 'minutes': 0, 'milliseconds': 0},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'dataPath': 'experiments/opfrunexperiment_test/checkpoints/data/a.csv',
 }
--- d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\a_plus_b\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\a_plus_b\description.py	(refactored)
@@ -34,8 +34,8 @@
 
 # the sub-experiment configuration
 config ={
-  'aggregationInfo' : {'seconds': 0, 'fields': [(u'c1', 'first'), (u'c0', 'first')], 'months': 0, 'days': 0, 'years': 0, 'hours': 1, 'microseconds': 0, 'weeks': 0, 'minutes': 0, 'milliseconds': 0},
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'aggregationInfo' : {'seconds': 0, 'fields': [('c1', 'first'), ('c0', 'first')], 'months': 0, 'days': 0, 'years': 0, 'hours': 1, 'microseconds': 0, 'weeks': 0, 'minutes': 0, 'milliseconds': 0},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'dataPath': 'experiments/opfrunexperiment_test/checkpoints/data/a_plus_b.csv',
 }
--- d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\b\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\checkpoints\b\description.py	(refactored)
@@ -34,8 +34,8 @@
 
 # the sub-experiment configuration
 config ={
-  'aggregationInfo' : {'seconds': 0, 'fields': [(u'c1', 'first'), (u'c0', 'first')], 'months': 0, 'days': 0, 'years': 0, 'hours': 1, 'microseconds': 0, 'weeks': 0, 'minutes': 0, 'milliseconds': 0},
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'aggregationInfo' : {'seconds': 0, 'fields': [('c1', 'first'), ('c0', 'first')], 'months': 0, 'days': 0, 'years': 0, 'hours': 1, 'microseconds': 0, 'weeks': 0, 'minutes': 0, 'milliseconds': 0},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'dataPath': 'experiments/opfrunexperiment_test/checkpoints/data/b.csv',
 
--- d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\simpleOPF\hotgym\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\simpleOPF\hotgym\description.py	(refactored)
@@ -385,7 +385,7 @@
       ],
 
       'metrics' : [
-        MetricSpec(field=u'consumption',
+        MetricSpec(field='consumption',
                    inferenceElement='prediction',
                    metric='aae',
                    params={'window': 200}),
--- d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\simpleOPF\hotgym_1hr_agg\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\simpleOPF\hotgym_1hr_agg\description.py	(refactored)
@@ -95,9 +95,9 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {
-    'fields': [   (u'timestamp', 'first'),
-                  (u'gym', 'first'),
-                  (u'consumption', 'sum')],
+    'fields': [   ('timestamp', 'first'),
+                  ('gym', 'first'),
+                  ('consumption', 'sum')],
     'days': 0,
     'hours': 1,
     'microseconds': 0,
@@ -128,22 +128,22 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'timestamp_timeOfDay':     {   'fieldname': u'timestamp',
-    'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay':     {   'fieldname': 'timestamp',
+    'name': 'timestamp_timeOfDay',
     'timeOfDay': (21, 1),
     'type': 'DateEncoder'},
-  u'timestamp_dayOfWeek':     {   'dayOfWeek': (21, 1),
-    'fieldname': u'timestamp',
-    'name': u'timestamp_dayOfWeek',
+  'timestamp_dayOfWeek':     {   'dayOfWeek': (21, 1),
+    'fieldname': 'timestamp',
+    'name': 'timestamp_dayOfWeek',
     'type': 'DateEncoder'},
-  u'timestamp_weekend':     {   'fieldname': u'timestamp',
-    'name': u'timestamp_weekend',
+  'timestamp_weekend':     {   'fieldname': 'timestamp',
+    'name': 'timestamp_weekend',
     'type': 'DateEncoder',
     'weekend': 21},
-  u'consumption':     {   'clipInput': True,
-    'fieldname': u'consumption',
+  'consumption':     {   'clipInput': True,
+    'fieldname': 'consumption',
     'n': 100,
-    'name': u'consumption',
+    'name': 'consumption',
     'type': 'AdaptiveScalarEncoder',
     'w': 21},
             },
@@ -159,7 +159,7 @@
             # Example for 1.5 days: sensorAutoReset = dict(days=1,hours=12),
             #
             # (value generated from SENSOR_AUTO_RESET)
-            'sensorAutoReset' : {   u'days': 0, u'hours': 0},
+            'sensorAutoReset' : {   'days': 0, 'hours': 0},
         },
 
         'spEnable': True,
@@ -350,12 +350,12 @@
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
   'dataset' : {   'aggregation': config['aggregationInfo'],
-        u'info': u'test_hotgym',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'hotGym.csv',
-                            u'last_record': 100,
-                            u'source': u'file://extra/hotgym/hotgym.csv'}],
-        u'version': 1},
+        'info': 'test_hotgym',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'hotGym.csv',
+                            'last_record': 100,
+                            'source': 'file://extra/hotgym/hotgym.csv'}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -368,13 +368,13 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'consumption', u'predictionSteps': [1, 5]},
+  "inferenceArgs":{'predictedField': 'consumption', 'predictionSteps': [1, 5]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'aae'}),
-    MetricSpec(field=u'consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'altMAPE'}),
+    MetricSpec(field='consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'aae'}),
+    MetricSpec(field='consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'altMAPE'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\simpleOPF\hotgym_no_agg\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\opfrunexperiment_test\simpleOPF\hotgym_no_agg\description.py	(refactored)
@@ -95,9 +95,9 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {
-    'fields': [   (u'timestamp', 'first'),
-                  (u'gym', 'first'),
-                  (u'consumption', 'sum')],
+    'fields': [   ('timestamp', 'first'),
+                  ('gym', 'first'),
+                  ('consumption', 'sum')],
     'days': 0,
     'hours': 0,
     'microseconds': 0,
@@ -128,22 +128,22 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'timestamp_timeOfDay':     {   'fieldname': u'timestamp',
-    'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay':     {   'fieldname': 'timestamp',
+    'name': 'timestamp_timeOfDay',
     'timeOfDay': (21, 1),
     'type': 'DateEncoder'},
-  u'timestamp_dayOfWeek':     {   'dayOfWeek': (21, 1),
-    'fieldname': u'timestamp',
-    'name': u'timestamp_dayOfWeek',
+  'timestamp_dayOfWeek':     {   'dayOfWeek': (21, 1),
+    'fieldname': 'timestamp',
+    'name': 'timestamp_dayOfWeek',
     'type': 'DateEncoder'},
-  u'timestamp_weekend':     {   'fieldname': u'timestamp',
-    'name': u'timestamp_weekend',
+  'timestamp_weekend':     {   'fieldname': 'timestamp',
+    'name': 'timestamp_weekend',
     'type': 'DateEncoder',
     'weekend': 21},
-  u'consumption':     {   'clipInput': True,
-    'fieldname': u'consumption',
+  'consumption':     {   'clipInput': True,
+    'fieldname': 'consumption',
     'n': 100,
-    'name': u'consumption',
+    'name': 'consumption',
     'type': 'AdaptiveScalarEncoder',
     'w': 21},
             },
@@ -159,7 +159,7 @@
             # Example for 1.5 days: sensorAutoReset = dict(days=1,hours=12),
             #
             # (value generated from SENSOR_AUTO_RESET)
-            'sensorAutoReset' : {   u'days': 0, u'hours': 0},
+            'sensorAutoReset' : {   'days': 0, 'hours': 0},
         },
 
         'spEnable': True,
@@ -350,12 +350,12 @@
   # Input stream specification per py/nupic/cluster/database/StreamDef.json.
   #
   'dataset' : {   'aggregation': config['aggregationInfo'],
-        u'info': u'test_hotgym',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'hotGym.csv',
-                            u'last_record': 100,
-                            u'source': u'file://extra/hotgym/hotgym.csv'}],
-        u'version': 1},
+        'info': 'test_hotgym',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'hotGym.csv',
+                            'last_record': 100,
+                            'source': 'file://extra/hotgym/hotgym.csv'}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -368,13 +368,13 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'consumption', u'predictionSteps': [1, 5]},
+  "inferenceArgs":{'predictedField': 'consumption', 'predictionSteps': [1, 5]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'aae'}),
-    MetricSpec(field=u'consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'altMAPE'}),
+    MetricSpec(field='consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'aae'}),
+    MetricSpec(field='consumption', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1, 5], 'errorMetric': 'altMAPE'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\examples\opf\experiments\params\EnsembleOnline.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\params\EnsembleOnline.py	(refactored)
@@ -45,9 +45,9 @@
         if command=='predict':
           self.index=self.index+1
           self.updateModelStats()
-          self.result_queue.put([(self.Scores[m], self.predictionStreams[m][-1], self.truth[self.index], m) for m in self.M.keys()])
+          self.result_queue.put([(self.Scores[m], self.predictionStreams[m][-1], self.truth[self.index], m) for m in list(self.M.keys())])
         if command=='getPredictionStreams':
-          self.result_queue.put(dict([(m, self.predictionStreams[m][:-windowSize]) for m in self.predictionStreams.keys()]))
+          self.result_queue.put(dict([(m, self.predictionStreams[m][:-windowSize]) for m in list(self.predictionStreams.keys())]))
         if command=='delete':
           delList=jobaux[1]
           for d in delList:
@@ -55,11 +55,11 @@
               del self.M[d]
               del self.Scores[d]
               del self.predictionStreams[d]
-              print 'deleted Model'+str(d)+" in process "+str(self.iden)
-          print "number of models remaining in "+str(self.iden)+": "+str(len(self.M))
+              print('deleted Model'+str(d)+" in process "+str(self.iden))
+          print("number of models remaining in "+str(self.iden)+": "+str(len(self.M)))
           self.result_queue.put(self.iden)
         if command=='getAAEs':
-          self.result_queue.put([(m, computeAAE(self.truth, self.predictionStreams[m],r ), self.getModelState(self.M[m]), self.M[m]['modelDescription']) for m in self.M.keys()])
+          self.result_queue.put([(m, computeAAE(self.truth, self.predictionStreams[m],r ), self.getModelState(self.M[m]), self.M[m]['modelDescription']) for m in list(self.M.keys())])
         if command=='addPSOVariants':
           for t in jobaux[1]:
             if(t[0]==self.iden):
@@ -77,7 +77,7 @@
               self.M[name]['v']=v
               self.Scores[name]=10000
               self.predictionStreams[name]=[0,]
-              print "added new model "+str(name)+" to process"+str(self.iden)
+              print("added new model "+str(name)+" to process"+str(self.iden))
 
 
 
@@ -118,7 +118,7 @@
 
   def updateModelStats(self):
     updatedTruth=False
-    for m in self.M.keys():
+    for m in list(self.M.keys()):
       truth, prediction=self.M[m]['client'].nextTruthPrediction(self.predictedField)
       if(not updatedTruth):
         self.truth.append(truth)
@@ -137,13 +137,13 @@
   if not median:
     for s in scores:
       if s[3]==currModel:
-        print [(score[0], score[3]) for score in scores]
+        print([(score[0], score[3]) for score in scores])
 
         return s[1], currModel
-    print [(s[0], s[3]) for s in scores], "switching voting Model!"
+    print([(s[0], s[3]) for s in scores], "switching voting Model!")
     return scores[0][1], scores[0][3]
   else:
-    print [(s[0], s[3]) for s in scores]
+    print([(s[0], s[3]) for s in scores])
     voters = sorted(scores, key=lambda t: t[1])
     for voter in voters:
       votes[voter[3]]=votes[voter[3]]+1
@@ -170,11 +170,11 @@
 def getModelDescriptionLists(numProcesses, experiment):
     config, control = helpers.loadExperiment(experiment)
     encodersList=getFieldPermutations(config, 'pounds')
-    ns=range(50, 140, 120)
+    ns=list(range(50, 140, 120))
     clAlphas=np.arange(0.01, 0.16, 0.104)
     synPermInactives=np.arange(0.01, 0.16, 0.105)
-    tpPamLengths=range(5, 8, 2)
-    tpSegmentActivations=range(13, 17, 12)
+    tpPamLengths=list(range(5, 8, 2))
+    tpSegmentActivations=list(range(13, 17, 12))
 
     if control['environment'] == 'opfExperiment':
       experimentTasks = control['tasks']
@@ -211,7 +211,7 @@
                 name=name+1
               #print modelInfo['modelConfig']['modelParams']['tmParams']
               #print modelInfo['modelConfig']['modelParams']['sensorParams']['encoders'][4]['n']
-    print "num Models"+str( len(ModelSetUpData))
+    print("num Models"+str( len(ModelSetUpData)))
 
     shuffle(ModelSetUpData)
     #print [ (m[1]['modelConfig']['modelParams']['tmParams']['pamLength'], m[1]['modelConfig']['modelParams']['sensorParams']['encoders']) for m in ModelSetUpData]
@@ -222,7 +222,7 @@
     """ Yield n successive chunks from l.
     """
     newn = int(1.0 * len(l) / n + 0.5)
-    for i in xrange(0, n-1):
+    for i in range(0, n-1):
         yield l[i*newn:i*newn+newn]
     yield l[n*newn-newn:]
 
@@ -233,14 +233,14 @@
 
 def getDuplicateList(streams, delta):
   delList=[]
-  keys=streams.keys()
+  keys=list(streams.keys())
   for key1 in keys:
     if key1 in streams:
-      for key2 in streams.keys():
+      for key2 in list(streams.keys()):
         if(key1 !=key2):
-          print 'comparing model'+str(key1)+" to "+str(key2)
+          print('comparing model'+str(key1)+" to "+str(key2))
           dist=sum([(a-b)**2 for a, b in zip(streams[key1], streams[key2])])
-          print dist
+          print(dist)
           if(dist<delta):
             delList.append(key2)
             del streams[key2]
@@ -266,9 +266,9 @@
     samples = np.arange(len(px))
     px = np.array(px) / (1.*sum(px))
     u = uniform(0, max(px))
-    for n in xrange(N):
+    for n in range(N):
         included = px>=u
-        choice = random.sample(range(np.sum(included)), 1)[0]
+        choice = random.sample(list(range(np.sum(included))), 1)[0]
         values[n] = samples[included][choice]
         u = uniform(0, px[included][choice])
     if x:
@@ -276,7 +276,7 @@
             x=np.array(x)
             values = x[values]
         else:
-            print "px and x are different lengths. Returning index locations for px."
+            print("px and x are different lengths. Returning index locations for px.")
 
     return values
 
@@ -284,12 +284,12 @@
 def getPSOVariants(modelInfos, votes, n):
   # get x, px lists for sampling
   norm=sum(votes.values())
-  xpx =[(m, float(votes[m])/norm) for m in votes.keys()]
+  xpx =[(m, float(votes[m])/norm) for m in list(votes.keys())]
   x,px = [[z[i] for z in xpx] for i in (0,1)]
   #sample form set of models
   variantIDs=slice_sampler(px, n, x)
-  print "variant IDS"
-  print variantIDs
+  print("variant IDS")
+  print(variantIDs)
   #best X
   x_best=modelInfos[0][2][0]
   # create PSO variates of models
@@ -298,14 +298,14 @@
     t=modelInfos[[i for i, v in enumerate(modelInfos) if v[0] == variantID][0]]
     x=t[2][0]
     v=t[2][1]
-    print "old x"
-    print x
+    print("old x")
+    print(x)
     modelDescriptionMod=copy.deepcopy(t[3])
     configmod=modelDescriptionMod['modelConfig']
     v=inertia*v+socRate*np.random.random_sample(len(v))*(x_best-x)
     x=x+v
-    print "new x"
-    print x
+    print("new x")
+    print(x)
     configmod['modelParams']['clParams']['alpha']=max(0.01, x[0])
     configmod['modelParams']['spParams']['synPermInactiveDec']=max(0.01, x[2])
     configmod['modelParams']['tmParams']['pamLength']=int(round(max(1, x[4])))
@@ -319,7 +319,7 @@
 
 def computeAAE(truth, predictions, windowSize):
   windowSize=min(windowSize, len(truth))
-  zipped=zip(truth[-windowSize:], predictions[-windowSize-1:])
+  zipped=list(zip(truth[-windowSize:], predictions[-windowSize-1:]))
   AAE=sum([abs(a - b) for a, b in zipped])/windowSize
   return AAE
 
@@ -336,7 +336,7 @@
     divisor=4
     ModelSetUpData=getModelDescriptionLists(divisor, './')
     num_processes=len(ModelSetUpData)
-    print num_processes
+    print(num_processes)
     work_queues=[]
     votes={}
     votingParameterStats={"tpSegmentActivationThreshold":[], "tpPamLength":[], "synPermInactiveDec":[], "clAlpha":[], "numBuckets":[]}
@@ -347,7 +347,7 @@
     workerName=0
     modelNameCount=0
     for modelData in ModelSetUpData:
-        print len(modelData)
+        print(len(modelData))
         modelNameCount+=len(modelData)
         work_queue= multiprocessing.Queue()
         work_queues.append(work_queue)
@@ -368,12 +368,12 @@
       for j in range(num_processes):
         subscore=result_queue.get()
         scores.extend(subscore)
-      print ""
-      print i
+      print("")
+      print(i)
       ensemblePrediction, currModel=getStableVote(scores, stableSize, votes, currModel)
       ensemblePredictions.append(ensemblePrediction)
       truth.append(scores[0][2])
-      print  computeAAE(truth,ensemblePredictions, windowSize), int(currModel)
+      print(computeAAE(truth,ensemblePredictions, windowSize), int(currModel))
       assert(result_queue.empty())
       if i%r==0 and i!=0: #refresh ensemble
         assert(result_queue.empty())
@@ -385,16 +385,16 @@
           AAEs.extend(subAAEs)
         AAEs=sorted(AAEs, key=lambda t: t[1])
         numToDelete=int(round(cutPercentage*len(AAEs)))
-        print "Single Model AAES"
-        print [(aae[0], aae[1]) for aae in AAEs]
-        print "Ensemble AAE"
-        print computeAAE(truth, ensemblePredictions, r)
+        print("Single Model AAES")
+        print([(aae[0], aae[1]) for aae in AAEs])
+        print("Ensemble AAE")
+        print(computeAAE(truth, ensemblePredictions, r))
         #add bottom models to delList
-        print "Vote counts"
-        print votes
+        print("Vote counts")
+        print(votes)
         delList=[t[0] for t in AAEs[-numToDelete:]]
-        print "delList"
-        print delList
+        print("delList")
+        print(delList)
         #find duplicate models(now unnecessary)
         #command('getPredictionStreams', work_queues, None)
         #streams={}
@@ -406,7 +406,7 @@
         command('delete', work_queues, delList)
         for iden in delList:
           del votes[iden]
-        print votes
+        print(votes)
         #wait for deletion to finish and collect processIndices for addition
         processIndices=[]
         for j in range(num_processes):
@@ -423,15 +423,15 @@
 
         command('addPSOVariants', work_queues, aux)
         #set votes to 0
-        for key in votes.keys():
+        for key in list(votes.keys()):
           votes[key]=0
 
 
 
 
-    print "AAE over full stream"
-    print computeAAE(truth, ensemblePredictions, len(truth))
-    print "AAE1000"
-    print computeAAE(truth, ensemblePredictions, 1000)
-
-
+    print("AAE over full stream")
+    print(computeAAE(truth, ensemblePredictions, len(truth)))
+    print("AAE1000")
+    print(computeAAE(truth, ensemblePredictions, 1000))
+
+
--- d:\nupic\src\python\python27\examples\opf\experiments\params\test_all.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\params\test_all.py	(refactored)
@@ -51,15 +51,15 @@
     metricSpecs = control['metrics']
 
     datasetPath = datasetURI[len("file://"):]
-    for i in xrange(1024, 2176, 128):
+    for i in range(1024, 2176, 128):
       #config['modelParams']['tmParams']['cellsPerColumn'] = 16
       config['modelParams']['tmParams']['columnCount'] = i
       config['modelParams']['spParams']['columnCount'] = i
-      print 'Running with 32 cells per column and %i columns.' % i
+      print('Running with 32 cells per column and %i columns.' % i)
       start = time.time()
       result = runOneExperiment(config, control['inferenceArgs'], metricSpecs,
                                 datasetPath)
-      print 'Total time: %d.' % (time.time() - start)
+      print('Total time: %d.' % (time.time() - start))
       pprint(result)
 
 
--- d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\make_datasets.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\make_datasets.py	(refactored)
@@ -52,7 +52,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('classification', 'string', ''), 
             ('field1', 'string', '')]  
   outFile = FileRecordStream(pathname, write=True, fields=fields)
@@ -66,7 +66,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for i in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   for seqIdx in seqIdxs:
@@ -98,7 +98,7 @@
   # Create the output file
   scriptDir = os.path.dirname(__file__)
   pathname = os.path.join(scriptDir, 'datasets', filename)
-  print "Creating %s..." % (pathname)
+  print("Creating %s..." % (pathname))
   fields = [('classification', 'float', ''), 
             ('field1', 'float', '')]  
   if includeRandom:
@@ -117,7 +117,7 @@
   # Write out the sequences in random order
   seqIdxs = []
   for i in range(numRepeats):
-    seqIdxs += range(numSequences)
+    seqIdxs += list(range(numSequences))
   random.shuffle(seqIdxs)
   
   for seqIdx in seqIdxs:
--- d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\run_exp_generator.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\run_exp_generator.py	(refactored)
@@ -92,14 +92,14 @@
   expGenArgs = ['--description=%s' % (json.dumps(searchDef)),
                 '--version=v2',
                 '--outDir=%s' % (options.outDir)]
-  print "Running ExpGenerator with the following arguments: ", expGenArgs
+  print("Running ExpGenerator with the following arguments: ", expGenArgs)
   expGenerator(expGenArgs)
 
 
   # Get the permutations file name
   permutationsFilename = os.path.join(options.outDir, 'permutations.py')
 
-  print "Successfully generated permutations file: %s" % (permutationsFilename)
+  print("Successfully generated permutations file: %s" % (permutationsFilename))
 
 
 
--- d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\base\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\base\description.py	(refactored)
@@ -124,17 +124,17 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'field1':     {
-                  'fieldname': u'field1',
+                'field1':     {
+                  'fieldname': 'field1',
                   'n': 121,
-                  'name': u'field1',
+                  'name': 'field1',
                   'type': 'SDRCategoryEncoder',
                   'w': 21},
-                u'classification':     {
+                'classification':     {
                   'classifierOnly': True,
-                  'fieldname': u'classification',
+                  'fieldname': 'classification',
                   'n': 121,
-                  'name': u'classification',
+                  'name': 'classification',
                   'type': 'SDRCategoryEncoder',
                   'w': 21},
             },
@@ -310,9 +310,9 @@
         },
 
         'anomalyParams': {
-          u'anomalyCacheRecords': None,
-          u'autoDetectThreshold': None,
-          u'autoDetectWaitRecords': None
+          'anomalyCacheRecords': None,
+          'autoDetectThreshold': None,
+          'autoDetectWaitRecords': None
         },
 
         'trainSPNetOnlyIfRequested': False,
@@ -352,11 +352,11 @@
 
   # Input stream specification per py/nupic/frameworks/opf/jsonschema/stream_def.json.
   #
-  'dataset' : {   u'info': u'testSpatialClassification',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'spatialClassification',
-                            u'source': config['dataSource']}],
-        u'version': 1},
+  'dataset' : {   'info': 'testSpatialClassification',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'spatialClassification',
+                            'source': config['dataSource']}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -369,7 +369,7 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'classification', u'predictionSteps': [0]},
+  "inferenceArgs":{'predictedField': 'classification', 'predictionSteps': [0]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
--- d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\scalar_0\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\scalar_0\description.py	(refactored)
@@ -37,22 +37,22 @@
       'encoders': {
         'field1': {
           'clipInput': True,
-          'fieldname': u'field1',
+          'fieldname': 'field1',
           'maxval': 0.1,
           'minval': 0.0,
           'n': 211,
-          'name': u'field1',
+          'name': 'field1',
           'type': 'ScalarEncoder',
           'w': 21
         },
         'classification': {
           'classifierOnly': True,
           'clipInput': True,
-          'fieldname': u'classification',
+          'fieldname': 'classification',
           'maxval': 1.0,
           'minval': 0.0,
           'n': 211,
-          'name': u'classification',
+          'name': 'classification',
           'type': 'ScalarEncoder',
           'w': 21
         },
--- d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\scalar_1\description.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\experiments\spatial_classification\scalar_1\description.py	(refactored)
@@ -37,22 +37,22 @@
       'encoders': {
         'field1': {
           'clipInput': True,
-          'fieldname': u'field1',
+          'fieldname': 'field1',
           'maxval': 5.0,
           'minval': 0.0,
           'n': 600,
-          'name': u'field1',
+          'name': 'field1',
           'type': 'ScalarEncoder',
           'w': 21
         },
         'classification': {
           'classifierOnly': True,
           'clipInput': True,
-          'fieldname': u'classification',
+          'fieldname': 'classification',
           'maxval': 50.0,
           'minval': 0.0,
           'n': 600,
-          'name': u'classification',
+          'name': 'classification',
           'type': 'ScalarEncoder',
           'w': 21
          },
--- d:\nupic\src\python\python27\examples\opf\simple_server\demo.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\simple_server\demo.py	(refactored)
@@ -69,14 +69,14 @@
 
     for i in range(_NUM_RECORDS):
       record = f.getNextRecord()
-      modelInput = dict(zip(headers, record))
+      modelInput = dict(list(zip(headers, record)))
       modelInput["consumption"] = float(modelInput["consumption"])
       modelInput["timestamp"] = modelInput["timestamp"].strftime("%m/%d/%y %H:%M")
 
       res = requests.post(
           "http://{server}:{port}/models/demo/run".format(server=server, port=port),
           json.dumps(modelInput))
-      print "result = %s" % res.text
+      print("result = %s" % res.text)
 
       isLast = i == _NUM_RECORDS
       if isLast:
--- d:\nupic\src\python\python27\examples\opf\simple_server\model_params.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\simple_server\model_params.py	(refactored)
@@ -55,20 +55,20 @@
 
             # Include the encoders we use
             'encoders': {
-                u'consumption':    {
-                    'fieldname': u'consumption',
+                'consumption':    {
+                    'fieldname': 'consumption',
                     'resolution': 0.88,
                     'seed': 1,
-                    'name': u'consumption',
+                    'name': 'consumption',
                     'type': 'RandomDistributedScalarEncoder',
                     },
 
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (21, 1),
                                            'type': 'DateEncoder'},
-                'timestamp_weekend': {   'fieldname': u'timestamp',
-                                         'name': u'timestamp_weekend',
+                'timestamp_weekend': {   'fieldname': 'timestamp',
+                                         'name': 'timestamp_weekend',
                                          'type': 'DateEncoder',
                                          'weekend': 21}
             },
@@ -238,9 +238,9 @@
             'implementation': 'py',
         },
 
-        'anomalyParams': { u'anomalyCacheRecords': None,
-                           u'autoDetectThreshold': None,
-                           u'autoDetectWaitRecords': 2184},
+        'anomalyParams': { 'anomalyCacheRecords': None,
+                           'autoDetectThreshold': None,
+                           'autoDetectWaitRecords': 2184},
 
         'trainSPNetOnlyIfRequested': False,
     },
--- d:\nupic\src\python\python27\examples\opf\tools\sp_plotter.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\tools\sp_plotter.py	(refactored)
@@ -64,8 +64,8 @@
       intOrigDist = int(origDist.sum()/2+0.1)
 
       if intDist < 2 and intOrigDist > 10:
-        print 'Elements %d,%d has very small SP distance: %d' % (i, j, intDist)
-        print 'Input elements distance is %d' % intOrigDist
+        print('Elements %d,%d has very small SP distance: %d' % (i, j, intDist))
+        print('Input elements distance is %d' % intOrigDist)
 
       x = int(PLOT_PRECISION*intDist/40.0)
       y = int(PLOT_PRECISION*intOrigDist/42.0)
@@ -102,7 +102,7 @@
 
   inputs = []
 
-  for _ in xrange(numRecords):
+  for _ in range(numRecords):
 
     input = np.zeros(elemSize, dtype=realDType)
     for _ in range(0,numSet):
@@ -123,9 +123,9 @@
   records to the same inputs list.
   """
   numInputs = len(inputs)
-  for i in xrange(numInputs):
+  for i in range(numInputs):
     input = inputs[i]
-    for j in xrange(len(input)-1):
+    for j in range(len(input)-1):
       if input[j] == 1 and input[j+1] == 0:
         newInput = copy.deepcopy(input)
         newInput[j] = 0
@@ -141,11 +141,11 @@
   """
   numInputs = len(inputs)
   skipOne = False
-  for i in xrange(numInputs):
+  for i in range(numInputs):
     input = inputs[i]
     numChanged = 0
     newInput = copy.deepcopy(input)
-    for j in xrange(len(input)-1):
+    for j in range(len(input)-1):
       if skipOne:
         skipOne = False
         continue
@@ -177,7 +177,7 @@
 
   runningIndex = -1
   numModsDone = 0
-  for i in xrange(inputWidth):
+  for i in range(inputWidth):
     if numModsDone >= changes:
       break
     if inputVal[i] == 1:
@@ -245,7 +245,7 @@
       appendInputWithNSimilarValues(inputs, 42)
 
     inputSize = len(inputs)
-    print 'Num random records = %d, inputs to process %d' % (numRecords, inputSize)
+    print('Num random records = %d, inputs to process %d' % (numRecords, inputSize))
 
     # Run a number of iterations, with learning on or off,
     # retrieve results from the last iteration only
@@ -255,8 +255,8 @@
     if doLearn:
       numIter = itr
 
-    for iter in xrange(numIter):
-      for i in xrange(inputSize):
+    for iter in range(numIter):
+      for i in range(inputSize):
         time.sleep(0.001)
         if iter == numIter - 1:
           # TODO: See https://github.com/numenta/nupic/issues/2072
@@ -348,7 +348,7 @@
 
   cleanPlot = False
 
-  for i in xrange(numRecords):
+  for i in range(numRecords):
     input1 = getRandomWithMods(inputs, 4)
     if i % 2 == 0:
       input2 = getRandomWithMods(inputs, 4)
@@ -481,7 +481,7 @@
   reader = csv.reader(file)
 
   for row in reader:
-    input = np.array(map(float, row), dtype=realDType)
+    input = np.array(list(map(float, row)), dtype=realDType)
     if len(input.nonzero()[0]) != numSet:
       continue
 
@@ -508,24 +508,24 @@
 
   doLearn = False
 
-  print 'Finished reading file, inputs/outputs to process =', len(inputs)
+  print('Finished reading file, inputs/outputs to process =', len(inputs))
 
   size = len(inputs)
 
-  for iter in xrange(100):
-
-    print 'Iteration', iter
+  for iter in range(100):
+
+    print('Iteration', iter)
 
     # Learn
     if iter != 0:
-      for learnRecs in xrange(pattern[0]):
+      for learnRecs in range(pattern[0]):
 
         # TODO: See https://github.com/numenta/nupic/issues/2072
         ind = np.random.random_integers(0, size-1, 1)[0]
         sp.compute(inputs[ind], learn=True, activeArray=outputs[ind])
 
     # Test
-    for _ in xrange(pattern[1]):
+    for _ in range(pattern[1]):
       rand1 = np.random.random_integers(0, size-1, 1)[0]
       rand2 = np.random.random_integers(0, size-1, 1)[0]
 
@@ -539,7 +539,7 @@
       intInDist = int(inDist.sum()/2+0.1)
 
       if intInDist != numSet or intOutDist != spSet:
-        print rand1, rand2, '-', intInDist, intOutDist
+        print(rand1, rand2, '-', intInDist, intOutDist)
 
       x = int(PLOT_PRECISION*intOutDist/spSet)
       y = int(PLOT_PRECISION*intInDist/numSet)
--- d:\nupic\src\python\python27\examples\opf\tools\testDiagnostics.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\tools\testDiagnostics.py	(refactored)
@@ -16,12 +16,12 @@
   numActive=len(np.nonzero(spOutput[0])[0])
   matrix = np.zeros([2*w+1,2*numActive+1])
     
-  for x in xrange(len(inputs)):
+  for x in range(len(inputs)):
     i = [_hammingDistance(inputs[x], z) for z in inputs[x:]]
     j = [_hammingDistance(spOutput[x], a) for a in spOutput[x:]]
     for p, q in zip(i,j):
       matrix[p,q]+=1
-    for y in xrange(len(matrix))  :
+    for y in range(len(matrix))  :
       matrix[y]=[max(10*x, 100) if (x<100 and x>0) else x for x in matrix[y]]
   
   cdict = {'red':((0.0,0.0,0.0),(0.01,0.7,0.5),(0.3,1.0,0.7),(1.0,1.0,1.0)),\
--- d:\nupic\src\python\python27\examples\opf\tools\MirrorImageViz\mirrorImageViz.py	(original)
+++ d:\nupic\src\python\python27\examples\opf\tools\MirrorImageViz\mirrorImageViz.py	(refactored)
@@ -51,12 +51,12 @@
   size = int(firstLine.pop(0))
   spOutput = np.zeros((len(lines),40))
   inputBits = np.zeros((len(lines),w))
-  print 'Total n:', size
-  print 'Total number of records in the file:', len(lines), '\n'
-  print 'w:', w
+  print('Total n:', size)
+  print('Total number of records in the file:', len(lines), '\n')
+  print('w:', w)
   
   count = 0
-  for x in xrange(len(lines)):
+  for x in range(len(lines)):
     inputSpace = []     #Encoded representation for each input 
     
     spBUout = [int(z) for z in lines[x].split(' ')]  
@@ -70,7 +70,7 @@
     inputBits[x]=input
     
     #Creating the encoding space 
-    for m in xrange(size):
+    for m in range(size):
       if m in tempInput:
         inputSpace.append(m)
       else:
@@ -94,7 +94,7 @@
 
   seen = []
   seen = (printOverlaps(coincs, coincs, seen))
-  print len(seen), 'sets of 40 cells'
+  print(len(seen), 'sets of 40 cells')
   seen = printOverlaps(reUsedCoincs, coincs, seen)
   
   Summ=[]
@@ -103,14 +103,14 @@
     for y in reUsedCoincs:
       c += len(z[1].intersection(y[1]))
     Summ.append(c)
-  print 'Sum: ', Summ
+  print('Sum: ', Summ)
   
-  for m in xrange(3):
+  for m in range(3):
     displayLimit = min(51, len(spOutput[m*200:]))
     if displayLimit>0:
       drawFile(dataset, np.zeros([len(inputBits[:(m+1)*displayLimit]),len(inputBits[:(m+1)*displayLimit])]), inputBits[:(m+1)*displayLimit], spOutput[:(m+1)*displayLimit], w, m+1)
     else: 
-      print 'No more records to display'
+      print('No more records to display')
   pyl.show()
   
 def drawFile(dataset, matrix, patterns, cells, w, fnum):
@@ -119,14 +119,14 @@
   score=0
   count = 0
   assert len(patterns)==len(cells)
-  for p in xrange(len(patterns)-1):
+  for p in range(len(patterns)-1):
     matrix[p+1:,p] = [len(set(patterns[p]).intersection(set(q)))*100/w for q in patterns[p+1:]]
     matrix[p,p+1:] = [len(set(cells[p]).intersection(set(r)))*5/2 for r in cells[p+1:]]
     
     score += sum(abs(np.array(matrix[p+1:,p])-np.array(matrix[p,p+1:])))
     count += len(matrix[p+1:,p])
   
-  print 'Score', score/count
+  print('Score', score/count)
   
   fig = pyl.figure(figsize = (10,10), num = fnum)
   pyl.matshow(matrix, fignum = fnum)
@@ -151,17 +151,17 @@
     closestInputs = []
     closestCells = []
     if len(seen)>0:
-      inputOverlap = max([len(seen[m][1].intersection(y[4])) for m in xrange(len(seen))])
-      cellOverlap = max([len(seen[m][0].intersection(y[1])) for m in xrange(len(seen))])
-      for m in xrange( len(seen) ):
+      inputOverlap = max([len(seen[m][1].intersection(y[4])) for m in range(len(seen))])
+      cellOverlap = max([len(seen[m][0].intersection(y[1])) for m in range(len(seen))])
+      for m in range( len(seen) ):
         if len(seen[m][1].intersection(y[4]))==inputOverlap:
           closestInputs.append(seen[m][2])
         if len(seen[m][0].intersection(y[1]))==cellOverlap:
           closestCells.append(seen[m][2])
     seen.append((y[1], y[4], y[0]))
         
-    print 'Pattern',y[0]+1,':',' '.join(str(len(z[1].intersection(y[1]))).rjust(2) for z in coincs),'input overlap:', inputOverlap, ';', len(closestInputs), 'closest encodings:',','.join(str(m+1) for m in closestInputs).ljust(15), \
-    'cell overlap:', cellOverlap, ';', len(closestCells), 'closest set(s):',','.join(str(m+1) for m in closestCells)
+    print('Pattern',y[0]+1,':',' '.join(str(len(z[1].intersection(y[1]))).rjust(2) for z in coincs),'input overlap:', inputOverlap, ';', len(closestInputs), 'closest encodings:',','.join(str(m+1) for m in closestInputs).ljust(15), \
+    'cell overlap:', cellOverlap, ';', len(closestCells), 'closest set(s):',','.join(str(m+1) for m in closestCells))
   
   return seen
 
--- d:\nupic\src\python\python27\examples\prediction\category_prediction\run.py	(original)
+++ d:\nupic\src\python\python27\examples\prediction\category_prediction\run.py	(refactored)
@@ -72,8 +72,8 @@
       "verbosity" : 0,
       "encoders": {
         "token": {
-          "fieldname": u"token",
-          "name": u"token",
+          "fieldname": "token",
+          "name": "token",
           "type": "CategoryEncoder",
           "categoryList": list(set(map(str.strip, open("tokens.txt").readlines()))),
           "w": 21
@@ -140,4 +140,4 @@
     modelInput = {"token": token}
     result = shifter.shift(model.run(modelInput))
     if result.inferences["multiStepPredictions"][1]:
-      out.writerow([token] + [y for x in sorted(result.inferences["multiStepPredictions"][1].items(), key=itemgetter(1)) for y in x])
+      out.writerow([token] + [y for x in sorted(list(result.inferences["multiStepPredictions"][1].items()), key=itemgetter(1)) for y in x])
--- d:\nupic\src\python\python27\examples\prediction\experiments\confidenceTest\base\description.py	(original)
+++ d:\nupic\src\python\python27\examples\prediction\experiments\confidenceTest\base\description.py	(refactored)
@@ -209,7 +209,7 @@
   encoder = MultiEncoder()  
   if 'filenameCategory' in datasets:
     categories = [x.strip() for x in 
-                              open(datasets['filenameCategory']).xreadlines()]
+                              open(datasets['filenameCategory'])]
   else:
     categories = [chr(x+ord('a')) for x in range(26)]
 
@@ -328,7 +328,7 @@
 
   if config['trainTP']:
     description['tpTrain'] = []
-    for i in xrange(config['trainTPRepeats']):
+    for i in range(config['trainTPRepeats']):
       stepDict = dict(name='step_%d' % (i), 
                       setup=sensorRewind, 
                       iterationCount=config['iterationCountTrain'],
--- d:\nupic\src\python\python27\examples\prediction\experiments\dutyCycle\base\description.py	(original)
+++ d:\nupic\src\python\python27\examples\prediction\experiments\dutyCycle\base\description.py	(refactored)
@@ -87,7 +87,7 @@
   config['encodingOnBitsB'] = config['encodingOnBitsA']
 
 if config['tpActivationThresholds'] is None:
-  config['tpActivationThresholds'] = range(8, config['spNumActivePerInhArea']+1)
+  config['tpActivationThresholds'] = list(range(8, config['spNumActivePerInhArea']+1))
 
 
 def getBaseDatasets():
@@ -131,13 +131,13 @@
 
   if (not os.path.exists(trainingFilename)) or \
      (testingFilename is not None and not os.path.exists(testingFilename)):
-    print "====================================================================="
-    print "Creating data set..."
+    print("=====================================================================")
+    print("Creating data set...")
 
     # Create the pool of A values
-    aValues = range(config['numAValues'])
+    aValues = list(range(config['numAValues']))
     # Create the pool of B values, allowing for unequal distribution
-    bValues = range(config['numBValues'])
+    bValues = list(range(config['numBValues']))
 
     # Pick a random A and B value
     random.seed(42)
@@ -147,15 +147,15 @@
       return (a, b)
 
     if config['b0Likelihood'] is not None:
-      print "In the B dataset, there is a %d%% chance of getting a B value of 0" \
-            % (int(100 * config['b0Likelihood']))
+      print("In the B dataset, there is a %d%% chance of getting a B value of 0" \
+            % (int(100 * config['b0Likelihood'])))
       # likelihood of B0 is: (numB0) / (numB0 + numBvalues)
       # solving for numB0 = numBValues / (1 - likelihood)
       numB0Values = int(round(len(bValues) / (1.0 - config['b0Likelihood'])))
       bValues.extend([0]*numB0Values)   # 90% chance of getting first B value
     else:
-      print "All values in B are equally likely"
-    print
+      print("All values in B are equally likely")
+    print()
 
     # -----------------------------------------------------------------------
     fields = [('fieldA', 'int', ''), ('fieldB', 'int', '')]
@@ -167,13 +167,13 @@
         testSet.add(generateSample())
       testList = list(testSet)
       testList.sort()
-      print "These (A,B) combinations are reserved for the test set:", testList
-      print
+      print("These (A,B) combinations are reserved for the test set:", testList)
+      print()
 
       # Write out the test set
-      print "Creating test set: %s..." % (testingFilename)
-      print "Contains %d unique combinations of A and B chosen from the %d possible" \
-              % (testSetSize, numUnique)
+      print("Creating test set: %s..." % (testingFilename))
+      print("Contains %d unique combinations of A and B chosen from the %d possible" \
+              % (testSetSize, numUnique))
       with File(testingFilename, fields=fields) as o:
         numSamples = 0
         while numSamples < config['iterationCount']:
@@ -183,18 +183,18 @@
             #print >>fd, "%d, %d" % (sample[0], sample[1])
 
             numSamples += 1
-      print
+      print()
 
     # ------------------------------------------------------------------------
     # Write out the training set
-    print "Creating training set: %s..." % (trainingFilename)
+    print("Creating training set: %s..." % (trainingFilename))
     if len(testSet) > 0:
-      print "Contains %d samples, chosen from %d of the possible %d combinations " \
+      print("Contains %d samples, chosen from %d of the possible %d combinations " \
             "that are not in the test set" % (config['iterationCount'],
-            numUnique - testSetSize, numUnique)
+            numUnique - testSetSize, numUnique))
     else:
-      print "Contains %d samples" % (config['iterationCount'])
-    print
+      print("Contains %d samples" % (config['iterationCount']))
+    print()
     with FileRecordStream(trainingFilename, write=True, fields=fields) as o:
       numSamples = 0
       while numSamples < config['iterationCount']:
@@ -220,7 +220,7 @@
   elif config['encodingFieldStyleA'] == 'sdr':
     encoder.addEncoder('fieldA', SDRCategoryEncoder(w=config['encodingOnBitsA'],
                         n=config['encodingFieldWidthA'],
-                        categoryList=range(config['numAValues']), name='fieldA'))
+                        categoryList=list(range(config['numAValues'])), name='fieldA'))
   else:
     assert False
 
@@ -232,7 +232,7 @@
   elif config['encodingFieldStyleB'] == 'sdr':
     encoder.addEncoder('fieldB', SDRCategoryEncoder(w=config['encodingOnBitsB'],
                       n=config['encodingFieldWidthB'],
-                      categoryList=range(config['numBValues']), name='fieldB'))
+                      categoryList=list(range(config['numBValues'])), name='fieldB'))
   else:
     assert False
 
--- d:\nupic\src\python\python27\examples\prediction\experiments\dutyCycle\base\permutationsActiveCount.py	(original)
+++ d:\nupic\src\python\python27\examples\prediction\experiments\dutyCycle\base\permutationsActiveCount.py	(refactored)
@@ -4,7 +4,7 @@
                 spPeriodicStats     = [0],
                 #spCoincCount = [200, 300, 400, 500],
                 spNumActivePerInhArea = [9, 11, 13, 15, 17],
-                tpActivationThresholds = [range(8,18)], 
+                tpActivationThresholds = [list(range(8,18))], 
                 #spSynPermInactiveDec = [0.005, 0.01, 0.02, 0.04],
                 )
 
--- d:\nupic\src\python\python27\examples\prediction\experiments\dutyCycle\base\permutationsEncoder.py	(original)
+++ d:\nupic\src\python\python27\examples\prediction\experiments\dutyCycle\base\permutationsEncoder.py	(refactored)
@@ -6,8 +6,8 @@
                 encodingFieldWidthA = [256],
                 encodingFieldWidthB = [256],
                 
-                encodingOnBitsA     = [5, 7, 9] + range(11, 40, 4) + range(43, 100, 8),
-                encodingOnBitsB     = [5, 7, 9] + range(11, 40, 4) + range(43, 100, 8),
+                encodingOnBitsA     = [5, 7, 9] + list(range(11, 40, 4)) + list(range(43, 100, 8)),
+                encodingOnBitsB     = [5, 7, 9] + list(range(11, 40, 4)) + list(range(43, 100, 8)),
                 
                 numAValues          = [25],
                 numBValues          = [25],
--- d:\nupic\src\python\python27\examples\prediction\experiments\generated_data\description.py	(original)
+++ d:\nupic\src\python\python27\examples\prediction\experiments\generated_data\description.py	(refactored)
@@ -42,7 +42,7 @@
   # Dictionary keys must match the names in the multiencoder
   d["date"] = t
   d["amount"] = amount
-  for i in xrange(info['nRandomFields']):
+  for i in range(info['nRandomFields']):
     d["random%d" %i] = random.randint(0, info['randomFieldWidth'])
   return d
 
@@ -57,7 +57,7 @@
   encoder = MultiEncoder()
   encoder.addEncoder("date", DateEncoder(timeOfDay=3))
   encoder.addEncoder("amount", LogEncoder(name="amount", maxval=1000))
-  for i in xrange(0, nRandomFields):
+  for i in range(0, nRandomFields):
     s = ScalarEncoder(name="scalar", minval=0, maxval=randomFieldWidth, resolution=1, w=3)
     encoder.addEncoder("random%d" % i, s)
 
@@ -108,7 +108,7 @@
   nodeParams.update(otherParams)
 
   def mySetupCallback(experiment):
-    print "Setup function called"
+    print("Setup function called")
 
   description = dict(
     options = dict(
--- d:\nupic\src\python\python27\examples\sp\hello_sp.py	(original)
+++ d:\nupic\src\python\python27\examples\sp\hello_sp.py	(refactored)
@@ -68,7 +68,7 @@
   def createInput(self):
     """create a random input vector"""
 
-    print "-" * 70 + "Creating a random input vector" + "-" * 70
+    print("-" * 70 + "Creating a random input vector" + "-" * 70)
 
     #clear the inputArray to zero before creating a new input vector
     self.inputArray[0:] = 0
@@ -81,12 +81,12 @@
   def run(self):
     """Run the spatial pooler with the input vector"""
 
-    print "-" * 80 + "Computing the SDR" + "-" * 80
+    print("-" * 80 + "Computing the SDR" + "-" * 80)
 
     #activeArray[column]=1 if column is active after spatial pooling
     self.sp.compute(self.inputArray, True, self.activeArray)
 
-    print self.activeArray.nonzero()
+    print(self.activeArray.nonzero())
 
 
   def addNoise(self, noiseLevel):
@@ -113,9 +113,9 @@
 example = Example((32, 32), (64, 64))
 
 # Lesson 1
-print "\n \nFollowing columns represent the SDR"
-print "Different set of columns each time since we randomize the input"
-print "Lesson - different input vectors give different SDRs\n\n"
+print("\n \nFollowing columns represent the SDR")
+print("Different set of columns each time since we randomize the input")
+print("Lesson - different input vectors give different SDRs\n\n")
 
 # Trying random vectors
 for i in range(3):
@@ -123,29 +123,29 @@
   example.run()
 
 # Lesson 2
-print "\n\nIdentical SDRs because we give identical inputs"
-print "Lesson - identical inputs give identical SDRs\n\n"
+print("\n\nIdentical SDRs because we give identical inputs")
+print("Lesson - identical inputs give identical SDRs\n\n")
 
-print "-" * 75 + "Using identical input vectors" + "-" * 75
+print("-" * 75 + "Using identical input vectors" + "-" * 75)
 
 # Trying identical vectors
 for i in range(2):
   example.run()
 
 # Lesson 3
-print "\n\nNow we are changing the input vector slightly."
-print "We change a small percentage of 1s to 0s and 0s to 1s."
-print "The resulting SDRs are similar, but not identical to the original SDR"
-print "Lesson - Similar input vectors give similar SDRs\n\n"
+print("\n\nNow we are changing the input vector slightly.")
+print("We change a small percentage of 1s to 0s and 0s to 1s.")
+print("The resulting SDRs are similar, but not identical to the original SDR")
+print("Lesson - Similar input vectors give similar SDRs\n\n")
 
 # Adding 10% noise to the input vector
 # Notice how the output SDR hardly changes at all
-print "-" * 75 + "After adding 10% noise to the input vector" + "-" * 75
+print("-" * 75 + "After adding 10% noise to the input vector" + "-" * 75)
 example.addNoise(0.1)
 example.run()
 
 # Adding another 20% noise to the already modified input vector
 # The output SDR should differ considerably from that of the previous output
-print "-" * 75 + "After adding another 20% noise to the input vector" + "-" * 75
+print("-" * 75 + "After adding another 20% noise to the input vector" + "-" * 75)
 example.addNoise(0.2)
 example.run()
--- d:\nupic\src\python\python27\examples\sp\sp_tutorial.py	(original)
+++ d:\nupic\src\python\python27\examples\sp\sp_tutorial.py	(refactored)
@@ -136,16 +136,16 @@
 for i in activeCols.nonzero():
   activeColsScores.append(overlaps[i])
 
-print ""
-print "---------------------------------"
-print "Figure 1 shows an histogram of the overlap scores"
-print "from all the columns in the spatial pooler, as well as the"
-print "overlap scores of those columns that were selected to build a"
-print "sparse representation of the input (shown in green)."
-print "The SP chooses 2% of the columns with the largest overlap score"
-print "to make such sparse representation."
-print "---------------------------------"
-print ""
+print("")
+print("---------------------------------")
+print("Figure 1 shows an histogram of the overlap scores")
+print("from all the columns in the spatial pooler, as well as the")
+print("overlap scores of those columns that were selected to build a")
+print("sparse representation of the input (shown in green).")
+print("The SP chooses 2% of the columns with the largest overlap score")
+print("to make such sparse representation.")
+print("---------------------------------")
+print("")
 
 bins = np.linspace(min(overlaps), max(overlaps), 28)
 plt.hist(overlaps, bins, alpha=0.5, label='All cols')
@@ -183,17 +183,17 @@
   x.append(noiseLevel)
   y.append(percentOverlap(inputX1, inputX2, inputSize))
 
-print ""
-print "---------------------------------"
-print "Figure 2 shows the input overlap between 2 identical binary"
-print "vectors in function of the noise applied to one of them."
-print "0 noise level means that the vector remains the same, whereas"
-print "1 means that the vector is the logical negation of the original"
-print "vector."
-print "The relationship between overlap and noise level is practically"
-print "linear and monotonically decreasing."
-print "---------------------------------"
-print ""
+print("")
+print("---------------------------------")
+print("Figure 2 shows the input overlap between 2 identical binary")
+print("vectors in function of the noise applied to one of them.")
+print("0 noise level means that the vector remains the same, whereas")
+print("1 means that the vector is the logical negation of the original")
+print("vector.")
+print("The relationship between overlap and noise level is practically")
+print("linear and monotonically decreasing.")
+print("---------------------------------")
+print("")
 
 plt.plot(x, y)
 plt.xlabel("Noise level")
@@ -229,16 +229,16 @@
   x.append(percentOverlap(inputX1, inputX2, inputSize))
   y.append(percentOverlap(outputX1, outputX2, columnNumber))
 
-print ""
-print "---------------------------------"
-print "Figure 3 shows the output overlap between two sparse representations"
-print "in function of their input overlap. Starting from two identical binary vectors"
-print "(which yield the same active columns) we add noise two one of them"
-print "feed it to the SP, and estimate the output overlap between the two"
-print "representations in terms of the common active columns between them."
-print "As expected, as the input overlap decrease, so does the output overlap."
-print "---------------------------------"
-print ""
+print("")
+print("---------------------------------")
+print("Figure 3 shows the output overlap between two sparse representations")
+print("in function of their input overlap. Starting from two identical binary vectors")
+print("(which yield the same active columns) we add noise two one of them")
+print("feed it to the SP, and estimate the output overlap between the two")
+print("representations in terms of the common active columns between them.")
+print("As expected, as the input overlap decrease, so does the output overlap.")
+print("---------------------------------")
+print("")
 
 plt.plot(x, y)
 plt.xlabel("Input overlap")
@@ -295,16 +295,16 @@
   x.append(percentOverlap(inputVectors[0][:], inputVectorsCorrupted[0][:], inputSize))
   y.append(percentOverlap(outputColumns[0][:], outputColumnsCorrupted[0][:], columnNumber))
 
-print ""
-print "---------------------------------"
-print "How robust is the SP to noise after learning?"
-print "Figure 4 shows again the output overlap between two binary vectors in function"
-print "of their input overlap. After training, the SP exhibits more robustness to noise"
-print "in its input, resulting in a -almost- sigmoid curve. This implies that even if a"
-print "previous input is presented again with a certain amount of noise its sparse"
-print "representation still resembles its original."
-print "---------------------------------"
-print ""
+print("")
+print("---------------------------------")
+print("How robust is the SP to noise after learning?")
+print("Figure 4 shows again the output overlap between two binary vectors in function")
+print("of their input overlap. After training, the SP exhibits more robustness to noise")
+print("in its input, resulting in a -almost- sigmoid curve. This implies that even if a")
+print("previous input is presented again with a certain amount of noise its sparse")
+print("representation still resembles its original.")
+print("---------------------------------")
+print("")
 
 plt.plot(x, y)
 plt.xlabel("Input overlap")
@@ -313,10 +313,10 @@
 plt.savefig("figure_4")
 plt.close()
 
-print ""
-print "+++++++++++++++++++++++++++++++++++++++++++++++++++"
-print " All images generated by this script will be saved"
-print " in your current working directory."
-print "+++++++++++++++++++++++++++++++++++++++++++++++++++"
-print ""
-
+print("")
+print("+++++++++++++++++++++++++++++++++++++++++++++++++++")
+print(" All images generated by this script will be saved")
+print(" in your current working directory.")
+print("+++++++++++++++++++++++++++++++++++++++++++++++++++")
+print("")
+
--- d:\nupic\src\python\python27\examples\swarm\test_db.py	(original)
+++ d:\nupic\src\python\python27\examples\swarm\test_db.py	(refactored)
@@ -91,28 +91,28 @@
   passwd = Configuration.get("nupic.cluster.database.passwd")
 
 
-  print "This script will validate that your MySQL is setup correctly for "
-  print "NuPIC. MySQL is required for NuPIC swarming. The settings are"
-  print "defined in a configuration file found in  "
-  print "$NUPIC/src/nupic/support/nupic-default.xml Out of the box those "
-  print "settings contain MySQL's default access credentials."
-  print
-  print "The nupic-default.xml can be duplicated to define user specific "
-  print "changes calling the copied file "
-  print "$NUPIC/src/nupic/support/nupic-site.xml Refer to the "
-  print "nupic-default.xml for additional instructions."
-  print
-  print "Defaults: localhost, 3306, root, no password"
-  print
-  print "Retrieved the following NuPIC configuration using: ", fileused
-  print "    host   :    ", host
-  print "    port   :    ", port
-  print "    user   :    ", user
-  print "    passwd :    ", "*" * len(passwd)
+  print("This script will validate that your MySQL is setup correctly for ")
+  print("NuPIC. MySQL is required for NuPIC swarming. The settings are")
+  print("defined in a configuration file found in  ")
+  print("$NUPIC/src/nupic/support/nupic-default.xml Out of the box those ")
+  print("settings contain MySQL's default access credentials.")
+  print()
+  print("The nupic-default.xml can be duplicated to define user specific ")
+  print("changes calling the copied file ")
+  print("$NUPIC/src/nupic/support/nupic-site.xml Refer to the ")
+  print("nupic-default.xml for additional instructions.")
+  print()
+  print("Defaults: localhost, 3306, root, no password")
+  print()
+  print("Retrieved the following NuPIC configuration using: ", fileused)
+  print("    host   :    ", host)
+  print("    port   :    ", port)
+  print("    user   :    ", user)
+  print("    passwd :    ", "*" * len(passwd))
 
 
   testDbConnection(host, port, user, passwd)
-  print "Connection successful!!"
+  print("Connection successful!!")
 
 if __name__ == "__main__":
   dbValidator()
--- d:\nupic\src\python\python27\examples\tm\hello_tm.py	(original)
+++ d:\nupic\src\python\python27\examples\tm\hello_tm.py	(refactored)
@@ -20,7 +20,7 @@
 # ----------------------------------------------------------------------
 
 
-print """
+print("""
 This program shows how to access the Temporal Memory directly by demonstrating
 how to create a TM instance, train it with vectors, get predictions, and
 inspect the state.
@@ -34,11 +34,11 @@
 
 PLEASE READ THROUGH THE CODE COMMENTS - THEY EXPLAIN THE OUTPUT IN DETAIL
 
-"""
+""")
 
 # Can't live without numpy
 import numpy
-from itertools import izip as zip, count
+from itertools import count
 
 from nupic.algorithms.temporal_memory import TemporalMemory as TM
 
@@ -94,10 +94,10 @@
 
     # The following print statements can be ignored.
     # Useful for tracing internal states
-    print("active cells " + str(tm.getActiveCells()))
-    print("predictive cells " + str(tm.getPredictiveCells()))
-    print("winner cells " + str(tm.getWinnerCells()))
-    print("# of active segments " + str(tm.connections.numSegments()))
+    print(("active cells " + str(tm.getActiveCells())))
+    print(("predictive cells " + str(tm.getPredictiveCells())))
+    print(("winner cells " + str(tm.getWinnerCells())))
+    print(("# of active segments " + str(tm.connections.numSegments())))
 
   # The reset command tells the TM that a sequence just ended and essentially
   # zeros out all the states. It is not strictly necessary but it's a bit
@@ -110,8 +110,8 @@
 # Step 3: send the same sequence of vectors and look at predictions made by
 # temporal memory
 for j in range(5):
-  print "\n\n--------","ABCDE"[j],"-----------"
-  print "Raw input vector : " + formatRow(x[j])
+  print("\n\n--------","ABCDE"[j],"-----------")
+  print("Raw input vector : " + formatRow(x[j]))
   activeColumns = set([i for i, j in zip(count(), x[j]) if j == 1])
   # Send each vector to the TM, with learning turned off
   tm.compute(activeColumns, learn = False)
@@ -122,12 +122,12 @@
   # What you should notice is that the columns where active state is 1
   # represent the SDR for the current input pattern and the columns where
   # predicted state is 1 represent the SDR for the next expected pattern
-  print "\nAll the active and predicted cells:"
+  print("\nAll the active and predicted cells:")
 
-  print("active cells " + str(tm.getActiveCells()))
-  print("predictive cells " + str(tm.getPredictiveCells()))
-  print("winner cells " + str(tm.getWinnerCells()))
-  print("# of active segments " + str(tm.connections.numSegments()))
+  print(("active cells " + str(tm.getActiveCells())))
+  print(("predictive cells " + str(tm.getPredictiveCells())))
+  print(("winner cells " + str(tm.getWinnerCells())))
+  print(("# of active segments " + str(tm.connections.numSegments())))
 
   activeColumnsIndeces = [tm.columnForCell(i) for i in tm.getActiveCells()]
   predictedColumnIndeces = [tm.columnForCell(i) for i in tm.getPredictiveCells()]
@@ -144,8 +144,8 @@
   # For convenience the cells are grouped
   # 10 at a time. When there are multiple cells per column the printout
   # is arranged so the cells in a column are stacked together
-  print "Active columns:    " + formatRow(actColStr)
-  print "Predicted columns: " + formatRow(predColStr)
+  print("Active columns:    " + formatRow(actColStr))
+  print("Predicted columns: " + formatRow(predColStr))
 
   # predictedCells[c][i] represents the state of the i'th cell in the c'th
   # column. To see if a column is predicted, we can simply take the OR
--- d:\nupic\src\python\python27\examples\tm\tm_constant_test.py	(original)
+++ d:\nupic\src\python\python27\examples\tm\tm_constant_test.py	(refactored)
@@ -36,7 +36,7 @@
 
 def _printOneTrainingVector(x):
   "Print a single vector succinctly."
-  print ''.join('1' if k != 0 else '.' for k in x)
+  print(''.join('1' if k != 0 else '.' for k in x))
 
 def _getSimplePatterns(numOnes, numPatterns):
   """Very simple patterns. Each pattern has numOnes consecutive
@@ -45,7 +45,7 @@
 
   numCols = numOnes * numPatterns
   p = []
-  for i in xrange(numPatterns):
+  for i in range(numPatterns):
     x = np.zeros(numCols, dtype='float32')
     x[i*numOnes:(i + 1)*numOnes] = 1
     p.append(x)
@@ -116,10 +116,10 @@
           tm.learn(seq)
         tm.reset()
 
-    print "Learning completed"
+    print("Learning completed")
 
     # Infer
-    print "Running inference"
+    print("Running inference")
 
     tm.collectStats = True
     for seq in trainingSet[0:5]:
@@ -128,20 +128,20 @@
       for _ in range(10):
         tm.infer(seq)
         if VERBOSITY > 1 :
-          print
+          print()
           _printOneTrainingVector(seq)
           tm.printStates(False, False)
-          print
-          print
+          print()
+          print()
       if VERBOSITY > 1:
-        print tm.getStats()
+        print(tm.getStats())
 
       # Ensure our predictions are accurate for each sequence
       self.assertGreater(tm.getStats()['predictionScoreAvg2'], 0.8)
-      print ("tm.getStats()['predictionScoreAvg2'] = ",
-             tm.getStats()['predictionScoreAvg2'])
+      print(("tm.getStats()['predictionScoreAvg2'] = ",
+             tm.getStats()['predictionScoreAvg2']))
 
-    print "TMConstantTest ok"
+    print("TMConstantTest ok")
 
   def testCppTmBasic(self):
     self._basicTest(self.cppTm)
--- d:\nupic\src\python\python27\examples\tm\tm_high_order.py	(original)
+++ d:\nupic\src\python\python27\examples\tm\tm_high_order.py	(refactored)
@@ -82,15 +82,15 @@
   """   
   for k in range(6):
     tm.reset()
-    print "--- " + "ABCDXY"[k] + " ---"
+    print("--- " + "ABCDXY"[k] + " ---")
     tm.compute(set(seqT[k][:].nonzero()[0].tolist()), learn=False)
     activeColumnsIndices = [tm.columnForCell(i) for i in tm.getActiveCells()]
     predictedColumnIndices = [tm.columnForCell(i) for i in tm.getPredictiveCells()]  
     currentColumns = [1 if i in activeColumnsIndices else 0 for i in range(tm.numberOfColumns())]
     predictedColumns = [1 if i in predictedColumnIndices else 0 for i in range(tm.numberOfColumns())]
-    print("Active cols: " + str(np.nonzero(currentColumns)[0]))
-    print("Predicted cols: " + str(np.nonzero(predictedColumns)[0]))
-    print ""
+    print(("Active cols: " + str(np.nonzero(currentColumns)[0])))
+    print(("Predicted cols: " + str(np.nonzero(predictedColumns)[0])))
+    print("")
 
     
 def trainTM(sequence, timeSteps, noiseLevel):
@@ -164,15 +164,15 @@
 # PART 1. Feed the TM with sequence "ABCD". The TM will eventually learn
 # the pattern and it's prediction accuracy will go to 1.0 (except in-between sequences
 # where the TM doesn't output any prediction)
-print ""
-print "-"*50
-print "Part 1. We present the sequence ABCD to the TM. The TM will eventually"
-print "will learn the sequence and predict the upcoming characters. This can be"
-print "measured by the prediction accuracy in Fig 1."
-print "N.B. In-between sequences the accuracy is 0.0 as the TM does not output"
-print "any prediction."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Part 1. We present the sequence ABCD to the TM. The TM will eventually")
+print("will learn the sequence and predict the upcoming characters. This can be")
+print("measured by the prediction accuracy in Fig 1.")
+print("N.B. In-between sequences the accuracy is 0.0 as the TM does not output")
+print("any prediction.")
+print("-"*50)
+print("")
 
 x = []
 y = []
@@ -186,30 +186,30 @@
 plt.savefig("figure_1")
 plt.close()
 
-print ""
-print "-"*50
-print "Once the TM has learned the sequence ABCD, we will present the individual"
-print "characters to the TM to know its prediction. The TM outputs the columns"
-print "that become active upon the presentation of a particular character as well"
-print "as the columns predicted in the next time step. Here, you should see that"
-print "A predicts B, B predicts C, C predicts D, and D does not output any"
-print "prediction."
-print "N.B. Here, we are presenting individual characters, that is, a character"
-print "deprived of context in a sequence. There is no prediction for characters"
-print "X and Y as we have not presented them to the TM in any sequence."
-print "-"*50
-print ""
-
-showPredictions()
-
-print ""
-print "-"*50
-print "Part 2. We now present the sequence XBCY to the TM. As expected, the accuracy will"
-print "drop until the TM learns the new sequence (Fig 2). What will be the prediction of"
-print "the TM if presented with the sequence BC? This would depend on what character"
-print "anteceding B. This is an important feature of high-order sequences."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Once the TM has learned the sequence ABCD, we will present the individual")
+print("characters to the TM to know its prediction. The TM outputs the columns")
+print("that become active upon the presentation of a particular character as well")
+print("as the columns predicted in the next time step. Here, you should see that")
+print("A predicts B, B predicts C, C predicts D, and D does not output any")
+print("prediction.")
+print("N.B. Here, we are presenting individual characters, that is, a character")
+print("deprived of context in a sequence. There is no prediction for characters")
+print("X and Y as we have not presented them to the TM in any sequence.")
+print("-"*50)
+print("")
+
+showPredictions()
+
+print("")
+print("-"*50)
+print("Part 2. We now present the sequence XBCY to the TM. As expected, the accuracy will")
+print("drop until the TM learns the new sequence (Fig 2). What will be the prediction of")
+print("the TM if presented with the sequence BC? This would depend on what character")
+print("anteceding B. This is an important feature of high-order sequences.")
+print("-"*50)
+print("")
 
 x = []
 y = []
@@ -227,30 +227,30 @@
 plt.savefig("figure_2")
 plt.close()
 
-print ""
-print "-"*50
-print "We will present again each of the characters individually to the TM, that is,"
-print "not within any of the two sequences. When presented with character A the TM"
-print "predicts B, B predicts C, but this time C outputs a simultaneous prediction of"
-print "both D and Y. In order to disambiguate, the TM would require to know if the"
-print "preceding characters were AB or XB. When presented with character X the TM"
-print "predicts B, whereas Y and D yield no prediction."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("We will present again each of the characters individually to the TM, that is,")
+print("not within any of the two sequences. When presented with character A the TM")
+print("predicts B, B predicts C, but this time C outputs a simultaneous prediction of")
+print("both D and Y. In order to disambiguate, the TM would require to know if the")
+print("preceding characters were AB or XB. When presented with character X the TM")
+print("predicts B, whereas Y and D yield no prediction.")
+print("-"*50)
+print("")
 
 showPredictions()
 
 # PART 3. Now we will present noisy inputs to the TM. We will add noise to the sequence XBCY
 # by corrupting 30% of its bits. We would like to see how the TM responds in the presence of
 # noise and how it recovers from it.
-print ""
-print "-"*50
-print "Part 3. We will add noise to the sequence XBCY by corrupting 30% of the bits in the vectors"
-print "encoding each character. We would expect to see a decrease in prediction accuracy as the"
-print "TM is unable to learn the random noise in the input (Fig 3). However, this decrease is not"
-print "significant."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Part 3. We will add noise to the sequence XBCY by corrupting 30% of the bits in the vectors")
+print("encoding each character. We would expect to see a decrease in prediction accuracy as the")
+print("TM is unable to learn the random noise in the input (Fig 3). However, this decrease is not")
+print("significant.")
+print("-"*50)
+print("")
 
 x = []
 y = []
@@ -264,41 +264,41 @@
 plt.savefig("figure_3")
 plt.close()
 
-print ""
-print "-"*50
-print "Let's have a look again at the output of the TM when presented with noisy"
-print "input (30%). Here, the noise is low that the TM is not affected by it,"
-print "which would be the case if we saw 'noisy' columns being predicted when"
-print "presented with individual characters. Thus, we could say that the TM exhibits"
-print "resilience to noise in its input."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Let's have a look again at the output of the TM when presented with noisy")
+print("input (30%). Here, the noise is low that the TM is not affected by it,")
+print("which would be the case if we saw 'noisy' columns being predicted when")
+print("presented with individual characters. Thus, we could say that the TM exhibits")
+print("resilience to noise in its input.")
+print("-"*50)
+print("")
 
 showPredictions()
 
 # Let's corrupt the sequence more by adding 50% of noise to each of its characters.
 # Here, we would expect to see some 'noisy' columns being predicted when the TM is
 # presented with the individual characters.
-print ""
-print "-"*50
-print "Now, we will set noise to be 50% of the bits in the characters X, B, C, and Y."
-print "As expected, the accuracy will decrease (Fig 5) and 'noisy' columns will be"
-print "predicted by the TM."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Now, we will set noise to be 50% of the bits in the characters X, B, C, and Y.")
+print("As expected, the accuracy will decrease (Fig 5) and 'noisy' columns will be")
+print("predicted by the TM.")
+print("-"*50)
+print("")
 
 x = []
 y = []
 trainTM(seq2, timeSteps=50, noiseLevel=0.5)
 
-print ""
-print "-"*50
-print "Let's have a look again at the output of the TM when presented with noisy"
-print "input. The prediction of some characters (eg. X) now includes columns that"
-print "are not related to any other character. This is because the TM tried to learn"
-print "the noise in the input patterns."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Let's have a look again at the output of the TM when presented with noisy")
+print("input. The prediction of some characters (eg. X) now includes columns that")
+print("are not related to any other character. This is because the TM tried to learn")
+print("the noise in the input patterns.")
+print("-"*50)
+print("")
 
 showPredictions()
 
@@ -318,13 +318,13 @@
 y = []
 trainTM(seq2, timeSteps=10, noiseLevel=0.0)
 
-print ""
-print "-"*50
-print "After presenting the original sequence XBCY to the TM, we would expect to see"
-print "the predicted noisy columns from the previous step disappear. We will verify that"
-print "by presenting the individual characters to the TM."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("After presenting the original sequence XBCY to the TM, we would expect to see")
+print("the predicted noisy columns from the previous step disappear. We will verify that")
+print("by presenting the individual characters to the TM.")
+print("-"*50)
+print("")
 
 showPredictions()
 
@@ -341,25 +341,25 @@
 # Let's corrupt the sequence even more and add 90% of noise to each of its characters.
 # Here, we would expect to see even more of a decrease in accuracy along with more 'noisy'
 # columns being predicted.
-print ""
-print "-"*50
-print "We will add more noise to the characters in the sequence XBCY. This time we will"
-print "corrupt 90% of its contents. As expected, the accuracy will decrease (Fig 6) and"
-print "'noisy' columns will be predicted by the TM."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("We will add more noise to the characters in the sequence XBCY. This time we will")
+print("corrupt 90% of its contents. As expected, the accuracy will decrease (Fig 6) and")
+print("'noisy' columns will be predicted by the TM.")
+print("-"*50)
+print("")
 
 x = []
 y = []
 trainTM(seq2, timeSteps=50, noiseLevel=0.9)
 
-print ""
-print "-"*50
-print "Next, we will have a look at the output of the TM when presented with the"
-print "individual characters of the sequence. As before, we see 'noisy' predicted"
-print "columns emerging as a result of the TM trying to learn the noise."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Next, we will have a look at the output of the TM when presented with the")
+print("individual characters of the sequence. As before, we see 'noisy' predicted")
+print("columns emerging as a result of the TM trying to learn the noise.")
+print("-"*50)
+print("")
 
 showPredictions()
 
@@ -394,15 +394,15 @@
 
 # The TM restores its prediction accuracy and it can be seen when presented with the individual characters.
 # There's no noisy columns being predicted.
-print ""
-print "-"*50
-print "After presenting noisy input to the TM, we present the original sequence in"
-print "order to make it re-learn XBCY. We verify that this was achieved by presenting"
-print "the TM with the individual characters and observing its output. Again, we can"
-print "see that the 'noisy' columns are not being predicted anymore, and that the"
-print "prediction accuracy goes back to 1.0 when the sequence is presented (Fig 7)."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("After presenting noisy input to the TM, we present the original sequence in")
+print("order to make it re-learn XBCY. We verify that this was achieved by presenting")
+print("the TM with the individual characters and observing its output. Again, we can")
+print("see that the 'noisy' columns are not being predicted anymore, and that the")
+print("prediction accuracy goes back to 1.0 when the sequence is presented (Fig 7).")
+print("-"*50)
+print("")
 
 showPredictions()
 
@@ -414,16 +414,16 @@
 # TM sees ABCDXBCY, another quarter it sees XBCYXBCY, and the last quarter it saw
 # XBCYABCD, then the TM would exhibit simultaneous predictions for characters D, Y
 # and C.
-print ""
-print "-"*50
-print "Part 4. We will present both sequences ABCD and XBCY randomly to the TM."
-print "Here, we might observe simultaneous predictions occurring when the TM is"
-print "presented with characters D, Y, and C. For this purpose we will use a"
-print "blank TM"
-print "NB. Here we will not reset the TM after presenting each sequence with the"
-print "purpose of making the TM learn different predictions for D and Y."
-print "-"*50
-print ""
+print("")
+print("-"*50)
+print("Part 4. We will present both sequences ABCD and XBCY randomly to the TM.")
+print("Here, we might observe simultaneous predictions occurring when the TM is")
+print("presented with characters D, Y, and C. For this purpose we will use a")
+print("blank TM")
+print("NB. Here we will not reset the TM after presenting each sequence with the")
+print("purpose of making the TM learn different predictions for D and Y.")
+print("-"*50)
+print("")
 
 tm = TM(columnDimensions = (2048,),
   cellsPerColumn=8,
@@ -445,25 +445,25 @@
     else:
       tm.compute(set(seq2[k][:].nonzero()[0].tolist()), learn=True)
 
-print ""
-print "-"*50
-print "We now have a look at the output of the TM when presented with the individual"
-print "characters A, B, C, D, X, and Y. We might observe simultaneous predictions when"
-print "presented with character D (predicting A and X), character Y (predicting A and X),"
-print "and when presented with character C (predicting D and Y)."
-print "N.B. Due to the stochasticity of this script, we might not observe simultaneous"
-print "predictions in *all* the aforementioned characters."
-print "-"*50
-print ""
-
-showPredictions()
-
-print ""
-print "-*"*25
-print "Scroll up to see the development of this simple"
-print "tutorial. Also open the source file to see more"
-print "comments regarding each part of the script."
-print "All images generated by this script will be saved"
-print "in your current working directory."
-print "-*"*25
-print ""
+print("")
+print("-"*50)
+print("We now have a look at the output of the TM when presented with the individual")
+print("characters A, B, C, D, X, and Y. We might observe simultaneous predictions when")
+print("presented with character D (predicting A and X), character Y (predicting A and X),")
+print("and when presented with character C (predicting D and Y).")
+print("N.B. Due to the stochasticity of this script, we might not observe simultaneous")
+print("predictions in *all* the aforementioned characters.")
+print("-"*50)
+print("")
+
+showPredictions()
+
+print("")
+print("-*"*25)
+print("Scroll up to see the development of this simple")
+print("tutorial. Also open the source file to see more")
+print("comments regarding each part of the script.")
+print("All images generated by this script will be saved")
+print("in your current working directory.")
+print("-*"*25)
+print("")
--- d:\nupic\src\python\python27\examples\tm\tm_overlapping_sequences.py	(original)
+++ d:\nupic\src\python\python27\examples\tm\tm_overlapping_sequences.py	(refactored)
@@ -65,13 +65,13 @@
 
 def printOneTrainingVector(x):
   "Print a single vector succinctly."
-  print ''.join('1' if k != 0 else '.' for k in x)
+  print(''.join('1' if k != 0 else '.' for k in x))
 
 
 
 def printAllTrainingSequences(trainingSequences, upTo = 99999):
   for i,trainingSequence in enumerate(trainingSequences):
-    print "============= Sequence",i,"================="
+    print("============= Sequence",i,"=================")
     for j,pattern in enumerate(trainingSequence):
       printOneTrainingVector(pattern)
 
@@ -97,7 +97,7 @@
   numCols = numNewBitsInEachPattern * numPatterns + patternOverlap
 
   p = []
-  for i in xrange(numPatterns):
+  for i in range(numPatterns):
     x = numpy.zeros(numCols, dtype='float32')
 
     startBit = i*numNewBitsInEachPattern
@@ -150,16 +150,16 @@
   # Create the training sequences
   trainingSequences = []
 
-  uniquePatternIndices = range(numSharedElements, numPatterns)
-  for _ in xrange(numSequences):
+  uniquePatternIndices = list(range(numSharedElements, numPatterns))
+  for _ in range(numSequences):
     sequence = []
 
     # pattern indices [0 ... numSharedElements-1] are reserved for the shared
     #  middle
-    sharedPatternIndices = range(numSharedElements)
+    sharedPatternIndices = list(range(numSharedElements))
 
     # Build up the sequence
-    for j in xrange(seqLen):
+    for j in range(seqLen):
       if j in sharedElements:
         patIdx = sharedPatternIndices.pop(0)
       else:
@@ -170,7 +170,7 @@
 
 
   if VERBOSITY >= 3:
-    print "\nTraining sequences"
+    print("\nTraining sequences")
     printAllTrainingSequences(trainingSequences)
 
   return (numCols, trainingSequences)
@@ -212,13 +212,13 @@
   # -----------------------------------------------------------------------
   # Create the training sequences
   trainingSequences = []
-  for _ in xrange(numSequences):
+  for _ in range(numSequences):
 
     # Build it up from patterns
     sequence = []
     length = random.choice(seqLen)
-    for _ in xrange(length):
-      patIdx = random.choice(xrange(numPatterns))
+    for _ in range(length):
+      patIdx = random.choice(range(numPatterns))
       sequence.append(patterns[patIdx])
 
     # Put it in
@@ -226,7 +226,7 @@
 
 
   if VERBOSITY >= 3:
-    print "\nTraining sequences"
+    print("\nTraining sequences")
     printAllTrainingSequences(trainingSequences)
 
   return (numCols, trainingSequences)
@@ -266,7 +266,7 @@
 
   if includeCPP:
     if VERBOSITY >= 2:
-      print "Creating BacktrackingTMCPP instance"
+      print("Creating BacktrackingTMCPP instance")
 
     cpp_tm = BacktrackingTMCPP(numberOfCols = numCols, cellsPerColumn = cellsPerCol,
                                initialPerm = initialPerm, connectedPerm = connectedPerm,
@@ -290,7 +290,7 @@
 
   if includePy:
     if VERBOSITY >= 2:
-      print "Creating PY TM instance"
+      print("Creating PY TM instance")
 
     py_tm = BacktrackingTM(numberOfCols = numCols,
                            cellsPerColumn = cellsPerCol,
@@ -331,7 +331,7 @@
   if len(tms) > 2:
     raise "Not implemented for more than 2 TMs"
 
-  same = fdrutils.tmDiff2(tms.values(), verbosity=VERBOSITY)
+  same = fdrutils.tmDiff2(list(tms.values()), verbosity=VERBOSITY)
   assert(same)
   return
 
@@ -364,23 +364,23 @@
 
   # First TM instance is used by default for verbose printing of input values,
   #  etc.
-  firstTM = tms.values()[0]
+  firstTM = list(tms.values())[0]
 
   assertNoTMDiffs(tms)
 
   # =====================================================================
   # Loop through the training set nTrainRepetitions times
   # ==========================================================================
-  for trainingNum in xrange(nTrainRepetitions):
+  for trainingNum in range(nTrainRepetitions):
     if VERBOSITY >= 2:
-      print "\n##############################################################"
-      print "################# Training round #%d of %d #################" \
-                % (trainingNum, nTrainRepetitions)
-      for (name,tm) in tms.iteritems():
-        print "TM parameters for %s: " % (name)
-        print "---------------------"
+      print("\n##############################################################")
+      print("################# Training round #%d of %d #################" \
+                % (trainingNum, nTrainRepetitions))
+      for (name,tm) in tms.items():
+        print("TM parameters for %s: " % (name))
+        print("---------------------")
         tm.printParameters()
-        print
+        print()
 
     # ======================================================================
     # Loop through the sequences in the training set
@@ -389,11 +389,11 @@
       numTimeSteps = len(trainingSequence)
 
       if VERBOSITY >= 2:
-        print "\n================= Sequence #%d of %d ================" \
-                  % (sequenceNum, numSequences)
+        print("\n================= Sequence #%d of %d ================" \
+                  % (sequenceNum, numSequences))
 
       if doResets:
-        for tm in tms.itervalues():
+        for tm in tms.values():
           tm.reset()
 
       # --------------------------------------------------------------------
@@ -402,62 +402,62 @@
 
         # Print Verbose info about this element
         if VERBOSITY >= 2:
-          print
+          print()
           if VERBOSITY >= 3:
-            print "------------------------------------------------------------"
-          print "--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
-                  % (sequenceNum, numSequences, t, numTimeSteps)
+            print("------------------------------------------------------------")
+          print("--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
+                  % (sequenceNum, numSequences, t, numTimeSteps))
           firstTM.printInput(x)
-          print "input nzs:", x.nonzero()
+          print("input nzs:", x.nonzero())
 
         # Train in this element
         x = numpy.array(x).astype('float32')
-        for tm in tms.itervalues():
+        for tm in tms.values():
           tm.learn(x, enableInference=True)
 
         # Print the input and output states
         if VERBOSITY >= 3:
-          for (name,tm) in tms.iteritems():
-            print "I/O states of %s TM:" % (name)
-            print "-------------------------------------",
+          for (name,tm) in tms.items():
+            print("I/O states of %s TM:" % (name))
+            print("-------------------------------------", end=' ')
             tm.printStates(printPrevious = (VERBOSITY >= 5))
-            print
+            print()
 
         assertNoTMDiffs(tms)
 
         # Print out number of columns that weren't predicted
         if VERBOSITY >= 2:
-          for (name,tm) in tms.iteritems():
+          for (name,tm) in tms.items():
             stats = tm.getStats()
-            print "# of unpredicted columns for %s TM: %d of %d" \
-                % (name, stats['curMissing'], x.sum())
+            print("# of unpredicted columns for %s TM: %d of %d" \
+                % (name, stats['curMissing'], x.sum()))
             numBurstingCols = tm.infActiveState['t'].min(axis=1).sum()
-            print "# of bursting columns for %s TM: %d of %d" \
-                % (name, numBurstingCols, x.sum())
+            print("# of bursting columns for %s TM: %d of %d" \
+                % (name, numBurstingCols, x.sum()))
 
 
       # Print the trained cells
       if VERBOSITY >= 4:
-        print "Sequence %d finished." % (sequenceNum)
-        for (name,tm) in tms.iteritems():
-          print "All cells of %s TM:" % (name)
-          print "-------------------------------------",
+        print("Sequence %d finished." % (sequenceNum))
+        for (name,tm) in tms.items():
+          print("All cells of %s TM:" % (name))
+          print("-------------------------------------", end=' ')
           tm.printCells()
-          print
+          print()
 
     # --------------------------------------------------------------------
     # Done training all sequences in this round, print the total number of
     #  missing, extra columns and make sure it's the same among the TMs
     if VERBOSITY >= 2:
-      print
+      print()
     prevResult = None
-    for (name,tm) in tms.iteritems():
+    for (name,tm) in tms.items():
       stats = tm.getStats()
       if VERBOSITY >= 1:
-        print "Stats for %s TM over all sequences for training round #%d of %d:" \
-                % (name, trainingNum, nTrainRepetitions)
-        print "   total missing:", stats['totalMissing']
-        print "   total extra:", stats['totalExtra']
+        print("Stats for %s TM over all sequences for training round #%d of %d:" \
+                % (name, trainingNum, nTrainRepetitions))
+        print("   total missing:", stats['totalMissing'])
+        print("   total extra:", stats['totalExtra'])
 
       if prevResult is None:
         prevResult = (stats['totalMissing'], stats['totalExtra'])
@@ -471,9 +471,9 @@
   # =====================================================================
   # Finish up learning
   if VERBOSITY >= 3:
-    print "Calling trim segments"
+    print("Calling trim segments")
   prevResult = None
-  for tm in tms.itervalues():
+  for tm in tms.values():
     nSegsRemoved, nSynsRemoved = tm.trimSegments()
     if prevResult is None:
       prevResult = (nSegsRemoved, nSynsRemoved)
@@ -484,22 +484,22 @@
   assertNoTMDiffs(tms)
 
   if VERBOSITY >= 4:
-    print "Training completed. Complete state:"
-    for (name,tm) in tms.iteritems():
-      print "%s:" % (name)
+    print("Training completed. Complete state:")
+    for (name,tm) in tms.items():
+      print("%s:" % (name))
       tm.printCells()
-      print
+      print()
 
 
   # ==========================================================================
   # Infer
   # ==========================================================================
   if VERBOSITY >= 2:
-    print "\n##############################################################"
-    print "########################## Inference #########################"
+    print("\n##############################################################")
+    print("########################## Inference #########################")
 
   # Reset stats in all TMs
-  for tm in tms.itervalues():
+  for tm in tms.values():
     tm.resetStats()
 
   # -------------------------------------------------------------------
@@ -510,12 +510,12 @@
 
     # Identify this sequence
     if VERBOSITY >= 2:
-      print "\n================= Sequence %d of %d ================" \
-                % (sequenceNum, numSequences)
+      print("\n================= Sequence %d of %d ================" \
+                % (sequenceNum, numSequences))
 
     # Send in the rest
     if doResets:
-      for tm in tms.itervalues():
+      for tm in tms.values():
         tm.reset()
 
     # -------------------------------------------------------------------
@@ -524,66 +524,66 @@
 
       # Print verbose info about this element
       if VERBOSITY >= 2:
-        print
+        print()
         if VERBOSITY >= 3:
-          print "------------------------------------------------------------"
-        print "--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
-                % (sequenceNum, numSequences, t, numTimeSteps)
+          print("------------------------------------------------------------")
+        print("--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
+                % (sequenceNum, numSequences, t, numTimeSteps))
         firstTM.printInput(x)
-        print "input nzs:", x.nonzero()
+        print("input nzs:", x.nonzero())
 
       # Infer on this element
-      for tm in tms.itervalues():
+      for tm in tms.values():
         tm.infer(x)
 
       assertNoTMDiffs(tms)
 
       # Print out number of columns that weren't predicted
       if VERBOSITY >= 2:
-        for (name,tm) in tms.iteritems():
+        for (name,tm) in tms.items():
           stats = tm.getStats()
-          print "# of unpredicted columns for %s TM: %d of %d" \
-              % (name, stats['curMissing'], x.sum())
+          print("# of unpredicted columns for %s TM: %d of %d" \
+              % (name, stats['curMissing'], x.sum()))
 
       # Debug print of internal state
       if VERBOSITY >= 3:
-        for (name,tm) in tms.iteritems():
-          print "I/O states of %s TM:" % (name)
-          print "-------------------------------------",
+        for (name,tm) in tms.items():
+          print("I/O states of %s TM:" % (name))
+          print("-------------------------------------", end=' ')
           tm.printStates(printPrevious = (VERBOSITY >= 5),
                          printLearnState = False)
-          print
+          print()
 
     # Done with this sequence
     # Debug print of all stats of the TMs
     if VERBOSITY >= 4:
-      print
-      for (name,tm) in tms.iteritems():
-        print "Interim internal stats for %s TM:" % (name)
-        print "---------------------------------"
+      print()
+      for (name,tm) in tms.items():
+        print("Interim internal stats for %s TM:" % (name))
+        print("---------------------------------")
         pprint.pprint(tm.getStats())
-        print
+        print()
 
 
   if VERBOSITY >= 2:
-    print "\n##############################################################"
-    print "####################### Inference Done #######################"
+    print("\n##############################################################")
+    print("####################### Inference Done #######################")
 
   # Get the overall stats for each TM and return them
   tmStats = dict()
-  for (name,tm) in tms.iteritems():
+  for (name,tm) in tms.items():
     tmStats[name] = stats = tm.getStats()
     if VERBOSITY >= 2:
-      print "Stats for %s TM over all sequences:" % (name)
-      print "   total missing:", stats['totalMissing']
-      print "   total extra:", stats['totalExtra']
-
-  for (name,tm) in tms.iteritems():
+      print("Stats for %s TM over all sequences:" % (name))
+      print("   total missing:", stats['totalMissing'])
+      print("   total extra:", stats['totalExtra'])
+
+  for (name,tm) in tms.items():
     if VERBOSITY >= 3:
-      print "\nAll internal stats for %s TM:" % (name)
-      print "-------------------------------------",
+      print("\nAll internal stats for %s TM:" % (name))
+      print("-------------------------------------", end=' ')
       pprint.pprint(tmStats[name])
-      print
+      print()
 
   return tmStats
 
@@ -632,16 +632,16 @@
 
   # -----------------------------------------------------------------------
   # Make sure there are the expected number of missing predictions
-  for (name, stats) in tmStats.iteritems():
-    print "Detected %d missing predictions overall during inference" \
-              % (stats['totalMissing'])
+  for (name, stats) in tmStats.items():
+    print("Detected %d missing predictions overall during inference" \
+              % (stats['totalMissing']))
     if expMissingMin is not None and stats['totalMissing'] < expMissingMin:
-      print "FAILURE: Expected at least %d total missing but got %d" \
-          % (expMissingMin, stats['totalMissing'])
+      print("FAILURE: Expected at least %d total missing but got %d" \
+          % (expMissingMin, stats['totalMissing']))
       assert False
     if expMissingMax is not None and stats['totalMissing'] > expMissingMax:
-      print "FAILURE: Expected at most %d total missing but got %d" \
-          % (expMissingMax, stats['totalMissing'])
+      print("FAILURE: Expected at most %d total missing but got %d" \
+          % (expMissingMax, stats['totalMissing']))
       assert False
 
 
@@ -689,13 +689,13 @@
     # ================================================================
     # Run various configs
     # No PAM, with 3 repetitions, still missing predictions
-    print "\nRunning without PAM, 3 repetitions of the training data..."
+    print("\nRunning without PAM, 3 repetitions of the training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=20,
                                expMissingMax=None, pamLength=1,
                                nTrainRepetitions=3))
 
     # With PAM, with only 3 repetitions, 0 missing predictions
-    print "\nRunning with PAM, 3 repetitions of the training data..."
+    print("\nRunning with PAM, 3 repetitions of the training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=0,
                                expMissingMax=0, pamLength=5,
                                nTrainRepetitions=3))
@@ -740,13 +740,13 @@
     # Run various configs
     # No PAM, requires 40 repetitions
     # No PAM, with 10 repetitions, still missing predictions
-    print "\nRunning without PAM, 10 repetitions of the training data..."
+    print("\nRunning without PAM, 10 repetitions of the training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=10,
                                expMissingMax=None, pamLength=1,
                                nTrainRepetitions=10))
 
     # With PAM, with only 10 repetitions, 0 missing predictions
-    print "\nRunning with PAM, 10 repetitions of the training data..."
+    print("\nRunning with PAM, 10 repetitions of the training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=0,
                                expMissingMax=0, pamLength=6,
                                nTrainRepetitions=10))
@@ -796,13 +796,13 @@
     # ================================================================
     # Run various configs
     # No PAM, with 10 repetitions, still missing predictions
-    print "\nRunning without PAM, 10 repetitions of the training data..."
+    print("\nRunning without PAM, 10 repetitions of the training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=10,
                                expMissingMax=None, pamLength=1,
                                nTrainRepetitions=10))
 
     # With PAM, with only 10 repetitions, 0 missing predictions
-    print "\nRunning with PAM, 10 repetitions of the training data..."
+    print("\nRunning with PAM, 10 repetitions of the training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=0,
                                expMissingMax=0, pamLength=6,
                                nTrainRepetitions=10))
@@ -856,29 +856,29 @@
     # Run various configs
     # Fast mode, no PAM
     # Fast mode, with PAM
-    print "\nRunning without PAM, fast learning, 2 repetitions of the " \
-          "training data..."
+    print("\nRunning without PAM, fast learning, 2 repetitions of the " \
+          "training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=50,
                                expMissingMax=None, pamLength=1,
                                nTrainRepetitions=2))
 
     # Fast mode, with PAM
-    print "\nRunning with PAM, fast learning, 2 repetitions of the " \
-          "training data..."
+    print("\nRunning with PAM, fast learning, 2 repetitions of the " \
+          "training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=0,
                                expMissingMax=0, pamLength=5,
                                nTrainRepetitions=2))
 
     # Slow mode, no PAM
-    print "\nRunning without PAM, slow learning, 8 repetitions of the " \
-          "training data..."
+    print("\nRunning without PAM, slow learning, 8 repetitions of the " \
+          "training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=1,
                                expMissingMax=None, initialPerm=0.31,
                                pamLength=1, nTrainRepetitions=8))
 
     # Fast mode, with PAM
-    print "\nRunning with PAM, slow learning, 8 repetitions of the " \
-          "training data..."
+    print("\nRunning with PAM, slow learning, 8 repetitions of the " \
+          "training data...")
     self.assertTrue(testConfig(baseParams=baseParams, expMissingMin=0,
                                expMissingMax=0, initialPerm=0.31, pamLength=5,
                                nTrainRepetitions=8))
@@ -907,9 +907,9 @@
   random.seed(SEED)
 
   if not INCLUDE_CPP_TM:
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
-    print "!!  WARNING: C++ TM testing is DISABLED until it can be updated."
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
+    print("!!  WARNING: C++ TM testing is DISABLED until it can be updated.")
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
 
   # Form the command line for the unit test framework.
   args = [sys.argv[0]] + args
--- d:\nupic\src\python\python27\examples\tm\tm_segment_learning.py	(original)
+++ d:\nupic\src\python\python27\examples\tm\tm_segment_learning.py	(refactored)
@@ -80,13 +80,13 @@
 
   def _printOneTrainingVector(self, x):
     """Print a single vector succinctly."""
-    print ''.join('1' if k != 0 else '.' for k in x)
+    print(''.join('1' if k != 0 else '.' for k in x))
 
 
   def _printAllTrainingSequences(self, trainingSequences):
     """Print all vectors"""
     for i, trainingSequence in enumerate(trainingSequences):
-      print "============= Sequence", i, "================="
+      print("============= Sequence", i, "=================")
       for pattern in trainingSequence:
         self._printOneTrainingVector(pattern)
 
@@ -128,7 +128,7 @@
 
     if g_testCPPTM:
       if g_options.verbosity > 1:
-        print "Creating BacktrackingTMCPP instance"
+        print("Creating BacktrackingTMCPP instance")
 
       cppTM = BacktrackingTMCPP(numberOfCols = numCols, cellsPerColumn = 4,
                                 initialPerm = initialPerm, connectedPerm = connectedPerm,
@@ -151,7 +151,7 @@
       cppTM = None
 
     if g_options.verbosity > 1:
-      print "Creating PY TM instance"
+      print("Creating PY TM instance")
     pyTM = BacktrackingTM(numberOfCols = numCols,
                           cellsPerColumn = 4,
                           initialPerm = initialPerm,
@@ -178,7 +178,7 @@
 
     numCols = numOnes * numPatterns
     p = []
-    for i in xrange(numPatterns):
+    for i in range(numPatterns):
       x = numpy.zeros(numCols, dtype='float32')
       x[i*numOnes:(i+1)*numOnes] = 1
       p.append(x)
@@ -206,9 +206,9 @@
 
     # Create noisy training sequence
     trainingSequences = []
-    for _ in xrange(numRepetitions):
+    for _ in range(numRepetitions):
       sequence = []
-      for j in xrange(numPatterns):
+      for j in range(numPatterns):
 
         # Make left half
         v = numpy.zeros(numCols)
@@ -223,16 +223,16 @@
 
     # Create a single clean test sequence
     testSequence = []
-    for j in xrange(numPatterns):
+    for j in range(numPatterns):
       # Make only left half
       v = numpy.zeros(numCols, dtype='float32')
       v[0:halfCols] = p[j]
       testSequence.append(v)
 
     if g_options.verbosity > 1:
-      print "\nTraining sequences"
+      print("\nTraining sequences")
       self._printAllTrainingSequences(trainingSequences)
-      print "\nTest sequence"
+      print("\nTest sequence")
       self._printAllTrainingSequences([testSequence])
 
     return (trainingSequences, [testSequence])
@@ -271,9 +271,9 @@
 
     # Create the noisy training sequence
     trainingSequences = []
-    for i in xrange(numRepetitions*numSequences):
+    for i in range(numRepetitions*numSequences):
       sequence = []
-      for j in xrange(numPatterns):
+      for j in range(numPatterns):
 
         # Make left half
         v = numpy.zeros(numCols, dtype='float32')
@@ -288,9 +288,9 @@
 
     # Create the clean test sequences
     testSequences = []
-    for i in xrange(numSequences):
+    for i in range(numSequences):
       sequence = []
-      for j in xrange(numPatterns):
+      for j in range(numPatterns):
         # Make only left half
         v = numpy.zeros(numCols, dtype='float32')
         v[0:halfCols] = p[indices[i % numSequences][j]]
@@ -298,9 +298,9 @@
       testSequences.append(sequence)
 
     if g_options.verbosity > 1:
-      print "\nTraining sequences"
+      print("\nTraining sequences")
       self._printAllTrainingSequences(trainingSequences)
-      print "\nTest sequences"
+      print("\nTest sequences")
       self._printAllTrainingSequences(testSequences)
 
     return (trainingSequences, testSequences)
@@ -328,18 +328,18 @@
     #--------------------------------------------------------------------------
     # Learn
     if g_options.verbosity > 0:
-      print "============= Training ================="
-      print "TM parameters:"
-      print "CPP"
+      print("============= Training =================")
+      print("TM parameters:")
+      print("CPP")
       if cppTM is not None:
-        print cppTM.printParameters()
-      print "\nPY"
-      print pyTM.printParameters()
+        print(cppTM.printParameters())
+      print("\nPY")
+      print(pyTM.printParameters())
 
     for sequenceNum, trainingSequence in enumerate(trainingSequences):
 
       if g_options.verbosity > 1:
-        print "============= New sequence ================="
+        print("============= New sequence =================")
 
       if doResets:
         if cppTM is not None:
@@ -349,9 +349,9 @@
       for t, x in enumerate(trainingSequence):
 
         if g_options.verbosity > 1:
-          print "Time step", t, "sequence number", sequenceNum
-          print "Input: ", pyTM.printInput(x)
-          print "NNZ:", x.nonzero()
+          print("Time step", t, "sequence number", sequenceNum)
+          print("Input: ", pyTM.printInput(x))
+          print("NNZ:", x.nonzero())
 
         x = numpy.array(x).astype('float32')
         if cppTM is not None:
@@ -364,23 +364,23 @@
 
         if g_options.verbosity > 2:
           if cppTM is not None:
-            print "CPP"
+            print("CPP")
             cppTM.printStates(printPrevious = (g_options.verbosity > 4))
-          print "\nPY"
+          print("\nPY")
           pyTM.printStates(printPrevious = (g_options.verbosity > 4))
-          print
+          print()
 
       if g_options.verbosity > 4:
-        print "Sequence finished. Complete state after sequence"
-        if cppTM is not None:
-          print "CPP"
+        print("Sequence finished. Complete state after sequence")
+        if cppTM is not None:
+          print("CPP")
           cppTM.printCells()
-        print "\nPY"
+        print("\nPY")
         pyTM.printCells()
-        print
+        print()
 
     if g_options.verbosity > 2:
-      print "Calling trim segments"
+      print("Calling trim segments")
 
     if cppTM is not None:
       nSegsRemovedCPP, nSynsRemovedCPP = cppTM.trimSegments()
@@ -392,22 +392,22 @@
     if cppTM is not None:
       assert fdrutils.tmDiff2(cppTM, pyTM, g_options.verbosity) == True
 
-    print "Training completed. Stats:"
+    print("Training completed. Stats:")
     info = pyTM.getSegmentInfo()
-    print "  nSegments:", info[0]
-    print "  nSynapses:", info[1]
+    print("  nSegments:", info[0])
+    print("  nSynapses:", info[1])
     if g_options.verbosity > 3:
-      print "Complete state:"
+      print("Complete state:")
       if cppTM is not None:
-        print "CPP"
+        print("CPP")
         cppTM.printCells()
-      print "\nPY"
+      print("\nPY")
       pyTM.printCells()
 
     #---------------------------------------------------------------------------
     # Infer
     if g_options.verbosity > 1:
-      print "============= Inference ================="
+      print("============= Inference =================")
 
     if cppTM is not None:
       cppTM.collectStats = True
@@ -419,7 +419,7 @@
     for sequenceNum, testSequence in enumerate(testSequences):
 
       if g_options.verbosity > 1:
-        print "============= New sequence ================="
+        print("============= New sequence =================")
 
       slen = len(testSequence)
 
@@ -431,7 +431,7 @@
       for t, x in enumerate(testSequence):
 
         if g_options.verbosity >= 2:
-          print "Time step", t, '\nInput:'
+          print("Time step", t, '\nInput:')
           pyTM.printInput(x)
 
         if cppTM is not None:
@@ -443,10 +443,10 @@
 
         if g_options.verbosity > 2:
           if cppTM is not None:
-            print "CPP"
+            print("CPP")
             cppTM.printStates(printPrevious = (g_options.verbosity > 4),
                            printLearnState = False)
-          print "\nPY"
+          print("\nPY")
           pyTM.printStates(printPrevious = (g_options.verbosity > 4),
                          printLearnState = False)
 
@@ -456,10 +456,10 @@
 
         if g_options.verbosity >= 2:
           if cppTM is not None:
-            print "CPP"
-            print cppScores
-          print "\nPY"
-          print pyScores
+            print("CPP")
+            print(cppScores)
+          print("\nPY")
+          print(pyScores)
 
         if t < slen-1 and t > pyTM.burnIn:
           nPredictions += 1
@@ -483,9 +483,9 @@
         passTest = True
 
     if not passTest:
-      print "CPP correct predictions:", cppNumCorrect
-      print "PY correct predictions:", pyNumCorrect
-      print "Total predictions:", nPredictions
+      print("CPP correct predictions:", cppNumCorrect)
+      print("PY correct predictions:", pyNumCorrect)
+      print("Total predictions:", nPredictions)
 
     return passTest
 
@@ -499,7 +499,7 @@
     else:
       testName = "TestSL1"
 
-    print "\nRunning %s..." % testName
+    print("\nRunning %s..." % testName)
 
     trainingSet, testSet = self._buildSegmentLearningTrainingSet(numOnes,
                                                                  numRepetitions)
@@ -511,10 +511,10 @@
     testResult = self._testSegmentLearningSequence(tms, trainingSet, testSet)
 
     if testResult:
-      print "%s PASS" % testName
+      print("%s PASS" % testName)
       return 1
     else:
-      print "%s FAILED" % testName
+      print("%s FAILED" % testName)
       return 0
 
 
@@ -527,7 +527,7 @@
     else:
       testName = "TestSL2"
 
-    print "\nRunning %s..." % testName
+    print("\nRunning %s..." % testName)
 
     trainingSet, testSet = self._buildSL2TrainingSet(numOnes, numRepetitions)
     numCols = len(trainingSet[0][0])
@@ -538,10 +538,10 @@
     testResult = self._testSegmentLearningSequence(tms, trainingSet, testSet)
 
     if testResult:
-      print "%s PASS" % testName
+      print("%s PASS" % testName)
       return 1
     else:
-      print "%s FAILED" % testName
+      print("%s FAILED" % testName)
       return 0
 
 
@@ -561,8 +561,8 @@
     """Test segment learning with fixed resources"""
 
     if not g_options.long:
-      print "Test %s only enabled with the --long option" % \
-                                (self._testMethodName)
+      print("Test %s only enabled with the --long option" % \
+                                (self._testMethodName))
       return
 
     self._testSL1(fixedResources=True,
@@ -573,8 +573,8 @@
     """Test segment learning without fixed resources"""
 
     if not g_options.long:
-      print "Test %s only enabled with the --long option" % \
-                                (self._testMethodName)
+      print("Test %s only enabled with the --long option" % \
+                                (self._testMethodName))
       return
 
     self._testSL2(fixedResources=False,
@@ -585,8 +585,8 @@
     """Test segment learning with fixed resources"""
 
     if not g_options.long:
-      print "Test %s only enabled with the --long option" % \
-                                (self._testMethodName)
+      print("Test %s only enabled with the --long option" % \
+                                (self._testMethodName))
       return
 
     self._testSL2(fixedResources=True,
--- d:\nupic\src\python\python27\examples\tm\tm_test.py	(original)
+++ d:\nupic\src\python\python27\examples\tm\tm_test.py	(refactored)
@@ -462,15 +462,15 @@
 
 def printOneTrainingVector(x):
 
-    print ''.join('1' if k != 0 else '.' for k in x)
+    print(''.join('1' if k != 0 else '.' for k in x))
 
 
 def printAllTrainingSequences(trainingSequences, upTo = 99999):
 
-    for t in xrange(min(len(trainingSequences[0]), upTo)):
-        print 't=',t,
+    for t in range(min(len(trainingSequences[0]), upTo)):
+        print('t=',t, end=' ')
         for i,trainingSequence in enumerate(trainingSequences):
-            print "\tseq#",i,'\t',
+            print("\tseq#",i,'\t', end=' ')
             printOneTrainingVector(trainingSequences[i][t])
 
 
@@ -562,7 +562,7 @@
 
   sharedSequence = []
 
-  for i in xrange(sharedSequenceLength):
+  for i in range(sharedSequenceLength):
     if disjointConsecutive and i > 0:
       x = generatePattern(numCols, minOnes, maxOnes, colSet,
                           sharedSequence[i-1])
@@ -579,18 +579,18 @@
   else:
       trailingLength = sequenceLength - sharedSequenceLength
 
-  for k,s in enumerate(xrange(numSequences)):
+  for k,s in enumerate(range(numSequences)):
 
     # TODO: implement no repetitions
     if len(trainingSequences) > 0 and 'shuffle' in seqGenMode:
 
-      r = range(subsequenceStartPos) \
-          + range(subsequenceStartPos + sharedSequenceLength, sequenceLength)
+      r = list(range(subsequenceStartPos)) \
+          + list(range(subsequenceStartPos + sharedSequenceLength, sequenceLength))
 
       rgen.shuffle(r)
 
       r = r[:subsequenceStartPos] \
-          + range(subsequenceStartPos, subsequenceStartPos + sharedSequenceLength) \
+          + list(range(subsequenceStartPos, subsequenceStartPos + sharedSequenceLength)) \
           + r[subsequenceStartPos:]
 
       sequence = [trainingSequences[k-1][j] for j in r]
@@ -599,7 +599,7 @@
         sequence = []
 
         if 'beginning' not in seqGenMode:
-          for i in xrange(subsequenceStartPos):
+          for i in range(subsequenceStartPos):
             if disjointConsecutive and i > 0:
               x = generatePattern(numCols, minOnes, maxOnes, colSet, sequence[i-1])
             else:
@@ -609,7 +609,7 @@
         if 'shared' in seqGenMode and 'no shared' not in seqGenMode:
           sequence.extend(sharedSequence)
 
-        for i in xrange(trailingLength):
+        for i in range(trailingLength):
           if disjointConsecutive and i > 0:
             x = generatePattern(numCols, minOnes, maxOnes, colSet, sequence[i-1])
           else:
@@ -623,7 +623,7 @@
   assert len(trainingSequences) == numSequences
 
   if VERBOSITY >= 2:
-    print "Training Sequences"
+    print("Training Sequences")
     pprint.pprint(trainingSequences)
 
   if sharedSequenceLength > 0:
@@ -638,7 +638,7 @@
 
   numCols = numOnes * numPatterns
   p = []
-  for i in xrange(numPatterns):
+  for i in range(numPatterns):
     x = numpy.zeros(numCols, dtype='float32')
     x[i*numOnes:(i+1)*numOnes] = 1
     p.append(x)
@@ -698,7 +698,7 @@
 
   s = []
   s.append(p[rgen.randint(3,23)])
-  for _ in xrange(20):
+  for _ in range(20):
     s.append(p[rgen.randint(3,23)])
     s.append(p[0])
     s.append(p[1])
@@ -724,25 +724,25 @@
 
   s = []
   s.append(p[rgen.randint(5,numPatterns)])
-  for _ in xrange(50):
+  for _ in range(50):
     r = rgen.randint(5,numPatterns)
-    print r,
+    print(r, end=' ')
     s.append(p[r])
     if rgen.binomial(1, 0.5) > 0:
-      print "S1",
+      print("S1", end=' ')
       s.append(p[0])
       s.append(p[1])
       s.append(p[2])
       s.append(p[4])
     else:
-      print "S2",
+      print("S2", end=' ')
       s.append(p[1])
       s.append(p[2])
       s.append(p[3])
     r = rgen.randint(5,numPatterns)
     s.append(p[r])
-    print r,
-  print
+    print(r, end=' ')
+  print()
 
   return ([s], [ [p[0], p[1], p[2], p[4]],  [p[1], p[2], p[3]] ])
 
@@ -780,7 +780,7 @@
                pamLength = 1000,
                checkSynapseConsistency=checkSynapseConsistency)
 
-  print "Creation ok"
+  print("Creation ok")
 
   #--------------------------------------------------------------------------------
   # Save and reload
@@ -799,7 +799,7 @@
 
   assert tm2.numberOfCols == numberOfCols
   assert tm2.cellsPerColumn == cellsPerColumn
-  print tm2.initialPerm
+  print(tm2.initialPerm)
   assert tm2.initialPerm == numpy.float32(.2)
   assert tm2.connectedPerm == numpy.float32(.8)
   assert tm2.minThreshold == minThreshold
@@ -814,11 +814,11 @@
   assert tm2.seed == SEED
   assert tm2.verbosity == verbosity
 
-  print "Save/load ok"
+  print("Save/load ok")
 
   #--------------------------------------------------------------------------------
   # Learn
-  for i in xrange(5):
+  for i in range(5):
     xi = rgen.randint(0,2,(numberOfCols))
     x = numpy.array(xi, dtype="uint32")
     y = tm.learn(x)
@@ -826,14 +826,14 @@
   #--------------------------------------------------------------------------------
   # Infer
   patterns = rgen.randint(0,2,(4,numberOfCols))
-  for i in xrange(10):
+  for i in range(10):
     xi = rgen.randint(0,2,(numberOfCols))
     x = numpy.array(xi, dtype="uint32")
     y = tm.infer(x)
     if i > 0:
         p = tm._checkPrediction([pattern.nonzero()[0] for pattern in patterns])
 
-  print "basicTest ok"
+  print("basicTest ok")
 
 #---------------------------------------------------------------------------------
 # Figure out acceptable patterns if none were passed to us.
@@ -894,7 +894,7 @@
 
     # Add patterns going forward
     acceptablePatterns += [trainingSequences[whichSequence][t] \
-                           for t in xrange(t,upTo)]
+                           for t in range(t,upTo)]
 
     return acceptablePatterns
 
@@ -996,12 +996,12 @@
 
   #--------------------------------------------------------------------------------
   # Learn
-  for r in xrange(nTrainingReps):
+  for r in range(nTrainingReps):
     if VERBOSITY > 1:
-      print "============= Learning round",r,"================="
+      print("============= Learning round",r,"=================")
     for sequenceNum, trainingSequence in enumerate(trainingSequences):
       if VERBOSITY > 1:
-        print "============= New sequence ================="
+        print("============= New sequence =================")
       if doResets:
           tm.reset()
           if compareToPy:
@@ -1013,9 +1013,9 @@
             noise_vector = rgen.binomial(len(x), noiseLevel, (len(x)))
             x = logical_xor(x, noise_vector)
         if VERBOSITY > 2:
-          print "Time step",t, "learning round",r, "sequence number", sequenceNum
-          print "Input: ",tm.printInput(x)
-          print "NNZ:", x.nonzero()
+          print("Time step",t, "learning round",r, "sequence number", sequenceNum)
+          print("Input: ",tm.printInput(x))
+          print("NNZ:", x.nonzero())
         x = numpy.array(x).astype('float32')
         y = tm.learn(x)
         if compareToPy:
@@ -1025,25 +1025,25 @@
 
         if VERBOSITY > 3:
           tm.printStates(printPrevious = (VERBOSITY > 4))
-          print
+          print()
       if VERBOSITY > 3:
-        print "Sequence finished. Complete state after sequence"
+        print("Sequence finished. Complete state after sequence")
         tm.printCells()
-        print
+        print()
 
   numPerfectAtHub = 0
 
   if compareToPy:
-      print "End of training"
+      print("End of training")
       assert fdrutils.tmDiff(tm, py_tm, VERBOSITY) == True
 
   #--------------------------------------------------------------------------------
   # Infer
-  if VERBOSITY > 1: print "============= Inference ================="
+  if VERBOSITY > 1: print("============= Inference =================")
 
   for s,testSequence in enumerate(testSequences):
 
-    if VERBOSITY > 1: print "============= New sequence ================="
+    if VERBOSITY > 1: print("============= New sequence =================")
 
     if doResets:
         tm.reset()
@@ -1061,7 +1061,7 @@
         noise_vector = rgen.binomial(len(x), noiseLevel, (len(x)))
         x = logical_xor(x, noise_vector)
 
-      if VERBOSITY > 2: print "Time step",t, '\nInput:', tm.printInput(x)
+      if VERBOSITY > 2: print("Time step",t, '\nInput:', tm.printInput(x))
 
       x = numpy.array(x).astype('float32')
       y = tm.infer(x)
@@ -1076,7 +1076,7 @@
       #     print ''.join('.' if z[i] == 0 else '1' for i in xrange(len(z)))
 
       if VERBOSITY > 3: tm.printStates(printPrevious = (VERBOSITY > 4),
-                                       printLearnState = False); print
+                                       printLearnState = False); print()
 
 
       if nMultiStepPrediction > 0:
@@ -1084,9 +1084,9 @@
         y_ms = tm.predict(nSteps=nMultiStepPrediction)
 
         if VERBOSITY > 3:
-          print "Multi step prediction at Time step", t
+          print("Multi step prediction at Time step", t)
           for i in range(nMultiStepPrediction):
-            print "Prediction at t+", i+1
+            print("Prediction at t+", i+1)
             tm.printColConfidence(y_ms[i])
 
         # Error Checking
@@ -1102,34 +1102,34 @@
             falsePositives = missingFromInput
 
             if VERBOSITY > 2:
-              print "Predition from %d to %d" % (t, t+i+1)
-              print "\t\tFalse Negatives:", falseNegatives
-              print "\t\tFalse Positivies:", falsePositives
+              print("Predition from %d to %d" % (t, t+i+1))
+              print("\t\tFalse Negatives:", falseNegatives)
+              print("\t\tFalse Positivies:", falsePositives)
 
             if falseNegatives > 0 or falsePositives > 0:
               numStrictErrors += 1
 
               if falseNegatives > 0 and VERBOSITY > 1:
-                print "Multi step prediction from t=", t, "to t=", t+i+1,\
-                      "false negative with error=",falseNegatives,
-                print "out of", totalActiveInInput,"ones"
+                print("Multi step prediction from t=", t, "to t=", t+i+1,\
+                      "false negative with error=",falseNegatives, end=' ')
+                print("out of", totalActiveInInput,"ones")
 
               if falsePositives > 0 and VERBOSITY > 1:
-                print "Multi step prediction from t=", t, "to t=", t+i+1,\
-                    "false positive with error=",falsePositives,
-                print "out of",totalActiveInInput,"ones"
+                print("Multi step prediction from t=", t, "to t=", t+i+1,\
+                    "false positive with error=",falsePositives, end=' ')
+                print("out of",totalActiveInInput,"ones")
 
               if falsePositives > 3 or falseNegatives > 3:
                 numFailures += 1
 
                 # Analyze the failure if we care about it
                 if VERBOSITY > 1 and not shouldFail:
-                  print 'Input at t=', t
-                  print '\t\t',; printOneTrainingVector(testSequence[t])
-                  print 'Prediction for t=', t+i+1
-                  print '\t\t',; printOneTrainingVector(y_ms[i])
-                  print 'Actual input at t=', t+i+1
-                  print '\t\t',; printOneTrainingVector(testSequence[t+i+1])
+                  print('Input at t=', t)
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t])
+                  print('Prediction for t=', t+i+1)
+                  print('\t\t', end=' '); printOneTrainingVector(y_ms[i])
+                  print('Actual input at t=', t+i+1)
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t+i+1])
 
 
       if t < slen-1:
@@ -1155,14 +1155,14 @@
           numStrictErrors += 1
 
           if falseNegatives > 0 and VERBOSITY > 1:
-            print "Pattern",s,"time",t,\
-                  "prediction false negative with error=",falseNegatives,
-            print "out of",int(testSequence[t+1].sum()),"ones"
+            print("Pattern",s,"time",t,\
+                  "prediction false negative with error=",falseNegatives, end=' ')
+            print("out of",int(testSequence[t+1].sum()),"ones")
 
           if falsePositives > 0 and VERBOSITY > 1:
-            print "Pattern",s,"time",t,\
-                  "prediction false positive with error=",falsePositives,
-            print "out of",int(testSequence[t+1].sum()),"ones"
+            print("Pattern",s,"time",t,\
+                  "prediction false positive with error=",falsePositives, end=' ')
+            print("out of",int(testSequence[t+1].sum()),"ones")
 
           if falseNegatives > 3 or falsePositives > 3:
 
@@ -1170,19 +1170,19 @@
 
             # Analyze the failure if we care about it
             if VERBOSITY > 1 and not shouldFail:
-              print 'Test sequences'
+              print('Test sequences')
               if len(testSequences) > 1:
                   printAllTrainingSequences(testSequences, t+1)
               else:
-                  print '\t\t',; printOneTrainingVector(testSequence[t])
-                  print '\t\t',; printOneTrainingVector(testSequence[t+1])
-              print 'Acceptable'
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t])
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t+1])
+              print('Acceptable')
               for p in acceptablePatterns:
-                  print '\t\t',; printOneTrainingVector(p)
-              print 'Output'
+                  print('\t\t', end=' '); printOneTrainingVector(p)
+              print('Output')
               diagnostic = ''
               output = sum(tm.currentOutput,axis=1)
-              print '\t\t',; printOneTrainingVector(output)
+              print('\t\t', end=' '); printOneTrainingVector(output)
 
         else:
             numPerfect += 1
@@ -1205,7 +1205,7 @@
 
   for numSequences in [1]:
 
-    print "Test "+name+" (sequence memory - 1 repetition - 1 sequence)"
+    print("Test "+name+" (sequence memory - 1 repetition - 1 sequence)")
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1232,13 +1232,13 @@
                                 activationThreshold = 8,
                                 doPooling = False)
       if numFailures == 0:
-        print "Test "+name+" ok"
+        print("Test "+name+" ok")
       else:
-        print "Test "+name+" failed"
+        print("Test "+name+" failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1250,7 +1250,7 @@
 
   for numSequences in [1]:
 
-    print "Test "+name+" (sequence memory - 4 repetition - 1 sequence - slow learning)"
+    print("Test "+name+" (sequence memory - 4 repetition - 1 sequence - slow learning)")
 
     for _ in range(nTests): # Test that configuration several times
 
@@ -1278,13 +1278,13 @@
                                 doPooling = False)
 
       if numFailures == 0:
-        print "Test "+name+" ok"
+        print("Test "+name+" ok")
       else:
-        print "Test "+name+" failed"
+        print("Test "+name+" failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures,
-        print "numStrictErrors=", numStrictErrors,
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures, end=' ')
+        print("numStrictErrors=", numStrictErrors, end=' ')
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1298,10 +1298,10 @@
 
   for numSequences in [1]: # TestC has multiple sequences
 
-    print "Test",name,"(sequence memory - second repetition of the same sequence" +\
-          " should not add synapses)"
-    print "Num patterns in sequence =", numUniquePatterns,
-    print "cellsPerColumn=",cellsPerColumn
+    print("Test",name,"(sequence memory - second repetition of the same sequence" +\
+          " should not add synapses)")
+    print("Num patterns in sequence =", numUniquePatterns, end=' ')
+    print("cellsPerColumn=",cellsPerColumn)
 
     for _ in range(nTests): # Test that configuration several times
 
@@ -1349,18 +1349,18 @@
       segmentInfo2 = tm2.getSegmentInfo()
       if (segmentInfo1[0] != segmentInfo2[0]) or \
          (segmentInfo1[1] != segmentInfo2[1]) :
-          print "Training twice incorrectly resulted in more segments or synapses"
-          print "Number of segments: ", segmentInfo1[0], segmentInfo2[0]
+          print("Training twice incorrectly resulted in more segments or synapses")
+          print("Number of segments: ", segmentInfo1[0], segmentInfo2[0])
           numFailures += 1
 
       if numFailures == 0:
-        print "Test",name,"ok"
+        print("Test",name,"ok")
       else:
-        print "Test",name,"failed"
+        print("Test",name,"failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1374,7 +1374,7 @@
 
   for numSequences in [2,5]:
 
-    print "Test B3 (sequence memory - 2 repetitions -", numSequences, "sequences)"
+    print("Test B3 (sequence memory - 2 repetitions -", numSequences, "sequences)")
 
     for _ in range(nTests): # Test that configuration several times
 
@@ -1401,13 +1401,13 @@
                                 activationThreshold = 8,
                                 doPooling = False)
       if numFailures == 0:
-        print "Test B3 ok"
+        print("Test B3 ok")
       else:
-        print "Test B3 failed"
+        print("Test B3 failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1417,7 +1417,7 @@
 
   cellsPerColumn = 4
 
-  print "Higher order test 0 with cellsPerColumn=",cellsPerColumn
+  print("Higher order test 0 with cellsPerColumn=",cellsPerColumn)
 
   trainingSet = buildSimpleTrainingSet(numOnes)
 
@@ -1441,13 +1441,13 @@
   if numFailures == 0 and \
      numStrictErrors == 0 and \
      numPerfect == len(trainingSet[0])*(len(trainingSet[0][0]) - 1):
-    print "Test PASS"
+    print("Test PASS")
     return 0
   else:
-    print "Test FAILED"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test FAILED")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1462,9 +1462,9 @@
 
   for numSequences in nSequences:
 
-    print "Higher order test with sequenceLength=",sequenceLength,
-    print "cellsPerColumn=",cellsPerColumn,"nTests=",nTests,
-    print "numSequences=",numSequences, "pctShared=", pctShared
+    print("Higher order test with sequenceLength=",sequenceLength, end=' ')
+    print("cellsPerColumn=",cellsPerColumn,"nTests=",nTests, end=' ')
+    print("numSequences=",numSequences, "pctShared=", pctShared)
 
     for _ in range(nTests): # Test that configuration several times
 
@@ -1496,17 +1496,17 @@
 
       if numFailures == 0 and not shouldFail \
              or numFailures > 0 and shouldFail:
-          print "Test PASS",
+          print("Test PASS", end=' ')
           if shouldFail:
-              print '(should fail, and failed)'
+              print('(should fail, and failed)')
           else:
-              print
+              print()
       else:
-        print "Test FAILED"
+        print("Test FAILED")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1516,7 +1516,7 @@
 
   cellsPerColumn = 4
 
-  print "Higher order test 11 with cellsPerColumn=",cellsPerColumn
+  print("Higher order test 11 with cellsPerColumn=",cellsPerColumn)
 
   trainingSet = buildAlternatingTrainingSet(numOnes= 3)
 
@@ -1539,13 +1539,13 @@
   if numFailures == 0 and \
      numStrictErrors == 0 and \
      numPerfect == len(trainingSet[0])*(len(trainingSet[0][0]) - 1):
-    print "Test PASS"
+    print("Test PASS")
     return 0
   else:
-    print "Test FAILED"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test FAILED")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1559,7 +1559,7 @@
       get correct high order prediction after multiple reps.
   """
 
-  print "Test H2a - second repetition of the same sequence should not add synapses"
+  print("Test H2a - second repetition of the same sequence should not add synapses")
 
   nFailed = 0
   subsequenceStartPos = 10
@@ -1567,10 +1567,10 @@
 
   for numSequences in nSequences:
 
-    print "Higher order test with sequenceLength=",sequenceLength,
-    print "cellsPerColumn=",cellsPerColumn,"nTests=",nTests,"numCols=", numCols
-    print "numSequences=",numSequences, "pctShared=", pctShared,
-    print "sharing mode=", seqGenMode
+    print("Higher order test with sequenceLength=",sequenceLength, end=' ')
+    print("cellsPerColumn=",cellsPerColumn,"nTests=",nTests,"numCols=", numCols)
+    print("numSequences=",numSequences, "pctShared=", pctShared, end=' ')
+    print("sharing mode=", seqGenMode)
 
     for _ in range(nTests): # Test that configuration several times
 
@@ -1581,7 +1581,7 @@
                                      numCols = numCols,
                                      minOnes = 21, maxOnes = 25)
 
-      print "============== 10 ======================"
+      print("============== 10 ======================")
 
       numFailures3, numStrictErrors3, numPerfect3, tm3 = \
                    testSequence(trainingSet,
@@ -1600,7 +1600,7 @@
                                 doPooling = False,
                                 shouldFail = shouldFail)
 
-      print "============== 2 ======================"
+      print("============== 2 ======================")
 
       numFailures, numStrictErrors, numPerfect, tm2 = \
                    testSequence(trainingSet,
@@ -1619,7 +1619,7 @@
                                 doPooling = False,
                                 shouldFail = shouldFail)
 
-      print "============== 1 ======================"
+      print("============== 1 ======================")
 
       numFailures1, numStrictErrors1, numPerfect1, tm1 = \
                    testSequence(trainingSet,
@@ -1643,32 +1643,32 @@
       segmentInfo2 = tm2.getSegmentInfo()
       if (abs(segmentInfo1[0] - segmentInfo2[0]) > 3) or \
          (abs(segmentInfo1[1] - segmentInfo2[1]) > 3*15) :
-          print "Training twice incorrectly resulted in too many segments or synapses"
-          print segmentInfo1
-          print segmentInfo2
-          print tm3.getSegmentInfo()
+          print("Training twice incorrectly resulted in too many segments or synapses")
+          print(segmentInfo1)
+          print(segmentInfo2)
+          print(tm3.getSegmentInfo())
           tm3.trimSegments()
-          print tm3.getSegmentInfo()
-
-          print "Failures for 1, 2, and N reps"
-          print numFailures1, numStrictErrors1, numPerfect1
-          print numFailures, numStrictErrors, numPerfect
-          print numFailures3, numStrictErrors3, numPerfect3
+          print(tm3.getSegmentInfo())
+
+          print("Failures for 1, 2, and N reps")
+          print(numFailures1, numStrictErrors1, numPerfect1)
+          print(numFailures, numStrictErrors, numPerfect)
+          print(numFailures3, numStrictErrors3, numPerfect3)
           numFailures += 1
 
       if numFailures == 0 and not shouldFail \
              or numFailures > 0 and shouldFail:
-          print "Test PASS",
+          print("Test PASS", end=' ')
           if shouldFail:
-              print '(should fail, and failed)'
+              print('(should fail, and failed)')
           else:
-              print
+              print()
       else:
-        print "Test FAILED"
+        print("Test FAILED")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1686,12 +1686,12 @@
 
   for numSequences in nSequences:
 
-    print "Pooling test with sequenceLength=",sequenceLength,
-    print 'numCols=', numCols,
-    print "cellsPerColumn=",cellsPerColumn,"nTests=",nTests,
-    print "numSequences=",numSequences, "pctShared=", pctShared,
-    print "nTrainingReps=", nTrainingReps, "minOnes=", minOnes,
-    print "maxOnes=", maxOnes
+    print("Pooling test with sequenceLength=",sequenceLength, end=' ')
+    print('numCols=', numCols, end=' ')
+    print("cellsPerColumn=",cellsPerColumn,"nTests=",nTests, end=' ')
+    print("numSequences=",numSequences, "pctShared=", pctShared, end=' ')
+    print("nTrainingReps=", nTrainingReps, "minOnes=", minOnes, end=' ')
+    print("maxOnes=", maxOnes)
 
     for _ in range(nTests): # Test that configuration several times
 
@@ -1723,12 +1723,12 @@
       if numFailures == 0 and \
          numStrictErrors == 0 and \
          numPerfect == numSequences*(sequenceLength - 1):
-        print "Test PASS"
+        print("Test PASS")
       else:
-        print "Test FAILED"
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("Test FAILED")
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
         nFailed = nFailed + 1
 
   return nFailed
@@ -1741,7 +1741,7 @@
   newSynapseCount = 5
   activationThreshold = newSynapseCount
 
-  print "HiLo test 0a with cellsPerColumn=",cellsPerColumn
+  print("HiLo test 0a with cellsPerColumn=",cellsPerColumn)
 
   trainingSet, testSet = buildHL0aTrainingSet()
   numCols = trainingSet[0][0].size
@@ -1766,22 +1766,22 @@
 
   tm.trimSegments()
   retAfter = tm.getSegmentInfo()
-  print retAfter[0], retAfter[1]
+  print(retAfter[0], retAfter[1])
   if retAfter[0] > 20:
-    print "Too many segments"
+    print("Too many segments")
     numFailures += 1
   if retAfter[1] > 100:
-    print "Too many synapses"
+    print("Too many synapses")
     numFailures += 1
 
   if numFailures == 0:
-    print "Test HL0a ok"
+    print("Test HL0a ok")
     return 0
   else:
-    print "Test HL0a failed"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test HL0a failed")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1792,11 +1792,11 @@
   newSynapseCount = 5
   activationThreshold = newSynapseCount
 
-  print "HiLo test 0b with cellsPerColumn=",cellsPerColumn
+  print("HiLo test 0b with cellsPerColumn=",cellsPerColumn)
 
   trainingSet, testSet = buildHL0bTrainingSet()
   numCols = trainingSet[0][0].size
-  print "numCols=", numCols
+  print("numCols=", numCols)
 
   numFailures, numStrictErrors, numPerfect, tm = \
                testSequence([trainingSet],
@@ -1820,13 +1820,13 @@
   tm.printCells()
 
   if numFailures == 0:
-    print "Test HL0 ok"
+    print("Test HL0 ok")
     return 0
   else:
-    print "Test HL0 failed"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test HL0 failed")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1848,12 +1848,12 @@
 
   for numSequences in nSequences:
 
-    print "Hilo test with sequenceLength=", sequenceLength,
-    print "cellsPerColumn=", cellsPerColumn, "nTests=", nTests,
-    print "numSequences=", numSequences, "pctShared=", pctShared,
-    print "nTrainingReps=", nTrainingReps, "minOnes=", minOnes,
-    print "maxOnes=", maxOnes,
-    print 'noiseModel=', noiseModel, 'noiseLevel=', noiseLevel
+    print("Hilo test with sequenceLength=", sequenceLength, end=' ')
+    print("cellsPerColumn=", cellsPerColumn, "nTests=", nTests, end=' ')
+    print("numSequences=", numSequences, "pctShared=", pctShared, end=' ')
+    print("nTrainingReps=", nTrainingReps, "minOnes=", minOnes, end=' ')
+    print("maxOnes=", maxOnes, end=' ')
+    print('noiseModel=', noiseModel, 'noiseLevel=', noiseLevel)
 
     for _ in range(nTests): # Test that configuration several times
 
@@ -1887,12 +1887,12 @@
       if numFailures == 0 and \
          numStrictErrors == 0 and \
          numPerfect == numSequences*(sequenceLength - 1):
-        print "Test PASS"
+        print("Test PASS")
       else:
-        print "Test FAILED"
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("Test FAILED")
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
         nFailed = nFailed + 1
 
   return nFailed
@@ -1907,7 +1907,7 @@
   sequenceLength = 10
   numCols = 200
 
-  print 'Started', cellsPerColumn, numSequences
+  print('Started', cellsPerColumn, numSequences)
 
   seqGenMode = 'shared subsequence, one pattern'
   subsequenceStartPos = 5
@@ -1961,9 +1961,9 @@
                             doPooling = False,
                             shouldFail = False)
 
-  print 'Completed',
-  print cellsPerColumn, numSequences, numFailures1, numStrictErrors1, numPerfect1, atHub, \
-         numFailures2, numStrictErrors2, numPerfect2
+  print('Completed', end=' ')
+  print(cellsPerColumn, numSequences, numFailures1, numStrictErrors1, numPerfect1, atHub, \
+         numFailures2, numStrictErrors2, numPerfect2)
 
   return cellsPerColumn, numSequences, numFailures1, numStrictErrors1, numPerfect1, atHub, \
          numFailures2, numStrictErrors2, numPerfect2
@@ -1981,16 +1981,16 @@
   from multiprocessing import Pool
   import itertools
 
-  print "Hub capacity test"
+  print("Hub capacity test")
   # scalar value on predictions by looking at max perm over column
 
   p = Pool(2)
 
-  results = p.map(worker, itertools.product([1,2,3,4,5,6,7,8], xrange(1,2000,200)))
+  results = p.map(worker, itertools.product([1,2,3,4,5,6,7,8], range(1,2000,200)))
 
   f = open('results-numPerfect.11.22.10.txt', 'w')
   for i,r in enumerate(results):
-      print >>f, '{%d,%d,%d,%d,%d,%d,%d,%d,%d},' % r
+      print('{%d,%d,%d,%d,%d,%d,%d,%d,%d},' % r, file=f)
   f.close()
 
 
@@ -2003,7 +2003,7 @@
 
   # always run this one: if that one fails, we can't do anything
   basicTest()
-  print
+  print()
 
   #---------------------------------------------------------------------------------
   if testLength == "long":
@@ -2019,7 +2019,7 @@
                                    cellsPerColumn = 4, name="B6")
   tests['B7'] = TestB7(numUniquePatterns, nTests)
 
-  print
+  print()
 
   #---------------------------------------------------------------------------------
 
@@ -2028,14 +2028,14 @@
 
   if True:
 
-      print "Test H0"
+      print("Test H0")
       tests['H0'] = TestH0(numOnes = 5)
 
-      print "Test H2"
+      print("Test H2")
       #tests['H2'] = TestH(numUniquePatterns, nTests, cellsPerColumn = 4,
       #                    nTrainingReps = numUniquePatterns, compareToPy = False)
 
-      print "Test H3"
+      print("Test H3")
       tests['H3'] = TestH(numUniquePatterns, nTests,
                           numCols = 200,
                           cellsPerColumn = 20,
@@ -2043,7 +2043,7 @@
                           compareToPy = False,
                           highOrder = True)
 
-      print "Test H4" # Produces 3 false positives, but otherwise fine.
+      print("Test H4") # Produces 3 false positives, but otherwise fine.
       # TODO: investigate initial false positives?
       tests['H4'] = TestH(numUniquePatterns, nTests,
                         cellsPerColumn = 20,
@@ -2051,14 +2051,14 @@
                         seqGenMode='shared subsequence at beginning')
 
   if True:
-      print "Test H0 with multistep prediction"
+      print("Test H0 with multistep prediction")
 
       tests['H0_MS'] = TestH0(numOnes = 5, nMultiStepPrediction=2)
 
 
   if True:
 
-      print "Test H1" # - Should Fail
+      print("Test H1") # - Should Fail
       tests['H1'] = TestH(numUniquePatterns, nTests,
                                       cellsPerColumn = 1, nTrainingReps = 1,
                                       shouldFail = True)
@@ -2071,13 +2071,13 @@
 
 
   if False:
-      print "Test H5" # make sure seqs are good even with shuffling, fast learning
+      print("Test H5") # make sure seqs are good even with shuffling, fast learning
       tests['H5'] = TestH(numUniquePatterns, nTests,
                             cellsPerColumn = 10,
                             pctShared = 0.0,
                             seqGenMode='shuffle, no shared subsequence')
 
-      print "Test H6" # should work
+      print("Test H6") # should work
       tests['H6'] = TestH(numUniquePatterns, nTests,
                             cellsPerColumn = 10,
                             pctShared = 0.4,
@@ -2100,7 +2100,7 @@
       #                                  pctShared = 0.4,
       #                                  seqGenMode='shuffle, shared subsequence')
 
-      print "Test H9" # plot hub capacity
+      print("Test H9") # plot hub capacity
       tests['H9'] = TestH(numUniquePatterns, nTests,
                                         cellsPerColumn = 10,
                                         pctShared = 0.4,
@@ -2112,11 +2112,11 @@
       #                                    pctShared = 0.4,
       #                                    seqGenMode='shuffle, shared subsequence')
 
-      print
+      print()
 
   #---------------------------------------------------------------------------------
   if False:
-      print "Test P1"
+      print("Test P1")
       tests['P1'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
@@ -2125,14 +2125,14 @@
 
   if False:
 
-      print "Test P2"
+      print("Test P2")
       tests['P2'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
                                         seqGenMode = 'no shared subsequence',
                                         nTrainingReps = 5)
 
-      print "Test P3"
+      print("Test P3")
       tests['P3'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
@@ -2140,7 +2140,7 @@
                                         nSequences = [2] if testLength == 'short' else [2,5],
                                         nTrainingReps = 5)
 
-      print "Test P4"
+      print("Test P4")
       tests['P4'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
@@ -2148,19 +2148,19 @@
                                         nSequences = [2] if testLength == 'short' else [2,5],
                                         nTrainingReps = 5)
 
-      print
+      print()
 
   #---------------------------------------------------------------------------------
   if True:
-      print "Test HL0a"
+      print("Test HL0a")
       tests['HL0a'] = TestHL0a(numOnes = 5)
 
   if False:
 
-      print "Test HL0b"
+      print("Test HL0b")
       tests['HL0b'] = TestHL0b(numOnes = 5)
 
-      print "Test HL1"
+      print("Test HL1")
       tests['HL1'] = TestHL(sequenceLength = 20,
                                            nTests = nTests,
                                            numCols = 100,
@@ -2172,7 +2172,7 @@
                                            noiseLevel = 0.1,
                                            doResets = False)
 
-      print "Test HL2"
+      print("Test HL2")
       tests['HL2'] = TestHL(numUniquePatterns = 20,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2184,7 +2184,7 @@
                                            noiseLevel = 0.1,
                                            doResets = False)
 
-      print "Test HL3"
+      print("Test HL3")
       tests['HL3'] = TestHL(numUniquePatterns = 30,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2197,7 +2197,7 @@
                                            noiseLevel = 0.0,
                                            doResets = True)
 
-      print "Test HL4"
+      print("Test HL4")
       tests['HL4'] = TestHL(numUniquePatterns = 30,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2210,7 +2210,7 @@
                                            noiseLevel = 0.0,
                                            doResets = False)
 
-      print "Test HL5"
+      print("Test HL5")
       tests['HL5'] = TestHL(numUniquePatterns = 30,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2223,7 +2223,7 @@
                                            noiseLevel = 0.1,
                                            doResets = False)
 
-      print "Test HL6"
+      print("Test HL6")
       tests['HL6'] = nTests - TestHL(numUniquePatterns = 20,
                                      nTests = nTests,
                                      numCols = 200,
@@ -2236,21 +2236,21 @@
                                      doResets = True,
                                      hiloOn = False)
 
-      print
+      print()
 
   #---------------------------------------------------------------------------------
   nFailures = 0
-  for k,v in tests.iteritems():
+  for k,v in tests.items():
     nFailures = nFailures + v
 
   if nFailures > 0: # 1 to account for H1
-    print "There are failed tests"
-    print "Test\tn failures"
-    for k,v in tests.iteritems():
-      print k, "\t", v
+    print("There are failed tests")
+    print("Test\tn failures")
+    for k,v in tests.items():
+      print(k, "\t", v)
     assert 0
   else:
-    print "All tests pass"
+    print("All tests pass")
 
   #---------------------------------------------------------------------------------
   # Keep
@@ -2270,9 +2270,9 @@
 if __name__=="__main__":
 
   if not TEST_CPP_TM:
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
-    print "!!  WARNING: C++ TM testing is DISABLED until it can be updated."
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
+    print("!!  WARNING: C++ TM testing is DISABLED until it can be updated.")
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
 
   # Three different test lengths are passed in through the command line.
   # Developer tests use --short. Autobuild does not pass in anything.
@@ -2293,7 +2293,7 @@
       if 'verbosity' in arg:
           VERBOSITY = int(sys.argv[i+1])
       if 'help' in arg:
-          print "TMTest.py --short|long --seed number|'rand' --verbosity number"
+          print("TMTest.py --short|long --seed number|'rand' --verbosity number")
           sys.exit()
       if "short" in arg:
         testLength = "short"
@@ -2307,19 +2307,19 @@
     numUniquePatterns = 50
     nTests = 1
   elif testLength == "autobuild":
-    print "Running autobuild tests"
+    print("Running autobuild tests")
     numUniquePatterns = 50
     nTests = 1
   elif testLength == "long":
     numUniquePatterns = 100
     nTests = 3
 
-  print "TM tests", testLength, "numUniquePatterns=", numUniquePatterns, "nTests=", nTests,
-  print "seed=", SEED
-  print
+  print("TM tests", testLength, "numUniquePatterns=", numUniquePatterns, "nTests=", nTests, end=' ')
+  print("seed=", SEED)
+  print()
 
   if testLength == "long":
-    print 'Testing BacktrackingTM'
+    print('Testing BacktrackingTM')
     TMClass = BacktrackingTM
     runTests(testLength)
 
@@ -2331,6 +2331,6 @@
     checkSynapseConsistency = False
 
   if TEST_CPP_TM:
-    print 'Testing C++ TM'
+    print('Testing C++ TM')
     TMClass = BacktrackingTMCPP
     runTests(testLength)
--- d:\nupic\src\python\python27\nupic\serializable.py	(original)
+++ d:\nupic\src\python\python27\nupic\serializable.py	(refactored)
@@ -23,7 +23,7 @@
 
 
 
-class Serializable(object):
+class Serializable(object, metaclass=ABCMeta):
   """
   Serializable base class establishing
   :meth:`~nupic.serializable.Serializable.read` and
@@ -31,8 +31,6 @@
   :meth:`.readFromFile` and :meth:`.writeToFile` concrete methods to support
   serialization with Cap'n Proto.
   """
-
-  __metaclass__ = ABCMeta
 
 
   @classmethod
--- d:\nupic\src\python\python27\nupic\simple_server.py	(original)
+++ d:\nupic\src\python\python27\nupic\simple_server.py	(refactored)
@@ -65,7 +65,7 @@
     [model1, model2, model3, ...] list of model names
     """
     global g_models
-    return json.dumps({"models": g_models.keys()})
+    return json.dumps({"models": list(g_models.keys())})
 
 
   def POST(self, name):
@@ -87,7 +87,7 @@
     modelParams = data["modelParams"]
     predictedFieldName = data["predictedFieldName"]
 
-    if name in g_models.keys():
+    if name in list(g_models.keys()):
       raise web.badrequest("Model with name <%s> already exists" % name)
 
     model = ModelFactory.create(modelParams)
@@ -124,7 +124,7 @@
     data["timestamp"] = datetime.datetime.strptime(
         data["timestamp"], "%m/%d/%y %H:%M")
 
-    if name not in g_models.keys():
+    if name not in list(g_models.keys()):
       raise web.notfound("Model with name <%s> does not exist." % name)
 
     modelResult = g_models[name].run(data)
--- d:\nupic\src\python\python27\nupic\algorithms\anomaly_likelihood.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\anomaly_likelihood.py	(refactored)
@@ -446,9 +446,9 @@
   """
   if verbosity > 1:
     print("In estimateAnomalyLikelihoods.")
-    print("Number of anomaly scores:", len(anomalyScores))
-    print("Skip records=", skipRecords)
-    print("First 20:", anomalyScores[0:min(20, len(anomalyScores))])
+    print(("Number of anomaly scores:", len(anomalyScores)))
+    print(("Skip records=", skipRecords))
+    print(("First 20:", anomalyScores[0:min(20, len(anomalyScores))]))
 
   if len(anomalyScores) == 0:
     raise ValueError("Must have at least one anomalyScore")
@@ -505,9 +505,9 @@
   if verbosity > 1:
     print("Discovered params=")
     print(params)
-    print("Number of likelihoods:", len(likelihoods))
-    print("First 20 likelihoods:", (
-      filteredLikelihoods[0:min(20, len(filteredLikelihoods))] ))
+    print(("Number of likelihoods:", len(likelihoods)))
+    print(("First 20 likelihoods:", (
+      filteredLikelihoods[0:min(20, len(filteredLikelihoods))] )))
     print("leaving estimateAnomalyLikelihoods")
 
 
@@ -549,9 +549,9 @@
   """
   if verbosity > 3:
     print("In updateAnomalyLikelihoods.")
-    print("Number of anomaly scores:", len(anomalyScores))
-    print("First 20:", anomalyScores[0:min(20, len(anomalyScores))])
-    print("Params:", params)
+    print(("Number of anomaly scores:", len(anomalyScores)))
+    print(("First 20:", anomalyScores[0:min(20, len(anomalyScores))]))
+    print(("Params:", params))
 
   if len(anomalyScores) == 0:
     raise ValueError("Must have at least one anomalyScore")
@@ -600,8 +600,8 @@
   assert len(newParams["historicalLikelihoods"]) <= windowSize
 
   if verbosity > 3:
-    print("Number of likelihoods:", len(likelihoods))
-    print("First 20 likelihoods:", likelihoods[0:min(20, len(likelihoods))])
+    print(("Number of likelihoods:", len(likelihoods)))
+    print(("First 20 likelihoods:", likelihoods[0:min(20, len(likelihoods))]))
     print("Leaving updateAnomalyLikelihoods.")
 
   return (likelihoods, aggRecordList, newParams)
@@ -665,7 +665,7 @@
     # Skip (but log) records without correct number of entries
     if not isinstance(record, (list, tuple)) or len(record) != 3:
       if verbosity >= 1:
-        print("Malformed record:", record)
+        print(("Malformed record:", record))
       continue
 
     avg, historicalValues, total = (
@@ -675,8 +675,8 @@
     averagedRecordList.append( [record[0], record[1], avg] )
 
     if verbosity > 2:
-      print("Aggregating input record:", record)
-      print("Result:", [record[0], record[1], avg])
+      print(("Aggregating input record:", record))
+      print(("Result:", [record[0], record[1], avg]))
 
   return averagedRecordList, historicalValues, total
 
--- d:\nupic\src\python\python27\nupic\algorithms\backtracking_tm.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\backtracking_tm.py	(refactored)
@@ -27,7 +27,7 @@
 """
 
 import copy
-import cPickle as pickle
+import pickle as pickle
 import itertools
 
 try:
@@ -266,9 +266,9 @@
     # Every self.cells[column][index] contains a list of segments
     # Each segment is a structure of class Segment
     self.cells = []
-    for c in xrange(self.numberOfCols):
+    for c in range(self.numberOfCols):
       self.cells.append([])
-      for _ in xrange(self.cellsPerColumn):
+      for _ in range(self.cellsPerColumn):
         self.cells[c].append([])
 
     self.lrnIterationIdx = 0
@@ -485,7 +485,7 @@
 
     segmentUpdatesListProto = proto.init("segmentUpdates",
                                          len(self.segmentUpdates))
-    for i, (key, updates) in enumerate(self.segmentUpdates.iteritems()):
+    for i, (key, updates) in enumerate(self.segmentUpdates.items()):
       cellSegmentUpdatesProto = segmentUpdatesListProto[i]
       cellSegmentUpdatesProto.columnIdx = key[0]
       cellSegmentUpdatesProto.cellIdx = key[1]
@@ -567,9 +567,9 @@
 
     obj.activeColumns = [int(col) for col in proto.activeColumns]
 
-    obj.cells = [[] for _ in xrange(len(proto.cells))]
+    obj.cells = [[] for _ in range(len(proto.cells))]
     for columnSegments, columnSegmentsProto in zip(obj.cells, proto.cells):
-      columnSegments.extend([[] for _ in xrange(len(columnSegmentsProto))])
+      columnSegments.extend([[] for _ in range(len(columnSegmentsProto))])
       for cellSegments, cellSegmentsProto in zip(columnSegments,
                                                  columnSegmentsProto):
         for segmentProto in cellSegmentsProto:
@@ -706,7 +706,7 @@
         if len(a) != len(b):
           diff.append((keys + ('len',), len(a), len(b)))
         else:
-          for i in xrange(len(a)):
+          for i in range(len(a)):
             toCheck.append((keys + (i,), a[i], b[i]))
       elif isinstance(a, numpy.ndarray):
         if len(a) != len(b):
@@ -774,7 +774,7 @@
     are reset to 0.
     """
     if self.verbosity >= 3:
-      print "\n==== RESET ====="
+      print("\n==== RESET =====")
 
     self.lrnActiveState['t-1'].fill(0)
     self.lrnActiveState['t'].fill(0)
@@ -1005,8 +1005,8 @@
       s += ' '
       return s
 
-    for i in xrange(self.cellsPerColumn):
-      print formatRow(aState, i)
+    for i in range(self.cellsPerColumn):
+      print(formatRow(aState, i))
 
 
   def printConfidence(self, aState, maxCols = 20):
@@ -1025,8 +1025,8 @@
       s += ' '
       return s
 
-    for i in xrange(self.cellsPerColumn):
-      print formatFPRow(aState, i)
+    for i in range(self.cellsPerColumn):
+      print(formatFPRow(aState, i))
 
 
   def printColConfidence(self, aState, maxCols = 20):
@@ -1045,7 +1045,7 @@
       s += ' '
       return s
 
-    print formatFPRow(aState)
+    print(formatFPRow(aState))
 
   def printStates(self, printPrevious = True, printLearnState = True):
     """
@@ -1064,30 +1064,30 @@
       s += ' '
       return s
 
-    print "\nInference Active state"
-    for i in xrange(self.cellsPerColumn):
+    print("\nInference Active state")
+    for i in range(self.cellsPerColumn):
       if printPrevious:
-        print formatRow(self.infActiveState['t-1'], i),
-      print formatRow(self.infActiveState['t'], i)
-
-    print "Inference Predicted state"
-    for i in xrange(self.cellsPerColumn):
+        print(formatRow(self.infActiveState['t-1'], i), end=' ')
+      print(formatRow(self.infActiveState['t'], i))
+
+    print("Inference Predicted state")
+    for i in range(self.cellsPerColumn):
       if printPrevious:
-        print formatRow(self.infPredictedState['t-1'], i),
-      print formatRow(self.infPredictedState['t'], i)
+        print(formatRow(self.infPredictedState['t-1'], i), end=' ')
+      print(formatRow(self.infPredictedState['t'], i))
 
     if printLearnState:
-      print "\nLearn Active state"
-      for i in xrange(self.cellsPerColumn):
+      print("\nLearn Active state")
+      for i in range(self.cellsPerColumn):
         if printPrevious:
-          print formatRow(self.lrnActiveState['t-1'], i),
-        print formatRow(self.lrnActiveState['t'], i)
-
-      print "Learn Predicted state"
-      for i in xrange(self.cellsPerColumn):
+          print(formatRow(self.lrnActiveState['t-1'], i), end=' ')
+        print(formatRow(self.lrnActiveState['t'], i))
+
+      print("Learn Predicted state")
+      for i in range(self.cellsPerColumn):
         if printPrevious:
-          print formatRow(self.lrnPredictedState['t-1'], i),
-        print formatRow(self.lrnPredictedState['t'], i)
+          print(formatRow(self.lrnPredictedState['t-1'], i), end=' ')
+        print(formatRow(self.lrnPredictedState['t'], i))
 
 
   def printOutput(self, y):
@@ -1097,11 +1097,11 @@
     :param y: 
     :return: 
     """
-    print "Output"
-    for i in xrange(self.cellsPerColumn):
-      for c in xrange(self.numberOfCols):
-        print int(y[c, i]),
-      print
+    print("Output")
+    for i in range(self.cellsPerColumn):
+      for c in range(self.numberOfCols):
+        print(int(y[c, i]), end=' ')
+      print()
 
 
   def printInput(self, x):
@@ -1111,32 +1111,32 @@
     :param x: 
     :return: 
     """
-    print "Input"
-    for c in xrange(self.numberOfCols):
-      print int(x[c]),
-    print
+    print("Input")
+    for c in range(self.numberOfCols):
+      print(int(x[c]), end=' ')
+    print()
 
 
   def printParameters(self):
     """
     Print the parameter settings for the TM.
     """
-    print "numberOfCols=", self.numberOfCols
-    print "cellsPerColumn=", self.cellsPerColumn
-    print "minThreshold=", self.minThreshold
-    print "newSynapseCount=", self.newSynapseCount
-    print "activationThreshold=", self.activationThreshold
-    print
-    print "initialPerm=", self.initialPerm
-    print "connectedPerm=", self.connectedPerm
-    print "permanenceInc=", self.permanenceInc
-    print "permanenceDec=", self.permanenceDec
-    print "permanenceMax=", self.permanenceMax
-    print "globalDecay=", self.globalDecay
-    print
-    print "doPooling=", self.doPooling
-    print "segUpdateValidDuration=", self.segUpdateValidDuration
-    print "pamLength=", self.pamLength
+    print("numberOfCols=", self.numberOfCols)
+    print("cellsPerColumn=", self.cellsPerColumn)
+    print("minThreshold=", self.minThreshold)
+    print("newSynapseCount=", self.newSynapseCount)
+    print("activationThreshold=", self.activationThreshold)
+    print()
+    print("initialPerm=", self.initialPerm)
+    print("connectedPerm=", self.connectedPerm)
+    print("permanenceInc=", self.permanenceInc)
+    print("permanenceDec=", self.permanenceDec)
+    print("permanenceMax=", self.permanenceMax)
+    print("globalDecay=", self.globalDecay)
+    print()
+    print("doPooling=", self.doPooling)
+    print("segUpdateValidDuration=", self.segUpdateValidDuration)
+    print("pamLength=", self.pamLength)
 
 
   def printActiveIndices(self, state, andValues=False):
@@ -1154,15 +1154,15 @@
       cellIdxs = numpy.zeros(len(cols))
 
     if len(cols) == 0:
-      print "NONE"
+      print("NONE")
       return
 
     prevCol = -1
     for (col, cellIdx) in zip(cols, cellIdxs):
       if col != prevCol:
         if prevCol != -1:
-          print "] ",
-        print "Col %d: [" % (col),
+          print("] ", end=' ')
+        print("Col %d: [" % (col), end=' ')
         prevCol = col
 
       if andValues:
@@ -1170,10 +1170,10 @@
           value = state[col, cellIdx]
         else:
           value = state[col]
-        print "%d: %s," % (cellIdx, value),
+        print("%d: %s," % (cellIdx, value), end=' ')
       else:
-        print "%d," % (cellIdx),
-    print "]"
+        print("%d," % (cellIdx), end=' ')
+    print("]")
 
 
   def printComputeEnd(self, output, learn=False):
@@ -1185,67 +1185,67 @@
     :param learn: TODO: document
     """
     if self.verbosity >= 3:
-      print "----- computeEnd summary: "
-      print "learn:", learn
-      print "numBurstingCols: %s, " % (
-          self.infActiveState['t'].min(axis=1).sum()),
-      print "curPredScore2: %s, " % (
-          self._internalStats['curPredictionScore2']),
-      print "curFalsePosScore: %s, " % (
-          self._internalStats['curFalsePositiveScore']),
-      print "1-curFalseNegScore: %s, " % (
-          1 - self._internalStats['curFalseNegativeScore'])
-      print "numSegments: ", self.getNumSegments(),
-      print "avgLearnedSeqLength: ", self.avgLearnedSeqLength
-
-      print "----- infActiveState (%d on) ------" % (
-          self.infActiveState['t'].sum())
+      print("----- computeEnd summary: ")
+      print("learn:", learn)
+      print("numBurstingCols: %s, " % (
+          self.infActiveState['t'].min(axis=1).sum()), end=' ')
+      print("curPredScore2: %s, " % (
+          self._internalStats['curPredictionScore2']), end=' ')
+      print("curFalsePosScore: %s, " % (
+          self._internalStats['curFalsePositiveScore']), end=' ')
+      print("1-curFalseNegScore: %s, " % (
+          1 - self._internalStats['curFalseNegativeScore']))
+      print("numSegments: ", self.getNumSegments(), end=' ')
+      print("avgLearnedSeqLength: ", self.avgLearnedSeqLength)
+
+      print("----- infActiveState (%d on) ------" % (
+          self.infActiveState['t'].sum()))
       self.printActiveIndices(self.infActiveState['t'])
       if self.verbosity >= 6:
         self.printState(self.infActiveState['t'])
 
-      print "----- infPredictedState (%d on)-----" % (
-          self.infPredictedState['t'].sum())
+      print("----- infPredictedState (%d on)-----" % (
+          self.infPredictedState['t'].sum()))
       self.printActiveIndices(self.infPredictedState['t'])
       if self.verbosity >= 6:
         self.printState(self.infPredictedState['t'])
 
-      print "----- lrnActiveState (%d on) ------" % (
-          self.lrnActiveState['t'].sum())
+      print("----- lrnActiveState (%d on) ------" % (
+          self.lrnActiveState['t'].sum()))
       self.printActiveIndices(self.lrnActiveState['t'])
       if self.verbosity >= 6:
         self.printState(self.lrnActiveState['t'])
 
-      print "----- lrnPredictedState (%d on)-----" % (
-          self.lrnPredictedState['t'].sum())
+      print("----- lrnPredictedState (%d on)-----" % (
+          self.lrnPredictedState['t'].sum()))
       self.printActiveIndices(self.lrnPredictedState['t'])
       if self.verbosity >= 6:
         self.printState(self.lrnPredictedState['t'])
 
 
-      print "----- cellConfidence -----"
+      print("----- cellConfidence -----")
       self.printActiveIndices(self.cellConfidence['t'], andValues=True)
       if self.verbosity >= 6:
         self.printConfidence(self.cellConfidence['t'])
 
-      print "----- colConfidence -----"
+      print("----- colConfidence -----")
       self.printActiveIndices(self.colConfidence['t'], andValues=True)
 
-      print "----- cellConfidence[t-1] for currently active cells -----"
+      print("----- cellConfidence[t-1] for currently active cells -----")
       cc = self.cellConfidence['t-1'] * self.infActiveState['t']
       self.printActiveIndices(cc, andValues=True)
 
       if self.verbosity == 4:
-        print "Cells, predicted segments only:"
+        print("Cells, predicted segments only:")
         self.printCells(predictedOnly=True)
       elif self.verbosity >= 5:
-        print "Cells, all segments:"
+        print("Cells, all segments:")
         self.printCells(predictedOnly=False)
-      print
+      print()
 
     elif self.verbosity >= 1:
-      print "TM: learn:", learn
-      print "TM: active outputs(%d):" % len(output.nonzero()[0]),
+      print("TM: learn:", learn)
+      print("TM: active outputs(%d):" % len(output.nonzero()[0]), end=' ')
       self.printActiveIndices(output.reshape(self.numberOfCols,
                                              self.cellsPerColumn))
 
@@ -1256,10 +1256,10 @@
     
     :return: 
     """
-    print "=== SEGMENT UPDATES ===, Num = ", len(self.segmentUpdates)
-    for key, updateList in self.segmentUpdates.iteritems():
+    print("=== SEGMENT UPDATES ===, Num = ", len(self.segmentUpdates))
+    for key, updateList in self.segmentUpdates.items():
       c, i = key[0], key[1]
-      print c, i, updateList
+      print(c, i, updateList)
 
 
   def printCell(self, c, i, onlyActiveSegments=False):
@@ -1273,13 +1273,13 @@
     """
 
     if len(self.cells[c][i]) > 0:
-      print "Column", c, "Cell", i, ":",
-      print len(self.cells[c][i]), "segment(s)"
+      print("Column", c, "Cell", i, ":", end=' ')
+      print(len(self.cells[c][i]), "segment(s)")
       for j, s in enumerate(self.cells[c][i]):
         isActive = self._isSegmentActive(s, self.infActiveState['t'])
         if not onlyActiveSegments or isActive:
           isActiveStr = "*" if isActive else " "
-          print "  %sSeg #%-3d" % (isActiveStr, j),
+          print("  %sSeg #%-3d" % (isActiveStr, j), end=' ')
           s.debugPrint()
 
 
@@ -1291,15 +1291,15 @@
     :return: 
     """
     if predictedOnly:
-      print "--- PREDICTED CELLS ---"
+      print("--- PREDICTED CELLS ---")
     else:
-      print "--- ALL CELLS ---"
-    print "Activation threshold=", self.activationThreshold,
-    print "min threshold=", self.minThreshold,
-    print "connected perm=", self.connectedPerm
-
-    for c in xrange(self.numberOfCols):
-      for i in xrange(self.cellsPerColumn):
+      print("--- ALL CELLS ---")
+    print("Activation threshold=", self.activationThreshold, end=' ')
+    print("min threshold=", self.minThreshold, end=' ')
+    print("connected perm=", self.connectedPerm)
+
+    for c in range(self.numberOfCols):
+      for i in range(self.cellsPerColumn):
         if not predictedOnly or self.infPredictedState['t'][c, i]:
           self.printCell(c, i, predictedOnly)
 
@@ -1449,7 +1449,7 @@
 
     # TODO: scan list of updates for that cell and consolidate?
     # But watch out for dates!
-    if self.segmentUpdates.has_key(key):
+    if key in self.segmentUpdates:
       self.segmentUpdates[key] += [(self.lrnIterationIdx, segUpdate)]
     else:
       self.segmentUpdates[key] = [(self.lrnIterationIdx, segUpdate)]
@@ -1496,7 +1496,7 @@
       #  Columns refers to TM columns, even though each TM column is a row
       #  in the numpy array.
       numCols = self.currentOutput.shape[0]
-      self.currentOutput[(xrange(numCols), mostActiveCellPerCol)] = 1
+      self.currentOutput[(range(numCols), mostActiveCellPerCol)] = 1
 
       # Don't turn on anything in columns which are not active at all
       activeCols = self.infActiveState['t'].max(axis=1)
@@ -1826,10 +1826,10 @@
         break
 
       if self.verbosity >= 3:
-        print (
+        print((
             "Trying to lock-on using startCell state from %d steps ago:" % (
                 numPrevPatterns - 1 - startOffset),
-            self._prevInfPatterns[startOffset])
+            self._prevInfPatterns[startOffset]))
 
       # Play through starting from starting point 'startOffset'
       inSequence = False
@@ -1849,8 +1849,8 @@
 
         # Compute predictedState['t'] given activeState['t']
         if self.verbosity >= 3:
-          print ("  backtrack: computing predictions from ",
-                 self._prevInfPatterns[offset])
+          print(("  backtrack: computing predictions from ",
+                 self._prevInfPatterns[offset]))
         inSequence = self._inferPhase2()
         if not inSequence:
           break
@@ -1868,9 +1868,9 @@
       candStartOffset = startOffset
 
       if self.verbosity >= 3 and startOffset != currentTimeStepsOffset:
-        print ("  # Prediction confidence of current input after starting %d "
+        print(("  # Prediction confidence of current input after starting %d "
                "steps ago:" % (numPrevPatterns - 1 - startOffset),
-               totalConfidence)
+               totalConfidence))
 
       if candStartOffset == currentTimeStepsOffset:  # no more to try
         break
@@ -1885,15 +1885,15 @@
     # active state that we had on entry
     if candStartOffset is None:
       if self.verbosity >= 3:
-        print "Failed to lock on. Falling back to bursting all unpredicted."
+        print("Failed to lock on. Falling back to bursting all unpredicted.")
       self.infActiveState['t'][:, :] = self.infActiveState['backup'][:, :]
       self._inferPhase2()
 
     else:
       if self.verbosity >= 3:
-        print ("Locked on to current input by using start cells from %d "
+        print(("Locked on to current input by using start cells from %d "
                " steps ago:" % (numPrevPatterns - 1 - candStartOffset),
-               self._prevInfPatterns[candStartOffset])
+               self._prevInfPatterns[candStartOffset]))
       # Install the candidate state, if it wasn't the last one we evaluated.
       if candStartOffset != currentTimeStepsOffset:
         self.infActiveState['t'][:, :] = self.infActiveState['candidate'][:, :]
@@ -1908,8 +1908,8 @@
       if (i in badPatterns or
           (candStartOffset is not None and i <= candStartOffset)):
         if self.verbosity >= 3:
-          print ("Removing useless pattern from history:",
-                 self._prevInfPatterns[0])
+          print(("Removing useless pattern from history:",
+                 self._prevInfPatterns[0]))
         self._prevInfPatterns.pop(0)
       else:
         break
@@ -1997,10 +1997,10 @@
 
     # Phase 2 - Compute new predicted state and update cell and column
     #   confidences
-    for c in xrange(self.numberOfCols):
+    for c in range(self.numberOfCols):
 
       # For each cell in the column
-      for i in xrange(self.cellsPerColumn):
+      for i in range(self.cellsPerColumn):
 
         # For each segment in the cell
         for s in self.cells[c][i]:
@@ -2013,7 +2013,7 @@
 
           # Incorporate the confidence into the owner cell and column
           if self.verbosity >= 6:
-            print "incorporating DC from cell[%d,%d]:   " % (c, i),
+            print("incorporating DC from cell[%d,%d]:   " % (c, i), end=' ')
             s.debugPrint()
           dc = s.dutyCycle()
           self.cellConfidence['t'][c, i] += dc
@@ -2129,15 +2129,15 @@
     # Status message
     if self.verbosity >= 3:
       if readOnly:
-        print (
+        print((
             "Trying to lock-on using startCell state from %d steps ago:" % (
                 numPrevPatterns - 1 - startOffset),
-            self._prevLrnPatterns[startOffset])
+            self._prevLrnPatterns[startOffset]))
       else:
-        print (
+        print((
             "Locking on using startCell state from %d steps ago:" % (
                 numPrevPatterns - 1 - startOffset),
-            self._prevLrnPatterns[startOffset])
+            self._prevLrnPatterns[startOffset]))
 
     # Play through up to the current time step
     inSequence = True
@@ -2175,7 +2175,7 @@
       # Computes predictedState['t'] given activeState['t'] and also queues
       # up active segments into self.segmentUpdates, unless this is readOnly
       if self.verbosity >= 3:
-        print "  backtrack: computing predictions from ", inputColumns
+        print("  backtrack: computing predictions from ", inputColumns)
       self._learnPhase2(readOnly=readOnly)
 
     # Return whether or not this starting point was valid
@@ -2226,7 +2226,7 @@
     numPrevPatterns = len(self._prevLrnPatterns) - 1
     if numPrevPatterns <= 0:
       if self.verbosity >= 3:
-        print "lrnBacktrack: No available history to backtrack from"
+        print("lrnBacktrack: No available history to backtrack from")
       return False
 
     # We will record which previous input patterns did not generate predictions
@@ -2272,9 +2272,9 @@
     # We did find a valid starting point in the past. Now, we need to
     # re-enforce all segments that became active when following this path.
     if self.verbosity >= 3:
-      print ("Discovered path to current input by using start cells from %d "
+      print(("Discovered path to current input by using start cells from %d "
              "steps ago:" % (numPrevPatterns - startOffset),
-             self._prevLrnPatterns[startOffset])
+             self._prevLrnPatterns[startOffset]))
 
     self._learnBacktrackFrom(startOffset, readOnly=False)
 
@@ -2283,8 +2283,8 @@
     for i in range(numPrevPatterns):
       if i in badPatterns or i <= startOffset:
         if self.verbosity >= 3:
-          print ("Removing useless pattern from history:",
-                 self._prevLrnPatterns[0])
+          print(("Removing useless pattern from history:",
+                 self._prevLrnPatterns[0]))
         self._prevLrnPatterns.pop(0)
       else:
         break
@@ -2341,7 +2341,7 @@
           c, self.lrnActiveState['t-1'], self.minThreshold)
       if s is not None and s.isSequenceSegment():
         if self.verbosity >= 4:
-          print "Learn branch 0, found segment match. Learning on col=", c
+          print("Learn branch 0, found segment match. Learning on col=", c)
         self.lrnActiveState['t'][c, i] = 1
         segUpdate = self._getSegmentActiveSynapses(
             c, i, s, self.lrnActiveState['t-1'], newSynapses = True)
@@ -2358,8 +2358,8 @@
         # Choose a cell in this column to add a new segment to
         i = self._getCellForNewSegment(c)
         if (self.verbosity >= 4):
-          print "Learn branch 1, no match. Learning on col=", c,
-          print ", newCellIdxInCol=", i
+          print("Learn branch 1, no match. Learning on col=", c, end=' ')
+          print(", newCellIdxInCol=", i)
         self.lrnActiveState['t'][c, i] = 1
         segUpdate = self._getSegmentActiveSynapses(
             c, i, None, self.lrnActiveState['t-1'], newSynapses=True)
@@ -2400,7 +2400,7 @@
     # Compute new predicted state. When computing predictions for
     # phase 2, we predict at  most one cell per column (the one with the best
     # matching segment).
-    for c in xrange(self.numberOfCols):
+    for c in range(self.numberOfCols):
 
       # Is there a cell predicted to turn on in this column?
       i, s, numActive = self._getBestMatchingCell(
@@ -2448,8 +2448,8 @@
         self._prevLrnPatterns.pop(0)
       self._prevLrnPatterns.append(activeColumns)
       if self.verbosity >= 4:
-        print "Previous learn patterns: \n"
-        print self._prevLrnPatterns
+        print("Previous learn patterns: \n")
+        print(self._prevLrnPatterns)
 
     # Process queued up segment updates, now that we have bottom-up, we
     # can update the permanences on the cells that we predicted to turn on
@@ -2475,8 +2475,8 @@
 
     # Print status of PAM counter, learned sequence length
     if self.verbosity >= 3:
-      print "pamCounter = ", self.pamCounter, "seqLength = ", \
-          self.learnedSeqLength
+      print("pamCounter = ", self.pamCounter, "seqLength = ", \
+          self.learnedSeqLength)
 
     # Start over on start cells if any of the following occur:
     #  1.) A reset was just called
@@ -2495,11 +2495,11 @@
          self.learnedSeqLength >= self.maxSeqLength)):
       if  self.verbosity >= 3:
         if self.resetCalled:
-          print "Starting over:", activeColumns, "(reset was called)"
+          print("Starting over:", activeColumns, "(reset was called)")
         elif self.pamCounter == 0:
-          print "Starting over:", activeColumns, "(PAM counter expired)"
+          print("Starting over:", activeColumns, "(PAM counter expired)")
         else:
-          print "Starting over:", activeColumns, "(reached maxSeqLength)"
+          print("Starting over:", activeColumns, "(reached maxSeqLength)")
 
       # Update average learned sequence length - this is a diagnostic statistic
       if self.pamCounter == 0:
@@ -2507,7 +2507,7 @@
       else:
         seqLength = self.learnedSeqLength
       if  self.verbosity >= 3:
-        print "  learned sequence length was:", seqLength
+        print("  learned sequence length was:", seqLength)
       self._updateAvgLearnedSeqLength(seqLength)
 
       # Backtrack to an earlier starting point, if we find one
@@ -2577,8 +2577,8 @@
     self.iterationIdx +=  1
 
     if self.verbosity >= 3:
-      print "\n==== PY Iteration: %d =====" % (self.iterationIdx)
-      print "Active cols:", activeColumns
+      print("\n==== PY Iteration: %d =====" % (self.iterationIdx))
+      print("Active cols:", activeColumns)
 
     # Update segment duty cycles if we are crossing a "tier"
     # We determine if it's time to update the segment duty cycles. Since the
@@ -2586,8 +2586,8 @@
     # important that we update all segments on each tier boundary
     if enableLearn:
       if self.lrnIterationIdx in Segment.dutyCycleTiers:
-        for c, i in itertools.product(xrange(self.numberOfCols),
-                                      xrange(self.cellsPerColumn)):
+        for c, i in itertools.product(range(self.numberOfCols),
+                                      range(self.cellsPerColumn)):
           for segment in self.cells[c][i]:
             segment.dutyCycle()
 
@@ -2616,8 +2616,8 @@
       # it can be called in adaptSegments, in the case where we
       # do global decay only episodically.
       if self.globalDecay > 0.0 and ((self.lrnIterationIdx % self.maxAge) == 0):
-        for c, i in itertools.product(xrange(self.numberOfCols),
-                                      xrange(self.cellsPerColumn)):
+        for c, i in itertools.product(range(self.numberOfCols),
+                                      range(self.cellsPerColumn)):
 
           segsToDel = [] # collect and remove outside the loop
           for segment in self.cells[c][i]:
@@ -2784,8 +2784,8 @@
 
     # Loop through all cells
     totalSegsRemoved, totalSynsRemoved = 0, 0
-    for c, i in itertools.product(xrange(self.numberOfCols),
-                                  xrange(self.cellsPerColumn)):
+    for c, i in itertools.product(range(self.numberOfCols),
+                                  range(self.cellsPerColumn)):
 
       (segsRemoved, synsRemoved) = self._trimSegmentsInCell(
           colIdx=c, cellIdx=i, segList=self.cells[c][i],
@@ -2795,7 +2795,7 @@
 
     # Print all cells if verbosity says to
     if self.verbosity >= 5:
-      print "Cells, all segments:"
+      print("Cells, all segments:")
       self.printCells(predictedOnly=False)
 
     return totalSegsRemoved, totalSynsRemoved
@@ -2813,7 +2813,7 @@
     """
     # TODO: check if the situation described in the docstring above actually
     #       occurs.
-    for key, updateList in self.segmentUpdates.iteritems():
+    for key, updateList in self.segmentUpdates.items():
       c, i = key[0], key[1]
       if c == col and i == cellIdx:
         for update in updateList:
@@ -2833,15 +2833,15 @@
 
     # Update all cached duty cycles for better performance right after loading
     # in the trained network.
-    for c, i in itertools.product(xrange(self.numberOfCols),
-                                  xrange(self.cellsPerColumn)):
+    for c, i in itertools.product(range(self.numberOfCols),
+                                  range(self.cellsPerColumn)):
       for segment in self.cells[c][i]:
         segment.dutyCycle()
 
     # For error checking purposes, make sure no start cell has incoming
     # connections
     if self.cellsPerColumn > 1:
-      for c in xrange(self.numberOfCols):
+      for c in range(self.numberOfCols):
         assert self.getNumSegmentsInCell(c, 0) == 0
 
 
@@ -2919,7 +2919,7 @@
 
     # Assign confidences to each pattern
     confidences = []
-    for i in xrange(numPatterns):
+    for i in range(numPatterns):
       # Sum of the column confidences for this pattern
       positivePredictionSum = colConfidence[patternNZs[i]].sum()
       # How many columns in this pattern
@@ -3017,7 +3017,7 @@
     bestSegIdxInCol = -1
     bestCellInCol = -1
 
-    for i in xrange(self.cellsPerColumn):
+    for i in range(self.cellsPerColumn):
 
       maxSegActivity = 0
       maxSegIdx = 0
@@ -3106,7 +3106,7 @@
     else:
       minIdx = 1                      # Don't include startCell in the mix
       maxIdx = self.cellsPerColumn-1
-    for i in xrange(minIdx, maxIdx+1):
+    for i in range(minIdx, maxIdx+1):
       numSegs = len(self.cells[colIdx][i])
       if numSegs < self.maxSegmentsPerCell:
         candidateCellIdxs.append(i)
@@ -3118,15 +3118,15 @@
       candidateCellIdx = (
           candidateCellIdxs[self._random.getUInt32(len(candidateCellIdxs))])
       if self.verbosity >= 5:
-        print "Cell [%d,%d] chosen for new segment, # of segs is %d" % (
-            colIdx, candidateCellIdx, len(self.cells[colIdx][candidateCellIdx]))
+        print("Cell [%d,%d] chosen for new segment, # of segs is %d" % (
+            colIdx, candidateCellIdx, len(self.cells[colIdx][candidateCellIdx])))
       return candidateCellIdx
 
     # All cells in the column are full, find a segment to free up
     candidateSegment = None
     candidateSegmentDC = 1.0
     # For each cell in this column
-    for i in xrange(minIdx, maxIdx+1):
+    for i in range(minIdx, maxIdx+1):
       # For each segment in this cell
       for s in self.cells[colIdx][i]:
         dc = s.dutyCycle()
@@ -3137,8 +3137,8 @@
 
     # Free up the least used segment
     if self.verbosity >= 5:
-      print ("Deleting segment #%d for cell[%d,%d] to make room for new "
-             "segment" % (candidateSegment.segID, colIdx, candidateCellIdx))
+      print(("Deleting segment #%d for cell[%d,%d] to make room for new "
+             "segment" % (candidateSegment.segID, colIdx, candidateCellIdx)))
       candidateSegment.debugPrint()
     self._cleanUpdatesList(colIdx, candidateCellIdx, candidateSegment)
     self.cells[colIdx][candidateCellIdx].remove(candidateSegment)
@@ -3251,7 +3251,7 @@
     # owner cell. The values are lists of segment updates for that cell
     removeKeys = []
     trimSegments = []
-    for key, updateList in self.segmentUpdates.iteritems():
+    for key, updateList in self.segmentUpdates.items():
 
       # Get the column number and cell index of the owner cell
       c, i = key[0], key[1]
@@ -3275,8 +3275,8 @@
         for (createDate, segUpdate) in updateList:
 
           if self.verbosity >= 4:
-            print "_nLrnIterations =", self.lrnIterationIdx,
-            print segUpdate
+            print("_nLrnIterations =", self.lrnIterationIdx, end=' ')
+            print(segUpdate)
 
           # If this segment has expired. Ignore this update (and hence remove it
           # from list)
@@ -3343,8 +3343,8 @@
     if segment is not None:
 
       if self.verbosity >= 4:
-        print "Reinforcing segment #%d for cell[%d,%d]" % (segment.segID, c, i)
-        print "  before:",
+        print("Reinforcing segment #%d for cell[%d,%d]" % (segment.segID, c, i))
+        print("  before:", end=' ')
         segment.debugPrint()
 
       # Mark it as recently useful
@@ -3358,7 +3358,7 @@
       # s is a synapse *index*, with index 0 in the segment being the tuple
       # (segId, sequence segment flag). See below, creation of segments.
       lastSynIndex = len(segment.syns) - 1
-      inactiveSynIndices = [s for s in xrange(0, lastSynIndex+1) \
+      inactiveSynIndices = [s for s in range(0, lastSynIndex+1) \
                             if s not in synToUpdate]
       trimSegment = segment.updateSynapses(inactiveSynIndices,
                                            -self.permanenceDec)
@@ -3380,7 +3380,7 @@
         segment.addSynapse(newSyn[0], newSyn[1], self.initialPerm)
 
       if self.verbosity >= 4:
-        print "   after:",
+        print("   after:", end=' ')
         segment.debugPrint()
 
     # Create a new segment
@@ -3395,7 +3395,7 @@
         newSegment.addSynapse(synapse[0], synapse[1], self.initialPerm)
 
       if self.verbosity >= 3:
-        print "New segment #%d for cell[%d,%d]" % (self.segID-1, c, i),
+        print("New segment #%d for cell[%d,%d]" % (self.segID-1, c, i), end=' ')
         newSegment.debugPrint()
 
       self.cells[c][i].append(newSegment)
@@ -3445,20 +3445,20 @@
     for i in range(numAgeBuckets):
       distAges.append(['%d-%d' % (i*ageBucketSize, (i+1)*ageBucketSize-1), 0])
 
-    for c in xrange(self.numberOfCols):
-      for i in xrange(self.cellsPerColumn):
+    for c in range(self.numberOfCols):
+      for i in range(self.cellsPerColumn):
 
         if len(self.cells[c][i]) > 0:
           nSegmentsThisCell = len(self.cells[c][i])
           nSegments += nSegmentsThisCell
-          if distNSegsPerCell.has_key(nSegmentsThisCell):
+          if nSegmentsThisCell in distNSegsPerCell:
             distNSegsPerCell[nSegmentsThisCell] += 1
           else:
             distNSegsPerCell[nSegmentsThisCell] = 1
           for seg in self.cells[c][i]:
             nSynapsesThisSeg = seg.getNumSynapses()
             nSynapses += nSynapsesThisSeg
-            if distSegSizes.has_key(nSynapsesThisSeg):
+            if nSynapsesThisSeg in distSegSizes:
               distSegSizes[nSynapsesThisSeg] += 1
             else:
               distSegSizes[nSynapsesThisSeg] = 1
@@ -3466,7 +3466,7 @@
             # Accumulate permanence value histogram
             for syn in seg.syns:
               p = int(syn[2]*10)
-              if distPermValues.has_key(p):
+              if p in distPermValues:
                 distPermValues[p] += 1
               else:
                 distPermValues[p] = 1
@@ -3680,23 +3680,23 @@
       [11,1]0.75 - synapse from column 11, cell #1, strength 0.75
     """
     # Segment ID
-    print "ID:%-5d" % (self.segID),
+    print("ID:%-5d" % (self.segID), end=' ')
 
     # Sequence segment or pooling segment
     if self.isSequenceSeg:
-      print "True",
+      print("True", end=' ')
     else:
-      print "False",
+      print("False", end=' ')
 
     # Duty cycle
-    print "%9.7f" % (self.dutyCycle(readOnly=True)),
+    print("%9.7f" % (self.dutyCycle(readOnly=True)), end=' ')
 
     # numPositive/totalActivations
-    print "(%4d/%-4d)" % (self.positiveActivations,
-                          self.totalActivations),
+    print("(%4d/%-4d)" % (self.positiveActivations,
+                          self.totalActivations), end=' ')
 
     # Age
-    print "%4d" % (self.tm.lrnIterationIdx - self.lastActiveIteration),
+    print("%4d" % (self.tm.lrnIterationIdx - self.lastActiveIteration), end=' ')
 
     # Print each synapses on this segment as: srcCellCol/srcCellIdx/perm
     # if the permanence is above connected, put [] around the synapse info
@@ -3704,8 +3704,8 @@
     #  order
     sortedSyns = sorted(self.syns)
     for _, synapse in enumerate(sortedSyns):
-      print "[%d,%d]%4.2f" % (synapse[0], synapse[1], synapse[2]),
-    print
+      print("[%d,%d]%4.2f" % (synapse[0], synapse[1], synapse[2]), end=' ')
+    print()
 
 
   def isSequenceSegment(self):
@@ -3728,11 +3728,11 @@
     assert (numToFree <= len(self.syns))
 
     if (verbosity >= 4):
-      print "\nIn PY freeNSynapses with numToFree =", numToFree,
-      print "inactiveSynapseIndices =",
+      print("\nIn PY freeNSynapses with numToFree =", numToFree, end=' ')
+      print("inactiveSynapseIndices =", end=' ')
       for i in inactiveSynapseIndices:
-        print self.syns[i][0:2],
-      print
+        print(self.syns[i][0:2], end=' ')
+      print()
 
     # Remove the lowest perm inactive synapses first
     if len(inactiveSynapseIndices) > 0:
@@ -3745,7 +3745,7 @@
 
     # Do we need more? if so, remove the lowest perm active synapses too
     if len(candidates) < numToFree:
-      activeSynIndices = [i for i in xrange(len(self.syns))
+      activeSynIndices = [i for i in range(len(self.syns))
                           if i not in inactiveSynapseIndices]
       perms = numpy.array([self.syns[i][2] for i in activeSynIndices])
       moreToFree = numToFree - len(candidates)
@@ -3754,9 +3754,9 @@
       candidates += list(moreCandidates)
 
     if verbosity >= 4:
-      print "Deleting %d synapses from segment to make room for new ones:" % (
-          len(candidates)), candidates
-      print "BEFORE:",
+      print("Deleting %d synapses from segment to make room for new ones:" % (
+          len(candidates)), candidates)
+      print("BEFORE:", end=' ')
       self.debugPrint()
 
     # Free up all the candidates now
@@ -3765,7 +3765,7 @@
       self.syns.remove(syn)
 
     if verbosity >= 4:
-      print "AFTER:",
+      print("AFTER:", end=' ')
       self.debugPrint()
 
 
--- d:\nupic\src\python\python27\nupic\algorithms\backtracking_tm_cpp.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\backtracking_tm_cpp.py	(refactored)
@@ -195,7 +195,7 @@
 
     # Additional CPP-specific deserialization
     newCells4 = Cells4.read(proto.cells4)
-    print newCells4
+    print(newCells4)
     obj.cells4 = newCells4
     obj.makeCells4Ephemeral = proto.makeCells4Ephemeral
     obj.seed = proto.seed
@@ -455,7 +455,7 @@
     Overrides :meth:`nupic.algorithms.backtracking_tm.BacktrackingTM.reset`.
     """
     if self.verbosity >= 3:
-      print "TM Reset"
+      print("TM Reset")
     self._setStatePointers()
     self.cells4.reset()
     BacktrackingTM.reset(self)
@@ -483,7 +483,7 @@
 
     # Print all cells if verbosity says to
     if self.verbosity >= 5:
-      print "Cells, all segments:"
+      print("Cells, all segments:")
       self.printCells(predictedOnly=False)
 
     return self.cells4.trimSegments(minPermanence=minPermanence, minNumSyns=minNumSyns)
@@ -503,12 +503,12 @@
 
     # Sequence segment or pooling segment
     if s[0][1] == True:
-      print "S",
+      print("S", end=' ')
     else:
-      print 'P',
+      print('P', end=' ')
 
     # Frequency count
-    print s[0][2],
+    print(s[0][2], end=' ')
 
     if self._isSegmentActive(s, 't'):
       ss = '[' + str(currAct) + ']'
@@ -520,7 +520,7 @@
     else:
       ss = ss + str(prevAct)
     ss = ss + ':'
-    print ss,
+    print(ss, end=' ')
 
     for i,synapse in enumerate(s[1:]):
 
@@ -538,22 +538,22 @@
         ss = ss + ']'
       if i < len(s)-2:
         ss = ss + ' |'
-      print ss,
+      print(ss, end=' ')
 
     if self.verbosity > 3:
       if self._isSegmentActive(s, 't') and \
              prevAct < self.activationThreshold and currAct >= self.activationThreshold:
-        print "reached activation",
+        print("reached activation", end=' ')
       if prevAct < self.minThreshold and currAct >= self.minThreshold:
-        print "reached min threshold",
+        print("reached min threshold", end=' ')
       if self._isSegmentActive(s, 't-1') and \
              prevAct >= self.activationThreshold and currAct < self.activationThreshold:
-        print "dropped below activation",
+        print("dropped below activation", end=' ')
       if prevAct >= self.minThreshold and currAct < self.minThreshold:
-        print "dropped below min",
+        print("dropped below min", end=' ')
       if self._isSegmentActive(s, 't') and self._isSegmentActive(s, 't-1') and \
              prevAct >= self.activationThreshold and currAct >= self.activationThreshold:
-        print "maintained activation",
+        print("maintained activation", end=' ')
 
   def printSegmentUpdates(self):
     """
@@ -561,10 +561,10 @@
     """
     # TODO: need to add C++ accessors to implement this method
     assert False
-    print "=== SEGMENT UPDATES ===, Num = ", len(self.segmentUpdates)
-    for key, updateList in self.segmentUpdates.iteritems():
+    print("=== SEGMENT UPDATES ===, Num = ", len(self.segmentUpdates))
+    for key, updateList in self.segmentUpdates.items():
       c,i = key[0],key[1]
-      print c,i,updateList
+      print(c,i,updateList)
 
 
   def _slowIsSegmentActive(self, seg, timeStep):
@@ -576,7 +576,7 @@
 
     numSyn = seg.size()
     numActiveSyns = 0
-    for synIdx in xrange(numSyn):
+    for synIdx in range(numSyn):
       if seg.getPermanence(synIdx) < self.connectedPerm:
         continue
       sc, si = self.getColCellIdx(seg.getSrcCellIdx(synIdx))
@@ -596,30 +596,30 @@
     if nSegs > 0:
       segList = self.cells4.getNonEmptySegList(c,i)
       gidx = c * self.cellsPerColumn + i
-      print "Column", c, "Cell", i, "(%d)"%(gidx),":", nSegs, "segment(s)"
+      print("Column", c, "Cell", i, "(%d)"%(gidx),":", nSegs, "segment(s)")
       for k,segIdx in enumerate(segList):
         seg = self.cells4.getSegment(c, i, segIdx)
         isActive = self._slowIsSegmentActive(seg, 't')
         if onlyActiveSegments and not isActive:
           continue
         isActiveStr = "*" if isActive else " "
-        print "  %sSeg #%-3d" % (isActiveStr, segIdx),
-        print seg.size(),
-        print seg.isSequenceSegment(), "%9.7f" % (seg.dutyCycle(
-              self.cells4.getNLrnIterations(), False, True)),
+        print("  %sSeg #%-3d" % (isActiveStr, segIdx), end=' ')
+        print(seg.size(), end=' ')
+        print(seg.isSequenceSegment(), "%9.7f" % (seg.dutyCycle(
+              self.cells4.getNLrnIterations(), False, True)), end=' ')
 
         # numPositive/totalActivations
-        print "(%4d/%-4d)" % (seg.getPositiveActivations(),
-                           seg.getTotalActivations()),
+        print("(%4d/%-4d)" % (seg.getPositiveActivations(),
+                           seg.getTotalActivations()), end=' ')
         # Age
-        print "%4d" % (self.cells4.getNLrnIterations()
-                       - seg.getLastActiveIteration()),
+        print("%4d" % (self.cells4.getNLrnIterations()
+                       - seg.getLastActiveIteration()), end=' ')
 
         numSyn = seg.size()
-        for s in xrange(numSyn):
+        for s in range(numSyn):
           sc, si = self.getColCellIdx(seg.getSrcCellIdx(s))
-          print "[%d,%d]%4.2f"%(sc, si, seg.getPermanence(s)),
-        print
+          print("[%d,%d]%4.2f"%(sc, si, seg.getPermanence(s)), end=' ')
+        print()
 
 
   def getAvgLearnedSeqLength(self):
@@ -659,7 +659,7 @@
                    seg.getLastPosDutyCycle(),
                    seg.getLastPosDutyCycleIteration()])
 
-    for s in xrange(numSyn):
+    for s in range(numSyn):
       sc, si = self.getColCellIdx(seg.getSrcCellIdx(s))
       result.append([int(sc), int(si), seg.getPermanence(s)])
 
@@ -706,24 +706,24 @@
       distAges.append(['%d-%d' % (i*ageBucketSize, (i+1)*ageBucketSize-1), 0])
 
 
-    for c in xrange(self.numberOfCols):
-      for i in xrange(self.cellsPerColumn):
+    for c in range(self.numberOfCols):
+      for i in range(self.cellsPerColumn):
 
         # Update histogram counting cell sizes
         nSegmentsThisCell = self.getNumSegmentsInCell(c,i)
         if nSegmentsThisCell > 0:
-          if distNSegsPerCell.has_key(nSegmentsThisCell):
+          if nSegmentsThisCell in distNSegsPerCell:
             distNSegsPerCell[nSegmentsThisCell] += 1
           else:
             distNSegsPerCell[nSegmentsThisCell] = 1
 
           # Update histogram counting segment sizes.
           segList = self.cells4.getNonEmptySegList(c,i)
-          for segIdx in xrange(nSegmentsThisCell):
+          for segIdx in range(nSegmentsThisCell):
             seg = self.getSegmentOnCell(c, i, segIdx)
             nSynapsesThisSeg = len(seg) - 1
             if nSynapsesThisSeg > 0:
-              if distSegSizes.has_key(nSynapsesThisSeg):
+              if nSynapsesThisSeg in distSegSizes:
                 distSegSizes[nSynapsesThisSeg] += 1
               else:
                 distSegSizes[nSynapsesThisSeg] = 1
@@ -731,7 +731,7 @@
               # Accumulate permanence value histogram
               for syn in seg[1:]:
                 p = int(syn[2]*10)
-                if distPermValues.has_key(p):
+                if p in distPermValues:
                   distPermValues[p] += 1
                 else:
                   distPermValues[p] = 1
--- d:\nupic\src\python\python27\nupic\algorithms\connections.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\connections.py	(refactored)
@@ -136,7 +136,7 @@
     # Save member variables
     self.numCells = numCells
 
-    self._cells = [CellData() for _ in xrange(numCells)]
+    self._cells = [CellData() for _ in range(numCells)]
     self._synapsesForPresynapticCell = defaultdict(set)
     self._segmentForFlatIdx = []
 
@@ -146,8 +146,8 @@
 
     # Whenever creating a new Synapse or Segment, give it a unique ordinal.
     # These can be used to sort synapses or segments by age.
-    self._nextSynapseOrdinal = long(0)
-    self._nextSegmentOrdinal = long(0)
+    self._nextSynapseOrdinal = int(0)
+    self._nextSegmentOrdinal = int(0)
 
 
   def segmentsForCell(self, cell):
@@ -430,7 +430,7 @@
     """
     protoCells = proto.init('cells', self.numCells)
 
-    for i in xrange(self.numCells):
+    for i in range(self.numCells):
       segments = self._cells[i]._segments
       protoSegments = protoCells[i].init('segments', len(segments))
 
@@ -495,14 +495,14 @@
     :param other: (:class:`Connections`) Connections instance to compare to
     """
     #pylint: disable=W0212
-    for i in xrange(self.numCells):
+    for i in range(self.numCells):
       segments = self._cells[i]._segments
       otherSegments = other._cells[i]._segments
 
       if len(segments) != len(otherSegments):
         return False
 
-      for j in xrange(len(segments)):
+      for j in range(len(segments)):
         segment = segments[j]
         otherSegment = otherSegments[j]
         synapses = segment._synapses
@@ -525,7 +525,7 @@
         len(self._synapsesForPresynapticCell)):
       return False
 
-    for i in self._synapsesForPresynapticCell.keys():
+    for i in list(self._synapsesForPresynapticCell.keys()):
       synapses = self._synapsesForPresynapticCell[i]
       otherSynapses = other._synapsesForPresynapticCell[i]
       if len(synapses) != len(otherSynapses):
--- d:\nupic\src\python\python27\nupic\algorithms\fdrutilities.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\fdrutilities.py	(refactored)
@@ -80,8 +80,8 @@
 
   coincMatrix0 = SM32(int(nCoinc), int(length))
   theOnes = numpy.array([1.0] * activity, dtype=numpy.float32)
-  for rowIdx in xrange(nCoinc):
-    coinc = numpy.array(random.sample(xrange(length),
+  for rowIdx in range(nCoinc):
+    coinc = numpy.array(random.sample(range(length),
                 activity), dtype=numpy.uint32)
     coinc.sort()
     coincMatrix0.setRowFromSparse(rowIdx, coinc, theOnes)
@@ -114,9 +114,9 @@
 
   vectors = []
   coinc = numpy.zeros(length, dtype='int32')
-  indexList = range(length)
-
-  for i in xrange(numVectors):
+  indexList = list(range(length))
+
+  for i in range(numVectors):
       coinc[:] = 0
       coinc[random.sample(indexList, activity)] = 1
       vectors.append(coinc.copy())
@@ -142,16 +142,16 @@
                containing the coincidence indices for that sequence.
   """
 
-  coincList = range(nCoinc)
+  coincList = list(range(nCoinc))
   seqList  = []
 
-  for i in xrange(nSeq):
+  for i in range(nSeq):
     if max(seqLength) <= nCoinc:
       seqList.append(random.sample(coincList, random.choice(seqLength)))
     else:
       len = random.choice(seqLength)
       seq = []
-      for x in xrange(len):
+      for x in range(len):
         seq.append(random.choice(coincList))
       seqList.append(seq)
 
@@ -180,12 +180,12 @@
   """
 
 
-  coincList = range(nCoinc)
+  coincList = list(range(nCoinc))
   for hub in hubs:
     coincList.remove(hub)
 
   seqList = []
-  for i in xrange(nSeq):
+  for i in range(nSeq):
     length = random.choice(seqLength)-1
     seq = random.sample(coincList,length)
     seq.insert(length//2, random.choice(hubs))
@@ -225,11 +225,11 @@
   #           activity=patternActivity)
 
   similarity = []
-  for i in xrange(nPatterns):
+  for i in range(nPatterns):
      similarity.append(patterns.rightVecProd(patterns.getRow(i)))
   similarity = numpy.array(similarity, dtype='int32')
 
-  print similarity
+  print(similarity)
 
 
   # Create the raw sequences
@@ -266,7 +266,7 @@
   coincMatrix = SM32(0, length)
   coinc = numpy.zeros(length, dtype='int32')
 
-  for i in xrange(nCoinc):
+  for i in range(nCoinc):
       coinc[:] = 0
       coinc[i*activity:(i+1)*activity] = 1
       coincMatrix.addRow(coinc)
@@ -451,8 +451,8 @@
                 "doPooling", "segUpdateValidDuration",
                 "burnIn", "pamLength", "maxAge"]:
     if getattr(tp1, param) != getattr(tp2,param):
-      print param,"is different"
-      print getattr(tp1, param), "vs", getattr(tp2,param)
+      print(param,"is different")
+      print(getattr(tp1, param), "vs", getattr(tp2,param))
       result = False
   return result
 
@@ -483,12 +483,12 @@
   # Now compare synapses, ignoring order of synapses
   for syn in seg2[1:]:
     if syn[2] <= 0:
-      print "A synapse with zero permanence encountered"
+      print("A synapse with zero permanence encountered")
       result = False
   if result == True:
     for syn in seg1[1:]:
       if syn[2] <= 0:
-        print "A synapse with zero permanence encountered"
+        print("A synapse with zero permanence encountered")
         result = False
       res = sameSynapse(syn, seg2[1:])
       if res == False:
@@ -509,7 +509,7 @@
 
   # First check basic parameters. If we fail here, don't continue
   if sameTMParams(tm1, tm2) == False:
-    print "Two TM's have different parameters"
+    print("Two TM's have different parameters")
     return False
 
   result = True
@@ -518,32 +518,32 @@
   # cells starts diverging
 
   if (tm1.activeState['t'] != tm2.activeState['t']).any():
-    print 'Active states diverge', numpy.where(tm1.activeState['t'] != tm2.activeState['t'])
+    print('Active states diverge', numpy.where(tm1.activeState['t'] != tm2.activeState['t']))
     result = False
 
   if (tm1.predictedState['t'] - tm2.predictedState['t']).any():
-    print 'Predicted states diverge', numpy.where(tm1.predictedState['t'] != tm2.predictedState['t'])
+    print('Predicted states diverge', numpy.where(tm1.predictedState['t'] != tm2.predictedState['t']))
     result = False
 
   # TODO: check confidence at T (confT)
 
   # Now check some high level learned parameters.
   if tm1.getNumSegments() != tm2.getNumSegments():
-    print "Number of segments are different", tm1.getNumSegments(), tm2.getNumSegments()
+    print("Number of segments are different", tm1.getNumSegments(), tm2.getNumSegments())
     result = False
 
   if tm1.getNumSynapses() != tm2.getNumSynapses():
-    print "Number of synapses are different", tm1.getNumSynapses(), tm2.getNumSynapses()
+    print("Number of synapses are different", tm1.getNumSynapses(), tm2.getNumSynapses())
     tm1.printCells()
     tm2.printCells()
     result = False
 
   # Check that each cell has the same number of segments and synapses
-  for c in xrange(tm1.numberOfCols):
-    for i in xrange(tm2.cellsPerColumn):
+  for c in range(tm1.numberOfCols):
+    for i in range(tm2.cellsPerColumn):
       if tm1.getNumSegmentsInCell(c, i) != tm2.getNumSegmentsInCell(c, i):
-        print "Num segments different in cell:",c,i,
-        print tm1.getNumSegmentsInCell(c, i), tm2.getNumSegmentsInCell(c, i)
+        print("Num segments different in cell:",c,i, end=' ')
+        print(tm1.getNumSegmentsInCell(c, i), tm2.getNumSegmentsInCell(c, i))
         result = False
 
   # If the above tests pass, then check each segment and report differences
@@ -551,30 +551,30 @@
   # make sure that, for each segment in tm1, there is an identical segment
   # in tm2.
   if result == True and not relaxSegmentTests:
-    for c in xrange(tm1.numberOfCols):
-      for i in xrange(tm2.cellsPerColumn):
+    for c in range(tm1.numberOfCols):
+      for i in range(tm2.cellsPerColumn):
         nSegs = tm1.getNumSegmentsInCell(c, i)
-        for segIdx in xrange(nSegs):
+        for segIdx in range(nSegs):
           tm1seg = tm1.getSegmentOnCell(c, i, segIdx)
 
           # Loop through all segments in tm2seg and see if any of them match tm1seg
           res = False
-          for tm2segIdx in xrange(nSegs):
+          for tm2segIdx in range(nSegs):
             tm2seg = tm2.getSegmentOnCell(c, i, tm2segIdx)
             if sameSegment(tm1seg, tm2seg) == True:
               res = True
               break
           if res == False:
-            print "\nSegments are different for cell:",c,i
+            print("\nSegments are different for cell:",c,i)
             if verbosity >= 1:
-              print "C++"
+              print("C++")
               tm1.printCell(c, i)
-              print "Py"
+              print("Py")
               tm2.printCell(c, i)
             result = False
 
   if result == True and (verbosity > 1):
-    print "TM's match"
+    print("TM's match")
 
   return result
 
@@ -596,7 +596,7 @@
 
   # First check basic parameters. If we fail here, don't continue
   if sameTMParams(tm1, tm2) == False:
-    print "Two TM's have different parameters"
+    print("Two TM's have different parameters")
     return False
 
   tm1Label = "<tm_1 (%s)>" % tm1.__class__.__name__
@@ -609,48 +609,48 @@
     # cells starts diverging
 
     if (tm1.infActiveState['t'] != tm2.infActiveState['t']).any():
-      print 'Active states diverged', numpy.where(tm1.infActiveState['t'] != tm2.infActiveState['t'])
+      print('Active states diverged', numpy.where(tm1.infActiveState['t'] != tm2.infActiveState['t']))
       result = False
 
     if (tm1.infPredictedState['t'] - tm2.infPredictedState['t']).any():
-      print 'Predicted states diverged', numpy.where(tm1.infPredictedState['t'] != tm2.infPredictedState['t'])
+      print('Predicted states diverged', numpy.where(tm1.infPredictedState['t'] != tm2.infPredictedState['t']))
       result = False
 
     if checkLearn and (tm1.lrnActiveState['t'] - tm2.lrnActiveState['t']).any():
-      print 'lrnActiveState[t] diverged', numpy.where(tm1.lrnActiveState['t'] != tm2.lrnActiveState['t'])
+      print('lrnActiveState[t] diverged', numpy.where(tm1.lrnActiveState['t'] != tm2.lrnActiveState['t']))
       result = False
 
     if checkLearn and (tm1.lrnPredictedState['t'] - tm2.lrnPredictedState['t']).any():
-      print 'lrnPredictedState[t] diverged', numpy.where(tm1.lrnPredictedState['t'] != tm2.lrnPredictedState['t'])
+      print('lrnPredictedState[t] diverged', numpy.where(tm1.lrnPredictedState['t'] != tm2.lrnPredictedState['t']))
       result = False
 
     if checkLearn and abs(tm1.getAvgLearnedSeqLength() - tm2.getAvgLearnedSeqLength()) > 0.01:
-      print "Average learned sequence lengths differ: ",
-      print tm1.getAvgLearnedSeqLength(), " vs ", tm2.getAvgLearnedSeqLength()
+      print("Average learned sequence lengths differ: ", end=' ')
+      print(tm1.getAvgLearnedSeqLength(), " vs ", tm2.getAvgLearnedSeqLength())
       result = False
 
   # TODO: check confidence at T (confT)
 
   # Now check some high level learned parameters.
   if tm1.getNumSegments() != tm2.getNumSegments():
-    print "Number of segments are different", tm1.getNumSegments(), tm2.getNumSegments()
+    print("Number of segments are different", tm1.getNumSegments(), tm2.getNumSegments())
     result = False
 
   if tm1.getNumSynapses() != tm2.getNumSynapses():
-    print "Number of synapses are different", tm1.getNumSynapses(), tm2.getNumSynapses()
+    print("Number of synapses are different", tm1.getNumSynapses(), tm2.getNumSynapses())
     if verbosity >= 3:
-      print "%s: " % tm1Label,
+      print("%s: " % tm1Label, end=' ')
       tm1.printCells()
-      print "\n%s  : " % tm2Label,
+      print("\n%s  : " % tm2Label, end=' ')
       tm2.printCells()
     #result = False
 
   # Check that each cell has the same number of segments and synapses
-  for c in xrange(tm1.numberOfCols):
-    for i in xrange(tm2.cellsPerColumn):
+  for c in range(tm1.numberOfCols):
+    for i in range(tm2.cellsPerColumn):
       if tm1.getNumSegmentsInCell(c, i) != tm2.getNumSegmentsInCell(c, i):
-        print "Num segments different in cell:",c,i,
-        print tm1.getNumSegmentsInCell(c, i), tm2.getNumSegmentsInCell(c, i)
+        print("Num segments different in cell:",c,i, end=' ')
+        print(tm1.getNumSegmentsInCell(c, i), tm2.getNumSegmentsInCell(c, i))
         result = False
 
   # If the above tests pass, then check each segment and report differences
@@ -658,30 +658,30 @@
   # make sure that, for each segment in tm1, there is an identical segment
   # in tm2.
   if result == True and not relaxSegmentTests and checkLearn:
-    for c in xrange(tm1.numberOfCols):
-      for i in xrange(tm2.cellsPerColumn):
+    for c in range(tm1.numberOfCols):
+      for i in range(tm2.cellsPerColumn):
         nSegs = tm1.getNumSegmentsInCell(c, i)
-        for segIdx in xrange(nSegs):
+        for segIdx in range(nSegs):
           tm1seg = tm1.getSegmentOnCell(c, i, segIdx)
 
           # Loop through all segments in tm2seg and see if any of them match tm1seg
           res = False
-          for tm2segIdx in xrange(nSegs):
+          for tm2segIdx in range(nSegs):
             tm2seg = tm2.getSegmentOnCell(c, i, tm2segIdx)
             if sameSegment(tm1seg, tm2seg) == True:
               res = True
               break
           if res == False:
-            print "\nSegments are different for cell:",c,i
+            print("\nSegments are different for cell:",c,i)
             result = False
             if verbosity >= 0:
-              print "%s : " % tm1Label,
+              print("%s : " % tm1Label, end=' ')
               tm1.printCell(c, i)
-              print "\n%s  : " % tm2Label,
+              print("\n%s  : " % tm2Label, end=' ')
               tm2.printCell(c, i)
 
   if result == True and (verbosity > 1):
-    print "TM's match"
+    print("TM's match")
 
   return result
 
@@ -712,15 +712,15 @@
 
     """
     if(len(SP1._masterConnectedM)!=len(SP2._masterConnectedM)):
-        print "Connected synapse matrices are different sizes"
+        print("Connected synapse matrices are different sizes")
         return False
 
     if(len(SP1._masterPotentialM)!=len(SP2._masterPotentialM)):
-        print "Potential synapse matrices are different sizes"
+        print("Potential synapse matrices are different sizes")
         return False
 
     if(len(SP1._masterPermanenceM)!=len(SP2._masterPermanenceM)):
-        print "Permanence matrices are different sizes"
+        print("Permanence matrices are different sizes")
         return False
 
 
@@ -730,35 +730,35 @@
         connected1 = SP1._masterConnectedM[i]
         connected2 = SP2._masterConnectedM[i]
         if(connected1!=connected2):
-            print "Connected Matrices for cell %d different"  % (i)
+            print("Connected Matrices for cell %d different"  % (i))
             return False
         #grab permanence Matrices and compare them
         permanences1 = SP1._masterPermanenceM[i];
         permanences2 = SP2._masterPermanenceM[i];
         if(permanences1!=permanences2):
-            print "Permanence Matrices for cell %d different" % (i)
+            print("Permanence Matrices for cell %d different" % (i))
             return False
         #grab the potential connection Matrices and compare them
         potential1 = SP1._masterPotentialM[i];
         potential2 = SP2._masterPotentialM[i];
         if(potential1!=potential2):
-            print "Potential Matrices for cell %d different" % (i)
+            print("Potential Matrices for cell %d different" % (i))
             return False
 
     #Check firing boosts
     if(not numpy.array_equal(SP1._firingBoostFactors,SP2._firingBoostFactors)):
-        print "Firing boost factors are different between spatial poolers"
+        print("Firing boost factors are different between spatial poolers")
         return False
 
     #Check duty cycles after inhibiton
     if(not numpy.array_equal(SP1._dutyCycleAfterInh,SP2._dutyCycleAfterInh)):
-        print "Duty cycles after inhibition are different between spatial poolers"
+        print("Duty cycles after inhibition are different between spatial poolers")
         return False
 
 
     #Check duty cycles before inhibition
     if(not numpy.array_equal(SP1._dutyCycleBeforeInh,SP2._dutyCycleBeforeInh)):
-        print "Duty cycles before inhibition are different between spatial poolers"
+        print("Duty cycles before inhibition are different between spatial poolers")
         return False
 
 
@@ -935,7 +935,7 @@
   for idx in nonzeros[1:]:
     if idx != prev+1:
       # Fill in the durations
-      durations[onStartIdx:onStartIdx+onTime] = range(1,onTime+1)
+      durations[onStartIdx:onStartIdx+onTime] = list(range(1,onTime+1))
       onTime       = 1
       onStartIdx = idx
     else:
@@ -943,7 +943,7 @@
     prev = idx
 
   # Fill in the last one
-  durations[onStartIdx:onStartIdx+onTime] = range(1,onTime+1)
+  durations[onStartIdx:onStartIdx+onTime] = list(range(1,onTime+1))
 
 
 
@@ -988,7 +988,7 @@
   # Fill in each non-zero of vectors with the on-time that that output was
   #  on for.
   durations = numpy.zeros(vectors.shape, dtype='int32')
-  for col in xrange(vectors.shape[1]):
+  for col in range(vectors.shape[1]):
     _fillInOnTimes(vectors[:,col], durations[:,col])
 
   # Compute the average on time for each time step
@@ -1030,7 +1030,7 @@
   # How many samples will we look at?
   if numSamples is None:
     numSamples = numElements
-    countOn    = range(numElements)
+    countOn    = list(range(numElements))
   else:
     countOn    = numpy.random.randint(0, numElements, numSamples)
 
@@ -1141,7 +1141,7 @@
 
   if numSamples is None:
     numSamples = numVectors-1
-    countOn = range(numVectors-1)
+    countOn = list(range(numVectors-1))
   else:
     countOn = numpy.random.randint(0, numVectors-1, numSamples)
 
@@ -1189,7 +1189,7 @@
 
     # Accumulated
     samplePctStable = float(stableOutputs) / data[0].sum()
-    print samplePctStable
+    print(samplePctStable)
     pctStable += samplePctStable
     numWindows += 1
 
@@ -1234,8 +1234,8 @@
   # We will use regions that are 15x15, which give us about a 1/225 (.4%) resolution
   #  on saturation.
   regionSize = 15
-  rows = xrange(regionSize+1, outputsShape[0]+1, regionSize)
-  cols = xrange(regionSize+1, outputsShape[1]+1, regionSize)
+  rows = range(regionSize+1, outputsShape[0]+1, regionSize)
+  cols = range(regionSize+1, outputsShape[1]+1, regionSize)
   regionSums = spOut.nNonZerosPerBox(rows, cols)
 
   # Get all the nonzeros out - those are our saturation sums
@@ -1247,7 +1247,7 @@
   #  are surrounded by activity above, below, left and right
   innerSat = []
   locationSet = set(locations)
-  for (location, value) in itertools.izip(locations, values):
+  for (location, value) in zip(locations, values):
     (row, col) = location
     if (row-1,col) in locationSet and (row, col-1) in locationSet \
       and (row+1, col) in locationSet and (row, col+1) in locationSet:
@@ -1297,11 +1297,11 @@
   missingFromPrediction = len(activeElementsInInput.difference(activeElementsInPrediction))
 
   if verbosity >= 1:
-    print "preds. found in input:", foundInInput, "out of", totalActiveInPrediction,
-    print "; preds. missing from input:", missingFromInput, "out of", \
-              totalActiveInPrediction,
-    print "; unexpected active in input:", missingFromPrediction, "out of", \
-              totalActiveInInput
+    print("preds. found in input:", foundInInput, "out of", totalActiveInPrediction, end=' ')
+    print("; preds. missing from input:", missingFromInput, "out of", \
+              totalActiveInPrediction, end=' ')
+    print("; unexpected active in input:", missingFromPrediction, "out of", \
+              totalActiveInInput)
 
   return (foundInInput, totalActiveInInput, missingFromInput,
           totalActiveInPrediction)
@@ -1366,7 +1366,7 @@
   nCellsPerCol = len(outputs[0]) // nCols
 
   # Evalulate prediction for each output sample
-  for idx in xrange(nSamples):
+  for idx in range(nSamples):
 
     # What are the active columns for this output?
     activeCols = outputs[idx].reshape(nCols, nCellsPerCol).max(axis=1)
@@ -1431,16 +1431,16 @@
   else:
     xMin = -1 * (shape[1] // 2)
     xMax = xMin + shape[1] - 1
-    xPositions = range(stepSize * xMin, stepSize * xMax + 1, stepSize)
+    xPositions = list(range(stepSize * xMin, stepSize * xMax + 1, stepSize))
 
     yMin = -1 * (shape[0] // 2)
     yMax = yMin + shape[0] - 1
-    yPositions = range(stepSize * yMin, stepSize * yMax + 1, stepSize)
+    yPositions = list(range(stepSize * yMin, stepSize * yMax + 1, stepSize))
 
     centerOffsets = list(cross(yPositions, xPositions))
 
   numCenterOffsets = len(centerOffsets)
-  print "centerOffsets:", centerOffsets
+  print("centerOffsets:", centerOffsets)
 
   # What is the range on the X and Y offsets of the spread points?
   shape = spreadShape
@@ -1451,11 +1451,11 @@
   else:
     xMin = -1 * (shape[1] // 2)
     xMax = xMin + shape[1] - 1
-    xPositions = range(stepSize * xMin, stepSize * xMax + 1, stepSize)
+    xPositions = list(range(stepSize * xMin, stepSize * xMax + 1, stepSize))
 
     yMin = -1 * (shape[0] // 2)
     yMax = yMin + shape[0] - 1
-    yPositions = range(stepSize * yMin, stepSize * yMax + 1, stepSize)
+    yPositions = list(range(stepSize * yMin, stepSize * yMax + 1, stepSize))
 
     spreadOffsets = list(cross(yPositions, xPositions))
 
@@ -1464,7 +1464,7 @@
     spreadOffsets.insert(0, (0,0))
 
   numSpreadOffsets = len(spreadOffsets)
-  print "spreadOffsets:", spreadOffsets
+  print("spreadOffsets:", spreadOffsets)
 
   return centerOffsets, spreadOffsets
 
@@ -1589,8 +1589,8 @@
   numDistinctMasters = outputCloningWidth * outputCloningHeight
 
   a = numpy.empty((columnsHeight, columnsWidth), 'uint32')
-  for row in xrange(columnsHeight):
-    for col in xrange(columnsWidth):
+  for row in range(columnsHeight):
+    for col in range(columnsWidth):
       a[row, col] = (col % outputCloningWidth) + \
                     (row % outputCloningHeight) * outputCloningWidth
 
@@ -1633,7 +1633,7 @@
     if includeIndices:
       format = '%d,%d:' + format
 
-    for r in xrange(rows):
+    for r in range(rows):
       if includeIndices:
         rowItems = [format % (r,c,x) for c,x in enumerate(array[r])]
       else:
--- d:\nupic\src\python\python27\nupic\algorithms\knn_classifier.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\knn_classifier.py	(refactored)
@@ -431,14 +431,14 @@
     :returns: The number of patterns currently stored in the classifier
     """
     if self.verbosity >= 1:
-      print "%s learn:" % g_debugPrefix
-      print "  category:", int(inputCategory)
-      print "  active inputs:", _labeledInput(inputPattern,
-                                              cellsPerCol=self.cellsPerCol)
+      print("%s learn:" % g_debugPrefix)
+      print("  category:", int(inputCategory))
+      print("  active inputs:", _labeledInput(inputPattern,
+                                              cellsPerCol=self.cellsPerCol))
 
     if isSparse > 0:
       assert all(inputPattern[i] <= inputPattern[i+1]
-                 for i in xrange(len(inputPattern)-1)), \
+                 for i in range(len(inputPattern)-1)), \
                      "Sparse inputPattern must be sorted."
       assert all(bit < isSparse for bit in inputPattern), \
         ("Sparse inputPattern must not index outside the dense "
@@ -737,13 +737,13 @@
       categoryDist.clip(0, 1.0, categoryDist)
 
     if self.verbosity >= 1:
-      print "%s infer:" % (g_debugPrefix)
-      print "  active inputs:",  _labeledInput(inputPattern,
-                                               cellsPerCol=self.cellsPerCol)
-      print "  winner category:", winner
-      print "  pct neighbors of each category:", inferenceResult
-      print "  dist of each prototype:", dist
-      print "  dist of each category:", categoryDist
+      print("%s infer:" % (g_debugPrefix))
+      print("  active inputs:",  _labeledInput(inputPattern,
+                                               cellsPerCol=self.cellsPerCol))
+      print("  winner category:", winner)
+      print("  pct neighbors of each category:", inferenceResult)
+      print("  dist of each prototype:", dist)
+      print("  dist of each category:", categoryDist)
 
     result = (winner, inferenceResult, dist, categoryDist)
     return result
@@ -900,7 +900,7 @@
     """
     :returns: a list containing unique (non-None) partition Ids (just the keys)
     """
-    return self._partitionIdMap.keys()
+    return list(self._partitionIdMap.keys())
 
 
   def getPatternIndicesWithPartitionId(self, partitionId):
@@ -1086,10 +1086,10 @@
     v = singularValues/singularValues[0]
     idx = numpy.where(v<fractionOfMax)[0]
     if len(idx):
-      print "Number of PCA dimensions chosen: ", idx[0], "out of ", len(v)
+      print("Number of PCA dimensions chosen: ", idx[0], "out of ", len(v))
       return idx[0]
     else:
-      print "Number of PCA dimensions chosen: ", len(v)-1, "out of ", len(v)
+      print("Number of PCA dimensions chosen: ", len(v)-1, "out of ", len(v))
       return len(v)-1
 
 
@@ -1112,11 +1112,11 @@
 
 
     if self._vt.shape[0] < self.numSVDDims:
-      print "******************************************************************"
+      print("******************************************************************")
       print ("Warning: The requested number of PCA dimensions is more than "
              "the number of pattern dimensions.")
-      print "Setting numSVDDims = ", self._vt.shape[0]
-      print "******************************************************************"
+      print("Setting numSVDDims = ", self._vt.shape[0])
+      print("******************************************************************")
       self.numSVDDims = self._vt.shape[0]
 
     self._vt = self._vt[:self.numSVDDims]
@@ -1148,7 +1148,7 @@
     categoryArray = numpy.array(self._categoryList)
     newCategoryArray = numpy.zeros(categoryArray.shape[0])
     newCategoryArray.fill(-1)
-    for i in xrange(len(mapping)):
+    for i in range(len(mapping)):
       newCategoryArray[categoryArray==i] = mapping[i]
     self._categoryList = list(newCategoryArray)
 
@@ -1171,7 +1171,7 @@
     elif not hasattr(categoryIndices, "__iter__"):
       categoryIndices = [categoryIndices] * len(vectorIndices)
 
-    for i in xrange(len(vectorIndices)):
+    for i in range(len(vectorIndices)):
       vectorIndex = vectorIndices[i]
       categoryIndex = categoryIndices[i]
 
--- d:\nupic\src\python\python27\nupic\algorithms\sdr_classifier.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\sdr_classifier.py	(refactored)
@@ -204,10 +204,10 @@
                    }
     """
     if self.verbosity >= 1:
-      print "  learn:", learn
-      print "  recordNum:", recordNum
-      print "  patternNZ (%d):" % len(patternNZ), patternNZ
-      print "  classificationIn:", classification
+      print("  learn:", learn)
+      print("  recordNum:", recordNum)
+      print("  patternNZ (%d):" % len(patternNZ), patternNZ)
+      print("  classificationIn:", classification)
 
     # ensures that recordNum increases monotonically
     if len(self._patternNZHistory) > 0:
@@ -282,7 +282,7 @@
         else:
           if (isinstance(actValue, int) or
                 isinstance(actValue, float) or
-                isinstance(actValue, long)):
+                isinstance(actValue, int)):
             self._actualValues[bucketIdx] = ((1.0 - self.actValueAlpha)
                                              * self._actualValues[bucketIdx]
                                              + self.actValueAlpha * actValue)
@@ -300,17 +300,17 @@
     # ------------------------------------------------------------------------
     # Verbose print
     if infer and self.verbosity >= 1:
-      print "  inference: combined bucket likelihoods:"
-      print "    actual bucket values:", retval["actualValues"]
-      for (nSteps, votes) in retval.items():
+      print("  inference: combined bucket likelihoods:")
+      print("    actual bucket values:", retval["actualValues"])
+      for (nSteps, votes) in list(retval.items()):
         if nSteps == "actualValues":
           continue
-        print "    %d steps: " % (nSteps), _pFormatArray(votes)
+        print("    %d steps: " % (nSteps), _pFormatArray(votes))
         bestBucketIdx = votes.argmax()
-        print ("      most likely bucket idx: "
+        print(("      most likely bucket idx: "
                "%d, value: %s" % (bestBucketIdx,
-                                  retval["actualValues"][bestBucketIdx]))
-      print
+                                  retval["actualValues"][bestBucketIdx])))
+      print()
 
     return retval
 
@@ -396,7 +396,7 @@
 
     patternNZHistoryProto = proto.patternNZHistory
     recordNumHistoryProto = proto.recordNumHistory
-    for i in xrange(len(patternNZHistoryProto)):
+    for i in range(len(patternNZHistoryProto)):
       classifier._patternNZHistory.append((recordNumHistoryProto[i],
                                            list(patternNZHistoryProto[i])))
 
@@ -407,7 +407,7 @@
 
     classifier._weightMatrix = {}
     weightMatrixProto = proto.weightMatrix
-    for i in xrange(len(weightMatrixProto)):
+    for i in range(len(weightMatrixProto)):
       classifier._weightMatrix[weightMatrixProto[i].steps] = numpy.reshape(
         weightMatrixProto[i].weight, newshape=(classifier._maxInputIdx+1,
                                                classifier._maxBucketIdx+1))
@@ -427,7 +427,7 @@
 
   def write(self, proto):
     stepsProto = proto.init("steps", len(self.steps))
-    for i in xrange(len(self.steps)):
+    for i in range(len(self.steps)):
       stepsProto[i] = self.steps[i]
 
     proto.alpha = self.alpha
@@ -444,9 +444,9 @@
     patternProto = proto.init("patternNZHistory", len(self._patternNZHistory))
     recordNumHistoryProto = proto.init("recordNumHistory",
                                        len(self._patternNZHistory))
-    for  i in xrange(len(self._patternNZHistory)):
+    for  i in range(len(self._patternNZHistory)):
       subPatternProto = patternProto.init(i, len(self._patternNZHistory[i][1]))
-      for j in xrange(len(self._patternNZHistory[i][1])):
+      for j in range(len(self._patternNZHistory[i][1])):
         subPatternProto[j] = int(self._patternNZHistory[i][1][j])
       recordNumHistoryProto[i] = int(self._patternNZHistory[i][0])
 
@@ -464,7 +464,7 @@
     proto.maxInputIdx = self._maxInputIdx
 
     actualValuesProto = proto.init("actualValues", len(self._actualValues))
-    for i in xrange(len(self._actualValues)):
+    for i in range(len(self._actualValues)):
       if self._actualValues[i] is not None:
         actualValuesProto[i] = self._actualValues[i]
       else:
--- d:\nupic\src\python\python27\nupic\algorithms\sdr_classifier_diff.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\sdr_classifier_diff.py	(refactored)
@@ -91,9 +91,9 @@
     assert set(result1.keys()) == set(result2.keys()), "diff detected: " \
       "py result=%s, C++ result=%s" % (result1, result2)
     # Assert that the values match.
-    for k, l in result1.iteritems():
+    for k, l in result1.items():
       assert type(l) == type(result2[k])
-      for i in xrange(len(l)):
+      for i in range(len(l)):
         if isinstance(classification['actValue'], numbers.Real):
           assert abs(float(l[i]) - float(result2[k][i])) < 0.0000001, (
               'Python SDRClassifier has value %f and C++ SDRClassifierCpp has '
--- d:\nupic\src\python\python27\nupic\algorithms\spatial_pooler.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\spatial_pooler.py	(refactored)
@@ -257,13 +257,13 @@
     columnDimensions = numpy.array(columnDimensions, ndmin=1)
     numColumns = columnDimensions.prod()
 
-    if not isinstance(numColumns, (int, long)) or numColumns <= 0:
+    if not isinstance(numColumns, int) or numColumns <= 0:
       raise InvalidSPParamValueError("Invalid number of columns ({})"
                                      .format(repr(numColumns)))
     inputDimensions = numpy.array(inputDimensions, ndmin=1)
     numInputs = inputDimensions.prod()
 
-    if not isinstance(numInputs, (int, long)) or numInputs <= 0:
+    if not isinstance(numInputs, int) or numInputs <= 0:
       raise InvalidSPParamValueError("Invalid number of inputs ({}"
                                      .format(repr(numInputs)))
 
@@ -341,7 +341,7 @@
     # Initialize a tiny random tie breaker. This is used to determine winning
     # columns where the overlaps are identical.
     self._tieBreaker = numpy.array([0.01 * self._random.getReal64() for i in
-                                      xrange(self._numColumns)],
+                                      range(self._numColumns)],
                                     dtype=realDType)
 
     # 'self._connectedSynapses' is a similar matrix to 'self._permanences'
@@ -362,7 +362,7 @@
     # Initialize the set of permanence values for each column. Ensure that
     # each column is connected to enough input bits to allow it to be
     # activated.
-    for columnIndex in xrange(numColumns):
+    for columnIndex in range(numColumns):
       potential = self._mapPotential(columnIndex)
       self._potentialPools.replace(columnIndex, potential.nonzero()[0])
       perm = self._initPermanence(potential, initConnectedPct)
@@ -983,7 +983,7 @@
     _updateMinDutyCyclesGlobal, here the values can be quite different for
     different columns.
     """
-    for column in xrange(self._numColumns):
+    for column in range(self._numColumns):
       neighborhood = self._getColumnNeighborhood(column)
 
       maxActiveDuty = self._activeDutyCycles[neighborhood].max()
@@ -1050,7 +1050,7 @@
 
     avgConnectedSpan = numpy.average(
                           [self._avgConnectedSpanForColumnND(i)
-                          for i in xrange(self._numColumns)]
+                          for i in range(self._numColumns)]
                         )
     columnsPerInput = self._avgColumnsPerInput()
     diameter = avgConnectedSpan * columnsPerInput
@@ -1319,7 +1319,7 @@
     # column's potential pool will be connected. This number is
     # given by the parameter "connectedPct"
     perm = numpy.zeros(self._numInputs, dtype=realDType)
-    for i in xrange(self._numInputs):
+    for i in range(self._numInputs):
       if (potential[i] < 1):
         continue
 
@@ -1501,7 +1501,7 @@
     # The targetDensity is the average activeDutyCycles of the neighboring
     # columns of each column.
     targetDensity = numpy.zeros(self._numColumns, dtype=realDType)
-    for i in xrange(self._numColumns):
+    for i in range(self._numColumns):
       maskNeighbors = self._getColumnNeighborhood(i)
       targetDensity[i] = numpy.mean(self._activeDutyCycles[maskNeighbors])
 
@@ -1866,7 +1866,7 @@
     instance._connectedCounts = numpy.zeros(numColumns, dtype=realDType)
     instance._connectedSynapses = BinaryCorticalColumns(numInputs)
     instance._connectedSynapses.resize(numColumns, numInputs)
-    for columnIndex in xrange(proto.numColumns):
+    for columnIndex in range(proto.numColumns):
       instance._updatePermanencesForColumn(
         instance._permanences[columnIndex], columnIndex, False
       )
@@ -1888,20 +1888,20 @@
     """
     Useful for debugging.
     """
-    print "------------PY  SpatialPooler Parameters ------------------"
-    print "numInputs                  = ", self.getNumInputs()
-    print "numColumns                 = ", self.getNumColumns()
-    print "columnDimensions           = ", self._columnDimensions
-    print "numActiveColumnsPerInhArea = ", self.getNumActiveColumnsPerInhArea()
-    print "potentialPct               = ", self.getPotentialPct()
-    print "globalInhibition           = ", self.getGlobalInhibition()
-    print "localAreaDensity           = ", self.getLocalAreaDensity()
-    print "stimulusThreshold          = ", self.getStimulusThreshold()
-    print "synPermActiveInc           = ", self.getSynPermActiveInc()
-    print "synPermInactiveDec         = ", self.getSynPermInactiveDec()
-    print "synPermConnected           = ", self.getSynPermConnected()
-    print "minPctOverlapDutyCycle     = ", self.getMinPctOverlapDutyCycles()
-    print "dutyCyclePeriod            = ", self.getDutyCyclePeriod()
-    print "boostStrength              = ", self.getBoostStrength()
-    print "spVerbosity                = ", self.getSpVerbosity()
-    print "version                    = ", self._version
+    print("------------PY  SpatialPooler Parameters ------------------")
+    print("numInputs                  = ", self.getNumInputs())
+    print("numColumns                 = ", self.getNumColumns())
+    print("columnDimensions           = ", self._columnDimensions)
+    print("numActiveColumnsPerInhArea = ", self.getNumActiveColumnsPerInhArea())
+    print("potentialPct               = ", self.getPotentialPct())
+    print("globalInhibition           = ", self.getGlobalInhibition())
+    print("localAreaDensity           = ", self.getLocalAreaDensity())
+    print("stimulusThreshold          = ", self.getStimulusThreshold())
+    print("synPermActiveInc           = ", self.getSynPermActiveInc())
+    print("synPermInactiveDec         = ", self.getSynPermInactiveDec())
+    print("synPermConnected           = ", self.getSynPermConnected())
+    print("minPctOverlapDutyCycle     = ", self.getMinPctOverlapDutyCycles())
+    print("dutyCyclePeriod            = ", self.getDutyCyclePeriod())
+    print("boostStrength              = ", self.getBoostStrength())
+    print("spVerbosity                = ", self.getSpVerbosity())
+    print("version                    = ", self._version)
--- d:\nupic\src\python\python27\nupic\algorithms\temporal_memory.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\temporal_memory.py	(refactored)
@@ -31,6 +31,7 @@
 from nupic.algorithms.connections import Connections, binSearch
 from nupic.serializable import Serializable
 from nupic.support.group_by import groupby2
+from functools import reduce
 
 EPSILON = 0.00001 # constant error threshold to check equality of permanences to
                   # other floats
@@ -276,13 +277,13 @@
 
     activeSegments = (
       self.connections.segmentForFlatIdx(i)
-      for i in xrange(len(numActiveConnected))
+      for i in range(len(numActiveConnected))
       if numActiveConnected[i] >= self.activationThreshold
     )
 
     matchingSegments = (
       self.connections.segmentForFlatIdx(i)
-      for i in xrange(len(numActivePotential))
+      for i in range(len(numActivePotential))
       if numActivePotential[i] >= self.minThreshold
     )
 
@@ -369,7 +370,7 @@
     """
 
     start = self.cellsPerColumn * column
-    cellsForColumn = xrange(start, start + self.cellsPerColumn)
+    cellsForColumn = range(start, start + self.cellsPerColumn)
 
     return self._burstColumn(
       self.connections, self._random, self.lastUsedIterationForSegment, column,
@@ -698,7 +699,7 @@
       key=lambda s: s._ordinal
     )
 
-    for _ in xrange(nDestroy):
+    for _ in range(nDestroy):
       if len(destroyCandidates) == 0:
         break
 
@@ -854,7 +855,7 @@
 
     start = self.cellsPerColumn * column
     end = start + self.cellsPerColumn
-    return range(start, end)
+    return list(range(start, end))
 
 
   def numberOfColumns(self):
@@ -1251,14 +1252,14 @@
       tm.numActivePotentialSynapsesForSegment[segment.flatIdx] = (
         int(protoSegment.number))
 
-    tm.iteration = long(proto.iteration)
+    tm.iteration = int(proto.iteration)
 
     for protoSegment in proto.lastUsedIterationForSegment:
       segment = tm.connections.getSegment(protoSegment.cell,
                                           protoSegment.idxOnCell)
 
       tm.lastUsedIterationForSegment[segment.flatIdx] = (
-        long(protoSegment.number))
+        int(protoSegment.number))
 
     return tm
 
--- d:\nupic\src\python\python27\nupic\algorithms\utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\utils.py	(refactored)
@@ -140,7 +140,7 @@
     function = getattr(module, self.funcName)
     # Re-map names.
     if isinstance(self.learningKeys, dict): # Safer check?
-      remapped = dict((k, learning[j]) for j, k in self.learningKeys.iteritems()
+      remapped = dict((k, learning[j]) for j, k in self.learningKeys.items()
           if j in learning)
     else: # Just collect arguments.
       remapped = dict((j, learning[j]) for j in self.learningKeys
@@ -190,8 +190,8 @@
   maxes = combined.max(1)
   s = str(combined)
   numpy.set_printoptions(linewidth=lw)
-  print "\n".join(("%s %s %f" % (t, l, m)) for t, l, m in
-      zip(titles, s.splitlines(), maxes))
+  print("\n".join(("%s %s %f" % (t, l, m)) for t, l, m in
+      zip(titles, s.splitlines(), maxes)))
 
 
 
--- d:\nupic\src\python\python27\nupic\algorithms\monitor_mixin\monitor_mixin_base.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\monitor_mixin\monitor_mixin_base.py	(refactored)
@@ -84,7 +84,7 @@
 
 
 
-class MonitorMixinBase(object):
+class MonitorMixinBase(object, metaclass=abc.ABCMeta):
   """
   Base class for MonitorMixin. Each subclass will be a mixin for a particular
   algorithm.
@@ -92,7 +92,6 @@
   All arguments, variables, and methods in monitor mixin classes should be
   prefixed with "mm" (to avoid collision with the classes they mix in to).
   """
-  __metaclass__ = abc.ABCMeta
 
 
   def __init__(self, *args, **kwargs):
@@ -133,7 +132,7 @@
     assert len(traces) > 0, "No traces found"
     table = PrettyTable(["#"] + [trace.prettyPrintTitle() for trace in traces])
 
-    for i in xrange(len(traces[0].data)):
+    for i in range(len(traces[0].data)):
       if breakOnResets and breakOnResets.data[i]:
         table.add_row(["<reset>"] * (len(traces) + 1))
       table.add_row([i] +
@@ -212,7 +211,7 @@
     plot = Plot(self, title)
     resetTrace = self.mmGetTraceResets().data
     data = numpy.zeros((cellCount, 1))
-    for i in xrange(len(cellTrace)):
+    for i in range(len(cellTrace)):
       # Set up a "background" vector that is shaded or blank
       if showReset and resetTrace[i]:
         activity = numpy.ones((cellCount, 1)) * resetShading
--- d:\nupic\src\python\python27\nupic\algorithms\monitor_mixin\temporal_memory_monitor_mixin.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\monitor_mixin\temporal_memory_monitor_mixin.py	(refactored)
@@ -152,9 +152,9 @@
     numCellsPerColumn = []
 
     for predictedActiveCells in (
-        self._mmData["predictedActiveCellsForSequence"].values()):
+        list(self._mmData["predictedActiveCellsForSequence"].values())):
       cellsForColumn = self.mapCellsToColumns(predictedActiveCells)
-      numCellsPerColumn += [len(x) for x in cellsForColumn.values()]
+      numCellsPerColumn += [len(x) for x in list(cellsForColumn.values())]
 
     return Metric(self,
                   "# predicted => active cells per column for each sequence",
@@ -174,13 +174,13 @@
     numSequencesForCell = defaultdict(lambda: 0)
 
     for predictedActiveCells in (
-          self._mmData["predictedActiveCellsForSequence"].values()):
+          list(self._mmData["predictedActiveCellsForSequence"].values())):
       for cell in predictedActiveCells:
         numSequencesForCell[cell] += 1
 
     return Metric(self,
                   "# sequences each predicted => active cells appears in",
-                  numSequencesForCell.values())
+                  list(numSequencesForCell.values()))
 
 
   def mmPrettyPrintConnections(self):
@@ -197,7 +197,7 @@
              "(#) [(source cell=permanence ...),       ...]\n")
     text += "------------------------------------\n"
 
-    columns = range(self.numberOfColumns())
+    columns = list(range(self.numberOfColumns()))
 
     for column in columns:
       cells = self.cellsForColumn(column)
@@ -220,8 +220,8 @@
 
         text += ("Column {0:3} / Cell {1:3}:\t({2}) {3}\n".format(
           column, cell,
-          len(segmentDict.values()),
-          "[{0}]".format(",       ".join(segmentDict.values()))))
+          len(list(segmentDict.values())),
+          "[{0}]".format(",       ".join(list(segmentDict.values())))))
 
       if column < len(columns) - 1:  # not last
         text += "\n"
@@ -243,9 +243,9 @@
     table = PrettyTable(["Pattern", "Column", "predicted=>active cells"])
 
     for sequenceLabel, predictedActiveCells in (
-          self._mmData["predictedActiveCellsForSequence"].iteritems()):
+          iter(self._mmData["predictedActiveCellsForSequence"].items())):
       cellsForColumn = self.mapCellsToColumns(predictedActiveCells)
-      for column, cells in cellsForColumn.iteritems():
+      for column, cells in cellsForColumn.items():
         table.add_row([sequenceLabel, column, list(cells)])
 
     return table.get_string(sortby=sortby).encode("utf-8")
@@ -418,7 +418,7 @@
       self._mmComputeTransitionTraces()
 
     cellTrace = copy.deepcopy(self._mmTraces[activityType].data)
-    for i in xrange(len(cellTrace)):
+    for i in range(len(cellTrace)):
       cellTrace[i] = self.getCellIndices(cellTrace[i])
 
     return self.mmGetCellTracePlot(cellTrace, self.numberOfCells(),
--- d:\nupic\src\python\python27\nupic\algorithms\monitor_mixin\trace.py	(original)
+++ d:\nupic\src\python\python27\nupic\algorithms\monitor_mixin\trace.py	(refactored)
@@ -29,12 +29,11 @@
 
 
 
-class Trace(object):
+class Trace(object, metaclass=abc.ABCMeta):
   """
   A record of the past data the algorithm has seen, with an entry for each
   iteration.
   """
-  __metaclass__ = abc.ABCMeta
 
 
   def __init__(self, monitor, title):
--- d:\nupic\src\python\python27\nupic\data\__init__.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\__init__.py	(refactored)
@@ -29,4 +29,4 @@
 
 SENTINEL_VALUE_FOR_MISSING_DATA = None
 
-from function_source import FunctionSource
+from .function_source import FunctionSource
--- d:\nupic\src\python\python27\nupic\data\aggregator.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\aggregator.py	(refactored)
@@ -207,7 +207,7 @@
     return None
 
   # Sort by counts
-  sortedCounts = valueCounts.items()
+  sortedCounts = list(valueCounts.items())
   sortedCounts.sort(cmp=lambda x,y: x[1] - y[1], reverse=True)
   return sortedCounts[0][0]
 
@@ -455,7 +455,7 @@
     """
 
     params = None
-    if isinstance(funcName, basestring):
+    if isinstance(funcName, str):
       if funcName == 'sum':
         fp = _aggr_sum
       elif funcName == 'first':
@@ -801,8 +801,8 @@
   if os.path.isfile(outputFilename) or \
      os.path.isfile(lockFilePath):
     while os.path.isfile(lockFilePath):
-      print 'Waiting for %s to be fully written by another process' % \
-            lockFilePath
+      print('Waiting for %s to be fully written by another process' % \
+            lockFilePath)
       time.sleep(1)
     return outputFilename
 
--- d:\nupic\src\python\python27\nupic\data\category_filter.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\category_filter.py	(refactored)
@@ -60,7 +60,7 @@
     Returns True if the record matches any of the provided filters
     '''
 
-    for field, meta in self.filterDict.iteritems():
+    for field, meta in self.filterDict.items():
       index = meta['index']
       categories = meta['categories']
       for category in categories:
--- d:\nupic\src\python\python27\nupic\data\dict_utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\dict_utils.py	(refactored)
@@ -36,7 +36,7 @@
     return self[name]
 
   def __setstate__(self, state):
-    for k, v in state.items():
+    for k, v in list(state.items()):
       self[k] = v
 
 
@@ -47,7 +47,7 @@
   dictPairs = [(original, updates)]
   while len(dictPairs) > 0:
     original, updates = dictPairs.pop()
-    for k, v in updates.iteritems():
+    for k, v in updates.items():
       if k in original and isinstance(original[k], dict) and isinstance(v, dict):
         dictPairs.append((original[k], v))
       else:
@@ -65,7 +65,7 @@
   remainingDicts = [(d, ())]
   while len(remainingDicts) > 0:
     current, prevKeys = remainingDicts.pop()
-    for k, v in current.iteritems():
+    for k, v in current.items():
       keys = prevKeys + (k,)
       if isinstance(v, dict):
         remainingDicts.insert(0, (v, keys))
@@ -77,7 +77,7 @@
   remainingDicts = [d]
   while len(remainingDicts) > 0:
     current = remainingDicts.pop()
-    for k, v in current.iteritems():
+    for k, v in current.items():
       if k == target:
         return v
       if isinstance(v, dict):
@@ -112,15 +112,15 @@
     return differences
 
   if differences['inAButNotInB']:
-    print ">>> inAButNotInB: %s" % differences['inAButNotInB']
+    print(">>> inAButNotInB: %s" % differences['inAButNotInB'])
 
   if differences['inBButNotInA']:
-    print ">>> inBButNotInA: %s" % differences['inBButNotInA']
+    print(">>> inBButNotInA: %s" % differences['inBButNotInA'])
 
   for key in differences['differentValues']:
-    print ">>> da[%s] != db[%s]" % (key, key)
-    print "da[%s] = %r" % (key, da[key])
-    print "db[%s] = %r" % (key, db[key])
+    print(">>> da[%s] != db[%s]" % (key, key))
+    print("da[%s] = %r" % (key, da[key]))
+    print("db[%s] = %r" % (key, db[key]))
 
   return differences
 
--- d:\nupic\src\python\python27\nupic\data\file_record_stream.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\file_record_stream.py	(refactored)
@@ -182,18 +182,18 @@
       # Verify all fields are 3-tuple
       assert all(isinstance(f, (tuple, FieldMetaInfo)) and len(f) == 3
                  for f in fields)
-      names, types, specials = zip(*fields)
+      names, types, specials = list(zip(*fields))
       self._writer = csv.writer(self._file)
     else:
       # Read header lines
       self._reader = csv.reader(self._file, dialect="excel")
       try:
-        names = [n.strip() for n in self._reader.next()]
+        names = [n.strip() for n in next(self._reader)]
       except:
         raise Exception('The header line of the file %s contained a NULL byte' \
                         % self._filename)
-      types = [t.strip() for t in self._reader.next()]
-      specials = [s.strip() for s in self._reader.next()]
+      types = [t.strip() for t in next(self._reader)]
+      specials = [s.strip() for s in next(self._reader)]
 
       # If there are no specials, this means there was a blank line
       if len(specials) == 0:
@@ -287,7 +287,7 @@
       rowsToSkip = 0
 
     while rowsToSkip > 0:
-      self.next()
+      next(self)
       rowsToSkip -= 1
 
 
@@ -332,9 +332,9 @@
     self._reader = csv.reader(self._file, dialect="excel")
 
     # Skip header rows
-    self._reader.next()
-    self._reader.next()
-    self._reader.next()
+    next(self._reader)
+    next(self._reader)
+    next(self._reader)
 
     # Reset record count, etc.
     self._recordCount = 0
@@ -352,7 +352,7 @@
 
     # Read the line
     try:
-      line = self._reader.next()
+      line = next(self._reader)
 
     except StopIteration:
       if self.rewindAtEOF:
@@ -360,7 +360,7 @@
           raise Exception("The source configured to reset at EOF but "
                           "'%s' appears to be empty" % self._filename)
         self.rewind()
-        line = self._reader.next()
+        line = next(self._reader)
 
       else:
         return None
@@ -405,7 +405,7 @@
     # Write header if needed
     if self._recordCount == 0:
       # Write the header
-      names, types, specials = zip(*self.getFields())
+      names, types, specials = list(zip(*self.getFields()))
       for line in names, types, specials:
         self._writer.writerow(line)
 
@@ -519,24 +519,24 @@
 
       # Create a new reader; read names, types, specials
       reader = csv.reader(inFile, dialect="excel")
-      names = [n.strip() for n in reader.next()]
-      types = [t.strip() for t in reader.next()]
+      names = [n.strip() for n in next(reader)]
+      types = [t.strip() for t in next(reader)]
       # Skip over specials
-      reader.next()
+      next(reader)
 
       # Initialize stats to all None
       self._stats = dict()
       self._stats['min'] = []
       self._stats['max'] = []
 
-      for i in xrange(len(names)):
+      for i in range(len(names)):
         self._stats['min'].append(None)
         self._stats['max'].append(None)
 
       # Read the file, collect stats
       while True:
         try:
-          line = reader.next()
+          line = next(reader)
           for i, f in enumerate(line):
             if (len(types) > i and
                 types[i] in [FieldMetaType.integer, FieldMetaType.float] and
@@ -669,9 +669,9 @@
     bookMarkFile = bookMarkDict.get('filepath', None)
 
     if bookMarkFile != realpath:
-      print ("Ignoring bookmark due to mismatch between File's "
+      print(("Ignoring bookmark due to mismatch between File's "
              "filename realpath vs. bookmark; realpath: %r; bookmark: %r") % (
-        realpath, bookMarkDict)
+        realpath, bookMarkDict))
       return 0
     else:
       return bookMarkDict['currentRow']
@@ -747,7 +747,7 @@
     return self
 
 
-  def next(self):
+  def __next__(self):
     """Implement the iterator protocol """
     record = self.getNextRecord()
     if record is None:
--- d:\nupic\src\python\python27\nupic\data\filters.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\filters.py	(refactored)
@@ -78,7 +78,7 @@
   def _getDatetimeField(self, data):
     datetimeField = None
     assert isinstance(data, dict)
-    for (name, value) in data.items():
+    for (name, value) in list(data.items()):
       if isinstance(value, datetime):
         datetimeField = name
         break
--- d:\nupic\src\python\python27\nupic\data\function_source.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\function_source.py	(refactored)
@@ -112,9 +112,9 @@
       prevSequenceId = getattr(self, "_prevSequenceId", None)
       )
     func = dict()
-    func['code'] = marshal.dumps(self.func.func_code)
-    func['name'] = self.func.func_name
-    func['doc'] = self.func.func_doc
+    func['code'] = marshal.dumps(self.func.__code__)
+    func['name'] = self.func.__name__
+    func['doc'] = self.func.__doc__
     state['func'] = func
 
     return state
@@ -122,8 +122,8 @@
   def __setstate__(self, state):
     funcinfo = state['func']
     self.func = types.FunctionType(marshal.loads(funcinfo['code']), globals())
-    self.func.func_name = funcinfo['name']
-    self.func.func_doc = funcinfo['doc']
+    self.func.__name__ = funcinfo['name']
+    self.func.__doc__ = funcinfo['doc']
 
     self.state = state['state']
     self.resetFieldName = state['resetFieldName']
--- d:\nupic\src\python\python27\nupic\data\inference_shifter.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\inference_shifter.py	(refactored)
@@ -58,10 +58,10 @@
 
     self._inferenceBuffer.appendleft(copy.deepcopy(modelResult.inferences))
 
-    for inferenceElement, inference in modelResult.inferences.iteritems():
+    for inferenceElement, inference in modelResult.inferences.items():
       if isinstance(inference, dict):
         inferencesToWrite[inferenceElement] = {}
-        for key, _ in inference.iteritems():
+        for key, _ in inference.items():
           delay = InferenceElement.getTemporalDelay(inferenceElement, key)
           if len(self._inferenceBuffer) > delay:
             prevInference = self._inferenceBuffer[delay][inferenceElement][key]
--- d:\nupic\src\python\python27\nupic\data\json_helpers.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\json_helpers.py	(refactored)
@@ -80,7 +80,7 @@
           ValidationError when value fails json validation
   """
 
-  assert len(kwds.keys()) >= 1
+  assert len(list(kwds.keys())) >= 1
   assert 'schemaPath' in kwds or 'schemaDict' in kwds
 
   schemaDict = None
@@ -137,34 +137,34 @@
     'myBool': False
   }
 
-  print "Validating schemaDict method in positive test..."
+  print("Validating schemaDict method in positive test...")
   validate(d, schemaDict=schemaDict)
-  print "ok\n"
+  print("ok\n")
 
-  print "Validating schemaDict method in negative test..."
+  print("Validating schemaDict method in negative test...")
   try:
     validate({}, schemaDict=schemaDict)
   except ValidationError:
-    print "ok\n"
+    print("ok\n")
   else:
-    print "FAILED\n"
+    print("FAILED\n")
     sys.exit(1)
 
 
   schemaPath = os.path.join(os.path.dirname(__file__), "testSchema.json")
-  print "Validating schemaPath method in positive test using %s..." % \
-            (os.path.abspath(schemaPath),)
+  print("Validating schemaPath method in positive test using %s..." % \
+            (os.path.abspath(schemaPath),))
   validate(d, schemaPath=schemaPath)
-  print "ok\n"
+  print("ok\n")
 
-  print "Validating schemaPath method in negative test using %s..." % \
-            (os.path.abspath(schemaPath),)
+  print("Validating schemaPath method in negative test using %s..." % \
+            (os.path.abspath(schemaPath),))
   try:
     validate({}, schemaPath=schemaPath)
   except ValidationError:
-    print "ok\n"
+    print("ok\n")
   else:
-    print "FAILED\n"
+    print("FAILED\n")
     sys.exit(1)
 
 
--- d:\nupic\src\python\python27\nupic\data\record_stream.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\record_stream.py	(refactored)
@@ -114,7 +114,7 @@
     """
 
     # Create the return dict
-    result = dict(zip(self._fieldNames, inputRow))
+    result = dict(list(zip(self._fieldNames, inputRow)))
 
     # Add in the special fields
     if self._categoryFieldIndex is not None:
@@ -211,12 +211,10 @@
 
 
 
-class RecordStreamIface(object):
+class RecordStreamIface(object, metaclass=ABCMeta):
   """
   This is the interface for the record input/output storage classes.
   """
-
-  __metaclass__ = ABCMeta
 
 
   def __init__(self):
--- d:\nupic\src\python\python27\nupic\data\sorter.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\sorter.py	(refactored)
@@ -186,26 +186,26 @@
 
 def writeTestFile(testFile, fields, big):
   if big:
-    print 'Creating big test file (763MB)...'
+    print('Creating big test file (763MB)...')
     payload = 'x' * 10 ** 8
   else:
-    print 'Creating a small big test file...'
+    print('Creating a small big test file...')
     payload = 'x' * 3
   with FileRecordStream(testFile, write=True, fields=fields) as o:
-    print '.'; o.appendRecord([1,3,6, payload])
-    print '.'; o.appendRecord([2,3,6, payload])
-    print '.'; o.appendRecord([1,4,6, payload])
-    print '.'; o.appendRecord([2,4,6, payload])
-    print '.'; o.appendRecord([1,3,5, payload])
-    print '.'; o.appendRecord([2,3,5, payload])
-    print '.'; o.appendRecord([1,4,5, payload])
-    print '.'; o.appendRecord([2,4,5, payload])
+    print('.'); o.appendRecord([1,3,6, payload])
+    print('.'); o.appendRecord([2,3,6, payload])
+    print('.'); o.appendRecord([1,4,6, payload])
+    print('.'); o.appendRecord([2,4,6, payload])
+    print('.'); o.appendRecord([1,3,5, payload])
+    print('.'); o.appendRecord([2,3,5, payload])
+    print('.'); o.appendRecord([1,4,5, payload])
+    print('.'); o.appendRecord([2,4,5, payload])
 
 def test(long):
   import shutil
   from tempfile import gettempdir
 
-  print 'Running sorter self-test...'
+  print('Running sorter self-test...')
 
   # Switch to a temp dir in order to create files freely
   workDir = os.path.join(gettempdir(), 'sorter_test')
@@ -213,7 +213,7 @@
     shutil.rmtree(workDir)
   os.makedirs(workDir)
   os.chdir(workDir)
-  print 'cwd:', os.getcwd()
+  print('cwd:', os.getcwd())
 
   # The fields definition used by all tests
   fields = [
@@ -226,7 +226,7 @@
   # Create a test file
   testFile = '1.csv'
   if not os.path.isfile(testFile):
-    writeTestFile(testFile, fields, big=long)
+    writeTestFile(testFile, fields, big=int)
 
   # Set watermark here to 300MB bellow current available memory. That ensures
   # multiple chunk files in the big testcase
@@ -234,7 +234,7 @@
   mem = psutil.avail_phymem()
   watermark = mem - 300 * 1024 * 1024
 
-  print 'Test sorting by f1 and f2, watermak:', watermark
+  print('Test sorting by f1 and f2, watermak:', watermark)
   results = []
   sort(testFile,
        key=['f1', 'f2'],
@@ -258,7 +258,7 @@
 
   mem = psutil.avail_phymem()
   watermark = mem - 300 * 1024 * 1024
-  print 'Test sorting by f2 and f1, watermark:', watermark
+  print('Test sorting by f2 and f1, watermark:', watermark)
   results = []
   sort(testFile,
        key=['f2', 'f1'],
@@ -281,7 +281,7 @@
 
   mem = psutil.avail_phymem()
   watermark = mem - 300 * 1024 * 1024
-  print 'Test sorting by f3 and f2, watermark:', watermark
+  print('Test sorting by f3 and f2, watermark:', watermark)
   results = []
   sort(testFile,
        key=['f3', 'f2'],
@@ -307,9 +307,9 @@
   os.chdir('..')
   shutil.rmtree(workDir)
 
-  print 'done'
+  print('done')
 
 if __name__=='__main__':
-  print 'Starting tests...'
+  print('Starting tests...')
   test('--long' in sys.argv)
-  print 'All tests pass'
+  print('All tests pass')
--- d:\nupic\src\python\python27\nupic\data\stats.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\stats.py	(refactored)
@@ -139,8 +139,8 @@
         r = pickle.load(open(statsFilename, "rb"))
       except:
         # Ok to ignore errors -- we will just re-generate the file
-        print "Warning: unable to load stats for %s -- " \
-              "will regenerate" % filename
+        print("Warning: unable to load stats for %s -- " \
+              "will regenerate" % filename)
         r = dict()
       requestedKeys = set([s for s in statsInfo])
       availableKeys = set(r.keys())
@@ -148,12 +148,12 @@
       if len(unavailableKeys ) == 0:
         return r
       else:
-        print "generateStats: re-generating stats file %s because " \
+        print("generateStats: re-generating stats file %s because " \
               "keys %s are not available" %  \
-              (filename, str(unavailableKeys))
+              (filename, str(unavailableKeys)))
         os.remove(filename)
 
-  print "Generating statistics for file '%s' with filters '%s'" % (filename, filters)
+  print("Generating statistics for file '%s' with filters '%s'" % (filename, filters))
   sensor = RecordSensor()
   sensor.dataSource = FileRecordStream(filename)
   sensor.preEncodingFilters = filters
@@ -173,19 +173,19 @@
   # Now collect the stats
   if maxSamples is None:
     maxSamples = 500000
-  for i in xrange(maxSamples):
+  for i in range(maxSamples):
     try:
       record = sensor.getNextRecord()
     except StopIteration:
       break
-    for (name, collector) in statsInfo.items():
+    for (name, collector) in list(statsInfo.items()):
       collector.add(record[name])
 
   del sensor
 
   # Assemble the results and return
   r = dict()
-  for (field, collector) in statsInfo.items():
+  for (field, collector) in list(statsInfo.items()):
     stats = collector.getStats()
     if field not in r:
       r[field] = stats
--- d:\nupic\src\python\python27\nupic\data\stats_v2.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\stats_v2.py	(refactored)
@@ -83,12 +83,12 @@
     stats[self.fieldname]['totalNumDistinctEntries'] = totalNumDistinctEntries
 
     if VERBOSITY > 1:
-      print "-"*40
-      print "Field '%s'" % self.fieldname
-      print "--"
-      print "Counts:"
-      print "Total number of entries:%d" % totalNumEntries
-      print "Total number of distinct entries:%d" % totalNumDistinctEntries
+      print("-"*40)
+      print("Field '%s'" % self.fieldname)
+      print("--")
+      print("Counts:")
+      print("Total number of entries:%d" % totalNumEntries)
+      print("Total number of distinct entries:%d" % totalNumDistinctEntries)
 
 class StringStatsCollector(BaseStatsCollector):
 
@@ -102,17 +102,17 @@
       for value in self.valueList:
         valueCountDict[value] += 1
 
-      print "--"
+      print("--")
       # Print the top 5 frequent strings
       topN = 5
-      print " Sorted list:"
-      for key, value in sorted(valueCountDict.iteritems(),
+      print(" Sorted list:")
+      for key, value in sorted(iter(valueCountDict.items()),
                                key=operator.itemgetter(1),
                                reverse=True,)[:topN]:
 
-        print "%s:%d" % (key, value)
+        print("%s:%d" % (key, value))
       if len(valueCountDict) > topN:
-        print "..."
+        print("...")
 
 class NumberStatsCollector(BaseStatsCollector):
 
@@ -135,10 +135,10 @@
     percentile99th = sortedNumberList[int(0.99*listLength)]
 
     differenceList = \
-               [(cur - prev) for prev, cur in itertools.izip(list(self.valueSet)[:-1],
+               [(cur - prev) for prev, cur in zip(list(self.valueSet)[:-1],
                                                              list(self.valueSet)[1:])]
     if min > max:
-      print self.fieldname, min, max, '-----'
+      print(self.fieldname, min, max, '-----')
     meanResolution = numpy.mean(differenceList)
 
 
@@ -157,25 +157,25 @@
       stats[self.fieldname]['data'] = self.valueList
 
     if VERBOSITY > 2:
-      print '--'
-      print "Statistics:"
-      print "min:", min
-      print "max:", max
-      print "mean:", mean
-      print "median:", median
-      print "1st percentile :", percentile1st
-      print "99th percentile:", percentile99th
-
-      print '--'
-      print "Resolution:"
-      print "Mean Resolution:", meanResolution
+      print('--')
+      print("Statistics:")
+      print("min:", min)
+      print("max:", max)
+      print("mean:", mean)
+      print("median:", median)
+      print("1st percentile :", percentile1st)
+      print("99th percentile:", percentile99th)
+
+      print('--')
+      print("Resolution:")
+      print("Mean Resolution:", meanResolution)
 
     if VERBOSITY > 3:
-      print '--'
-      print "Histogram:"
+      print('--')
+      print("Histogram:")
       counts, bins = numpy.histogram(self.valueList, new=True)
-      print "Counts:", counts.tolist()
-      print "Bins:", bins.tolist()
+      print("Counts:", counts.tolist())
+      print("Bins:", bins.tolist())
 
 
 class IntStatsCollector(NumberStatsCollector):
@@ -232,10 +232,10 @@
     decodedInput = encoder.decode(totalOrEncoderOutput)[0]
 
     if VERBOSITY > 2:
-      print "--"
-      print "Sub-encoders:"
+      print("--")
+      print("Sub-encoders:")
       for subEncoderName,_ in encoderDescription:
-        print "%s:%s" % (subEncoderName, stats[self.fieldname][subEncoderName])
+        print("%s:%s" % (subEncoderName, stats[self.fieldname][subEncoderName]))
 
 def generateStats(filename, maxSamples = None,):
   """
@@ -267,8 +267,8 @@
                            }
 
   filename = resource_filename("nupic.datafiles", filename)
-  print "*"*40
-  print "Collecting statistics for file:'%s'" % (filename,)
+  print("*"*40)
+  print("Collecting statistics for file:'%s'" % (filename,))
   dataFile = FileRecordStream(filename)
 
   # Initialize collector objects
@@ -284,7 +284,7 @@
   # Now collect the stats
   if maxSamples is None:
     maxSamples = 500000
-  for i in xrange(maxSamples):
+  for i in range(maxSamples):
     record = dataFile.getNextRecord()
     if record is None:
       break
--- d:\nupic\src\python\python27\nupic\data\stream_reader.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\stream_reader.py	(refactored)
@@ -354,7 +354,7 @@
     # Do we need to re-order the fields in the record?
     if self._needFieldsFiltering:
       values = []
-      srcDict = dict(zip(self._recordStoreFieldNames, fieldValues))
+      srcDict = dict(list(zip(self._recordStoreFieldNames, fieldValues)))
       for name in self._streamFieldNames:
         values.append(srcDict[name])
       fieldValues = values
@@ -483,8 +483,8 @@
 
     # We need to convert each item to represent the fields of the *stream*
     streamStats = dict()
-    for (key, values) in recordStoreStats.items():
-      fieldStats = dict(zip(self._recordStoreFieldNames, values))
+    for (key, values) in list(recordStoreStats.items()):
+      fieldStats = dict(list(zip(self._recordStoreFieldNames, values)))
       streamValues = []
       for name in self._streamFieldNames:
         streamValues.append(fieldStats[name])
--- d:\nupic\src\python\python27\nupic\data\utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\utils.py	(refactored)
@@ -146,8 +146,8 @@
   if s is None:
     return ''
 
-  assert isinstance(s, basestring), \
-        "expected %s but got %s; value=%s" % (basestring, type(s), s)
+  assert isinstance(s, str), \
+        "expected %s but got %s; value=%s" % (str, type(s), s)
   s = s.replace('\\', '\\\\')
   s = s.replace('\n', '\\n')
   s = s.replace('\t', '\\t')
@@ -165,7 +165,7 @@
   :param s: (string) to unescape
   :returns: (string) unescaped string
   """
-  assert isinstance(s, basestring)
+  assert isinstance(s, str)
   s = s.replace('\t', ',')
   s = s.replace('\\,', ',')
   s = s.replace('\\n', '\n')
@@ -182,7 +182,7 @@
   :param s: (string) string to parse
   :returns: (list) SDR out
   """
-  assert isinstance(s, basestring)
+  assert isinstance(s, str)
   sdr = [int(c) for c in s if c in ("0", "1")]
   if len(sdr) != len(s):
     raise ValueError("The provided string %s is malformed. The string should "
@@ -211,7 +211,7 @@
   :param s: (string) to parse
   :returns: (list) binary SDR
   """
-  assert isinstance(s, basestring)
+  assert isinstance(s, str)
   return [int(i) for i in s.split()]
 
 
--- d:\nupic\src\python\python27\nupic\data\generators\anomalyzer.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\generators\anomalyzer.py	(refactored)
@@ -165,7 +165,7 @@
   # Select random rows in the sample range to delete until the desired number
   # of rows are left.
   numDeletes =  initialN - n
-  for i in xrange(numDeletes):
+  for i in range(numDeletes):
     delIndex = random.randint(start, stop - i)
     del rows[delIndex]
   # Remove outside rows if specified.
@@ -237,6 +237,6 @@
 
 if __name__ == "__main__":
   if len(sys.argv) <= 1:
-    print USAGE
+    print(USAGE)
     sys.exit(1)
   main(sys.argv[1:])
--- d:\nupic\src\python\python27\nupic\data\generators\data_generator.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\generators\data_generator.py	(refactored)
@@ -85,10 +85,10 @@
     try:
       dataClass=eval(dataClassName)(fieldParams)
 
-    except TypeError, e:
-      print ("#### Error in constructing %s class object. Possibly missing "
+    except TypeError as e:
+      print(("#### Error in constructing %s class object. Possibly missing "
               "some required constructor parameters. Parameters "
-              "that were provided are: %s" % (dataClass, fieldParams))
+              "that were provided are: %s" % (dataClass, fieldParams)))
       raise
 
     encoderParams['dataClass']=dataClass
@@ -166,7 +166,7 @@
   def generateRecords(self, records):
     """Generate multiple records. Refer to definition for generateRecord"""
 
-    if self.verbosity>0: print 'Generating', len(records), 'records...'
+    if self.verbosity>0: print('Generating', len(records), 'records...')
     for record in records:
       self.generateRecord(record)
 
@@ -206,7 +206,7 @@
                    the corresponding fields
     """
     encoding=[self.fields[i].encodeValue(record[i], toBeAdded) for i in \
-              xrange(len(self.fields))]
+              range(len(self.fields))]
 
     return encoding
 
@@ -224,7 +224,7 @@
     """
     if records is None:
       records = self.getAllRecords()
-    if self.verbosity>0: print 'Encoding', len(records), 'records.'
+    if self.verbosity>0: print('Encoding', len(records), 'records.')
     encodings = [self.encodeRecord(record, toBeAdded) for record in records]
 
     return encodings
@@ -359,8 +359,8 @@
       writer.writerow(self.getAllFlags())
       writer.writerows(self.getAllRecords())
     if self.verbosity>0:
-      print '******', numRecords,'records exported in numenta format to file:',\
-                path,'******\n'
+      print('******', numRecords,'records exported in numenta format to file:',\
+                path,'******\n')
 
 
   def removeAllRecords(self):
--- d:\nupic\src\python\python27\nupic\data\generators\pattern_machine.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\generators\pattern_machine.py	(refactored)
@@ -105,7 +105,7 @@
 
     numbers = set()
 
-    for index, pattern in self._patterns.iteritems():
+    for index, pattern in self._patterns.items():
       if bit in pattern:
         numbers.add(index)
 
@@ -148,8 +148,8 @@
     text = ""
 
     numberList = []
-    numberItems = sorted(numberMap.iteritems(),
-                         key=lambda (number, bits): len(bits),
+    numberItems = sorted(iter(numberMap.items()),
+                         key=lambda number_bits: len(number_bits[1]),
                          reverse=True)
 
     for number, bits in numberItems:
@@ -173,8 +173,8 @@
     """
     Generates set of random patterns.
     """
-    candidates = np.array(range(self._n), np.uint32)
-    for i in xrange(self._num):
+    candidates = np.array(list(range(self._n)), np.uint32)
+    for i in range(self._num):
       self._random.shuffle(candidates)
       pattern = candidates[0:self._getW()]
       self._patterns[i] = set(pattern)
@@ -208,6 +208,6 @@
 
     assert type(w) is int, "List for w not supported"
 
-    for i in xrange(n / w):
-      pattern = set(xrange(i * w, (i+1) * w))
+    for i in range(n / w):
+      pattern = set(range(i * w, (i+1) * w))
       self._patterns[i] = pattern
--- d:\nupic\src\python\python27\nupic\data\generators\sequence_machine.py	(original)
+++ d:\nupic\src\python\python27\nupic\data\generators\sequence_machine.py	(refactored)
@@ -99,7 +99,7 @@
     """
     text = ""
 
-    for i in xrange(len(sequence)):
+    for i in range(len(sequence)):
       pattern = sequence[i]
 
       if pattern == None:
@@ -128,12 +128,12 @@
     if sharedRange:
       sharedStart, sharedEnd = sharedRange
       sharedLength = sharedEnd - sharedStart
-      sharedNumbers = range(numSequences * sequenceLength,
-                            numSequences * sequenceLength + sharedLength)
+      sharedNumbers = list(range(numSequences * sequenceLength,
+                            numSequences * sequenceLength + sharedLength))
 
-    for i in xrange(numSequences):
+    for i in range(numSequences):
       start = i * sequenceLength
-      newNumbers = np.array(range(start, start + sequenceLength), np.uint32)
+      newNumbers = np.array(list(range(start, start + sequenceLength)), np.uint32)
       self._random.shuffle(newNumbers)
       newNumbers = list(newNumbers)
 
--- d:\nupic\src\python\python27\nupic\database\client_jobs_dao.py	(original)
+++ d:\nupic\src\python\python27\nupic\database\client_jobs_dao.py	(refactored)
@@ -21,7 +21,7 @@
 
 # Add Context Manager (with ...) support for Jython/Python 2.5.x (
 # ClientJobManager used to use Jython); it's a noop in newer Python versions.
-from __future__ import with_statement
+
 
 import collections
 import logging
@@ -905,13 +905,13 @@
     # ---------------------------------------------------------------------
     # Generate the name conversion dicts
     self._jobs.pubToDBNameDict = dict(
-      zip(self._jobs.publicFieldNames, self._jobs.dbFieldNames))
+      list(zip(self._jobs.publicFieldNames, self._jobs.dbFieldNames)))
     self._jobs.dbToPubNameDict = dict(
-      zip(self._jobs.dbFieldNames, self._jobs.publicFieldNames))
+      list(zip(self._jobs.dbFieldNames, self._jobs.publicFieldNames)))
     self._models.pubToDBNameDict = dict(
-      zip(self._models.publicFieldNames, self._models.dbFieldNames))
+      list(zip(self._models.publicFieldNames, self._models.dbFieldNames)))
     self._models.dbToPubNameDict = dict(
-      zip(self._models.dbFieldNames, self._models.publicFieldNames))
+      list(zip(self._models.dbFieldNames, self._models.publicFieldNames)))
 
 
     # ---------------------------------------------------------------------
@@ -951,14 +951,14 @@
 
     assert fieldsToMatch, repr(fieldsToMatch)
     assert all(k in tableInfo.dbFieldNames
-               for k in fieldsToMatch.iterkeys()), repr(fieldsToMatch)
+               for k in fieldsToMatch.keys()), repr(fieldsToMatch)
 
     assert selectFieldNames, repr(selectFieldNames)
     assert all(f in tableInfo.dbFieldNames for f in selectFieldNames), repr(
       selectFieldNames)
 
     # NOTE: make sure match expressions and values are in the same order
-    matchPairs = fieldsToMatch.items()
+    matchPairs = list(fieldsToMatch.items())
     matchExpressionGen = (
       p[0] +
       (' IS ' + {True:'TRUE', False:'FALSE'}[p[1]] if isinstance(p[1], bool)
@@ -2126,8 +2126,8 @@
     # Form the sequecce of key=value strings that will go into the
     #  request
     assignmentExpressions = ','.join(
-      ["%s=%%s" % (self._jobs.pubToDBNameDict[f],) for f in fields.iterkeys()])
-    assignmentValues = fields.values()
+      ["%s=%%s" % (self._jobs.pubToDBNameDict[f],) for f in fields.keys()])
+    assignmentValues = list(fields.values())
 
     query = 'UPDATE %s SET %s ' \
             '          WHERE job_id=%%s' \
@@ -2342,7 +2342,7 @@
                      particleHash, self._connectionID)
         try:
           numRowsAffected = conn.cursor.execute(query, sqlParams)
-        except Exception, e:
+        except Exception as e:
           # NOTE: We have seen instances where some package in the calling
           #  chain tries to interpret the exception message using unicode.
           #  Since the exception message contains binary data (the hashes), this
@@ -2610,8 +2610,8 @@
     # Form the sequence of key=value strings that will go into the
     #  request
     assignmentExpressions = ','.join(
-      '%s=%%s' % (self._models.pubToDBNameDict[f],) for f in fields.iterkeys())
-    assignmentValues = fields.values()
+      '%s=%%s' % (self._models.pubToDBNameDict[f],) for f in fields.keys())
+    assignmentValues = list(fields.values())
 
     query = 'UPDATE %s SET %s, update_counter = update_counter+1 ' \
             '          WHERE model_id=%%s' \
@@ -3308,4 +3308,4 @@
   # Print DB name?
   if options.getDBName:
     cjDAO = ClientJobsDAO()
-    print cjDAO.dbName
+    print(cjDAO.dbName)
--- d:\nupic\src\python\python27\nupic\datafiles\extra\firstOrder\raw\makeDataset.py	(original)
+++ d:\nupic\src\python\python27\nupic\datafiles\extra\firstOrder\raw\makeDataset.py	(refactored)
@@ -44,21 +44,21 @@
 def generateFirstOrderData(model, numIterations=10000, seqLength=5,
                            resets=True, suffix='train'):
   
-  print "Creating %d iteration file with seqLength %d" % (numIterations, seqLength)
-  print "Filename", 
+  print("Creating %d iteration file with seqLength %d" % (numIterations, seqLength))
+  print("Filename", end=' ') 
   categoryList, initProbability, transitionTable = model
   initProbability = initProbability.cumsum()
   transitionTable = transitionTable.cumsum(axis=1)
   
   outputFile = 'fo_%d_%d_%s.csv' % (numIterations, seqLength, suffix)
-  print "Filename", outputFile
+  print("Filename", outputFile)
   fields = [('reset', 'int', 'R'), ('name', 'string', '')]
   o = File(outputFile, fields)
   
   seqIdx = 0
   rand = numpy.random.rand()
   catIdx = numpy.searchsorted(initProbability, rand)
-  for i in xrange(numIterations):
+  for i in range(numIterations):
     rand = numpy.random.rand()
     if seqIdx == 0 and resets:
       catIdx = numpy.searchsorted(initProbability, rand)
--- d:\nupic\src\python\python27\nupic\datafiles\extra\generated\GenerateSampleData.py	(original)
+++ d:\nupic\src\python\python27\nupic\datafiles\extra\generated\GenerateSampleData.py	(refactored)
@@ -196,7 +196,7 @@
   # (a,d -> g), (a, e -> h), (a, f -> i) and so on, with probabilities
   # (0.9, 0.05, 0.05)
   
-  print 'Generating %s with %s records, test #%s' % \
-        (sys.argv[1], sys.argv[2], sys.argv[3])
+  print('Generating %s with %s records, test #%s' % \
+        (sys.argv[1], sys.argv[2], sys.argv[3]))
         
   writeSimpleTest1(sys.argv[1], int(sys.argv[2]), int(sys.argv[3]))
--- d:\nupic\src\python\python27\nupic\datafiles\extra\gym\raw\makeDataset.py	(original)
+++ d:\nupic\src\python\python27\nupic\datafiles\extra\gym\raw\makeDataset.py	(refactored)
@@ -58,19 +58,19 @@
     
   def processAttendance(self, f):
     # Skip first two
-    line = f.next()
+    line = next(f)
     assert line == ',,,,,,,,,,,,,,,,,,,\n'
 
-    line = f.next()
+    line = next(f)
     assert line == 'Date Of Swipe, < 6 am,6-7 am,7-8 am,8-9 am,9-10 am,10-11 am,11-12 am,12-1 pm,1-2 pm,2-3 pm,3-4 pm,4-5 pm,5-6 pm,6-7 pm,7-8 pm,8-9 pm,9-10 pm,> 10 pm,Totals\n'
     
     for i, line in enumerate(f):
       # Check weather we're done with this club
       if line == ',,,,,,,,,,,,,,,,,,,\n':
         # skip next two lines
-        line = f.next()
+        line = next(f)
         assert line.startswith('Club Totals:')
-        line = f.next()
+        line = next(f)
         assert line == ',,,,,,,,,,,,,,,,,,,\n'
         return
       else:
@@ -116,7 +116,7 @@
     # Locate record
     key = ((yyyy, mm, dd), t)
     if not key in self.records:
-      print self.name, 'is missing attendance data for', key
+      print(self.name, 'is missing attendance data for', key)
     else:
       r = self.records[key]
       r.consumption = consumption
@@ -136,9 +136,9 @@
   """
   try:
     # Skip as many empty lines as necessary (file format inconsistent)
-    line = f.next()
+    line = next(f)
     while line == ',,,,,,,,,,,,,,,,,,,\n':
-      line = f.next()
+      line = next(f)
     
     # The first non-empty line should have the name as the first field
     name = line.split(',')[0]
@@ -168,10 +168,10 @@
   """
   try:
     # Skip header line
-    line = f.next()
+    line = next(f)
     assert line.endswith('"   ","SITE_LOCATION_NAME","TIMESTAMP","TOTAL_KWH"\n')
 
-    valid_times = range(24)
+    valid_times = list(range(24))
     t = 0 # used to track time
     club = None
     clubName = None
@@ -267,8 +267,8 @@
   with File('gym.csv', fields) as f:
     ## write header
     #f.write('Gym Name,Date,Time,Attendee Count,Consumption (KWH)\n')
-    for c in clubs.values():
-      for k, r in sorted(c.records.iteritems(), key=operator.itemgetter(0)):          
+    for c in list(clubs.values()):
+      for k, r in sorted(iter(c.records.items()), key=operator.itemgetter(0)):          
         #dd = r.date[2]
         #mm = r.date[1]
         #yyyy = r.date[0]
@@ -279,5 +279,5 @@
       
 if __name__=='__main__':
   makeDataset()
-  print 'Done.'
-  
+  print('Done.')
+  
--- d:\nupic\src\python\python27\nupic\datafiles\extra\hotgym\raw\makeDataset.py	(original)
+++ d:\nupic\src\python\python27\nupic\datafiles\extra\hotgym\raw\makeDataset.py	(refactored)
@@ -110,7 +110,7 @@
       f.readline()
       
       # iterate over all the lines in the input file
-      for line in f.xreadlines():
+      for line in f:
         
         # Parse the fields in the current line
         record = _parseLine(line)
@@ -120,7 +120,7 @@
         
         if record[0] != gymName:
           gymName = record[0]
-          print gymName
+          print(gymName)
  		  
   return total, missing
   
@@ -128,6 +128,6 @@
 if __name__ == '__main__':
   makeDataset()
   
-  print 'Done.'
+  print('Done.')
   
 
--- d:\nupic\src\python\python27\nupic\datafiles\extra\regression\makeDataset.py	(original)
+++ d:\nupic\src\python\python27\nupic\datafiles\extra\regression\makeDataset.py	(refactored)
@@ -99,13 +99,13 @@
   """
   
   # Create the file
-  print "Creating %s..." % (filename)
+  print("Creating %s..." % (filename))
   numRecords, numFields = data.shape
   
   fields = [('field%d'%(i+1), 'float', '') for i in range(numFields)]
   outFile = File(filename, fields)
   
-  for i in xrange(numRecords):
+  for i in range(numRecords):
     outFile.write(data[i].tolist())
     
   outFile.close()
--- d:\nupic\src\python\python27\nupic\datafiles\extra\secondOrder\makeDataset.py	(original)
+++ d:\nupic\src\python\python27\nupic\datafiles\extra\secondOrder\makeDataset.py	(refactored)
@@ -388,7 +388,7 @@
   """
   
   # Create the file
-  print "Creating %s..." % (filename)
+  print("Creating %s..." % (filename))
   fields = [('reset', 'int', 'R'), ('name', 'string', '')]
   outFile = FileRecordStream(filename, write=True, fields=fields)
   
@@ -397,11 +397,11 @@
   initCumProb = initProb.cumsum()
   
   firstOrderCumProb = dict()
-  for (key,value) in firstOrderProb.iteritems():
+  for (key,value) in firstOrderProb.items():
     firstOrderCumProb[key] = value.cumsum()
     
   secondOrderCumProb = dict()
-  for (key,value) in secondOrderProb.iteritems():
+  for (key,value) in secondOrderProb.items():
     secondOrderCumProb[key] = value.cumsum()
     
 
@@ -410,7 +410,7 @@
   elementsInSeq = []
   numElementsSinceReset = 0
   maxCatIdx = len(categoryList) - 1
-  for i in xrange(numRecords):
+  for i in range(numRecords):
 
     # Generate a reset?
     if numElementsSinceReset == 0:
@@ -465,7 +465,7 @@
   
   # =====================================================================
   # Create our categories and category file. 
-  print "Creating %s..." % (filenameCategory)
+  print("Creating %s..." % (filenameCategory))
   categoryList = ['cat%d' % i for i in range(1, numCategories+1)]
   categoryFile = open(filenameCategory, 'w')
   for category in categoryList:
--- d:\nupic\src\python\python27\nupic\encoders\__init__.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\__init__.py	(refactored)
@@ -19,19 +19,19 @@
 # http://numenta.org/licenses/
 # ----------------------------------------------------------------------
 
-from scalar import ScalarEncoder
-from random_distributed_scalar import RandomDistributedScalarEncoder
-from adaptive_scalar import AdaptiveScalarEncoder
-from date import DateEncoder
-from logarithm import LogEncoder
-from category import CategoryEncoder
-from sdr_category import SDRCategoryEncoder
-from delta import DeltaEncoder
-from scalar_space import ScalarSpaceEncoder
-from coordinate import CoordinateEncoder
-from geospatial_coordinate import GeospatialCoordinateEncoder
-from pass_through import PassThroughEncoder
-from sparse_pass_through import SparsePassThroughEncoder
+from .scalar import ScalarEncoder
+from .random_distributed_scalar import RandomDistributedScalarEncoder
+from .adaptive_scalar import AdaptiveScalarEncoder
+from .date import DateEncoder
+from .logarithm import LogEncoder
+from .category import CategoryEncoder
+from .sdr_category import SDRCategoryEncoder
+from .delta import DeltaEncoder
+from .scalar_space import ScalarSpaceEncoder
+from .coordinate import CoordinateEncoder
+from .geospatial_coordinate import GeospatialCoordinateEncoder
+from .pass_through import PassThroughEncoder
+from .sparse_pass_through import SparsePassThroughEncoder
 # multiencoder must be imported last because it imports * from this module!
-from multi import MultiEncoder
-from utils import bitsToString
+from .multi import MultiEncoder
+from .utils import bitsToString
--- d:\nupic\src\python\python27\nupic\encoders\adaptive_scalar.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\adaptive_scalar.py	(refactored)
@@ -126,16 +126,16 @@
       if minOverWindow < self.minval:
         #initialBump = abs(self.minval-minOverWindow)*(1-(min(self.recordNum, 200.0)/200.0))*2      #decrement minval more aggressively in the beginning
         if self.verbosity >= 2:
-          print "Input %s=%.2f smaller than minval %.2f. Adjusting minval to %.2f"\
-                          % (self.name, input, self.minval, minOverWindow)
+          print("Input %s=%.2f smaller than minval %.2f. Adjusting minval to %.2f"\
+                          % (self.name, input, self.minval, minOverWindow))
         self.minval = minOverWindow       #-initialBump
         self._setEncoderParams()
 
       if maxOverWindow > self.maxval:
         #initialBump = abs(self.maxval-maxOverWindow)*(1-(min(self.recordNum, 200.0)/200.0))*2     #decrement maxval more aggressively in the beginning
         if self.verbosity >= 2:
-          print "Input %s=%.2f greater than maxval %.2f. Adjusting maxval to %.2f" \
-                          % (self.name, input, self.maxval, maxOverWindow)
+          print("Input %s=%.2f greater than maxval %.2f. Adjusting maxval to %.2f" \
+                          % (self.name, input, self.maxval, maxOverWindow))
         self.maxval = maxOverWindow       #+initialBump
         self._setEncoderParams()
 
--- d:\nupic\src\python\python27\nupic\encoders\base.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\base.py	(refactored)
@@ -220,7 +220,7 @@
     if isinstance(obj, dict):
       if not fieldName in obj:
         knownFields = ", ".join(
-          key for key in obj.keys() if not key.startswith("_")
+          key for key in list(obj.keys()) if not key.startswith("_")
         )
         raise ValueError(
           "Unknown field name '%s' in input record. Known fields are '%s'.\n"
@@ -404,7 +404,7 @@
 
     # Find which field it's in
     description = self.getDescription() + [("end", self.getWidth())]
-    for i in xrange(len(description)):
+    for i in range(len(description)):
       (name, offset) = description[i]
       if (name == fieldName):
         break
@@ -430,7 +430,7 @@
     # Find which field it's in
     (prevFieldName, prevFieldOffset) = (None, None)
     description = self.getDescription()
-    for i in xrange(len(description)):
+    for i in range(len(description)):
       (name, offset) = description[i]
       if formatted:
         offset = offset + i
@@ -460,9 +460,9 @@
 
     :param prefix: printed before the header if specified
     """
-    print prefix,
+    print(prefix, end=' ')
     description = self.getDescription() + [("end", self.getWidth())]
-    for i in xrange(len(description) - 1):
+    for i in range(len(description) - 1):
       name = description[i][0]
       width = description[i+1][1] - description[i][1]
       formatStr = "%%-%ds |" % width
@@ -470,9 +470,9 @@
         pname = name[0:width]
       else:
         pname = name
-      print formatStr % pname,
-    print
-    print prefix, "-" * (self.getWidth() + (len(description) - 1)*3 - 1)
+      print(formatStr % pname, end=' ')
+    print()
+    print(prefix, "-" * (self.getWidth() + (len(description) - 1)*3 - 1))
 
 
   def pprint(self, output, prefix=""):
@@ -482,13 +482,13 @@
     :param output: to print
     :param prefix: printed before the header if specified
     """
-    print prefix,
+    print(prefix, end=' ')
     description = self.getDescription() + [("end", self.getWidth())]
-    for i in xrange(len(description) - 1):
+    for i in range(len(description) - 1):
       offset = description[i][1]
       nextoffset = description[i+1][1]
-      print "%s |" % bitsToString(output[offset:nextoffset]),
-    print
+      print("%s |" % bitsToString(output[offset:nextoffset]), end=' ')
+    print()
 
 
   def decode(self, encoded, parentFieldName=''):
@@ -571,7 +571,7 @@
 
     if self.encoders is not None:
       # Merge decodings of all child encoders together
-      for i in xrange(len(self.encoders)):
+      for i in range(len(self.encoders)):
 
         # Get the encoder and the encoded output
         (name, encoder, offset) = self.encoders[i]
@@ -649,7 +649,7 @@
     # Concatenate the results from bucketInfo on each child encoder
     retVals = []
     bucketOffset = 0
-    for i in xrange(len(self.encoders)):
+    for i in range(len(self.encoders)):
       (name, encoder, offset) = self.encoders[i]
 
       if encoder.encoders is not None:
@@ -685,7 +685,7 @@
 
     # Concatenate the results from topDownCompute on each child encoder
     retVals = []
-    for i in xrange(len(self.encoders)):
+    for i in range(len(self.encoders)):
       (name, encoder, offset) = self.encoders[i]
 
       if i < len(self.encoders)-1:
--- d:\nupic\src\python\python27\nupic\encoders\category.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\category.py	(refactored)
@@ -60,7 +60,7 @@
     self.categoryToIndex = dict()
     self.indexToCategory = dict()
     self.indexToCategory[0] = UNKNOWN
-    for i in xrange(len(categoryList)):
+    for i in range(len(categoryList)):
       self.categoryToIndex[categoryList[i]] = i+1
       self.indexToCategory[i+1] = categoryList[i]
 
@@ -124,8 +124,8 @@
       self.encoder.encodeIntoArray(val, output)
 
     if self.verbosity >= 2:
-      print "input:", input, "va:", val, "output:", output
-      print "decoded:", self.decodedToStr(self.decode(output))
+      print("input:", input, "va:", val, "output:", output)
+      print("decoded:", self.decodedToStr(self.decode(output)))
 
 
   def decode(self, encoded, parentFieldName=''):
@@ -142,7 +142,7 @@
 
     # Get the list of categories the scalar values correspond to and
     #  generate the description from the category name(s).
-    (inRanges, inDesc) = fieldsDict.values()[0]
+    (inRanges, inDesc) = list(fieldsDict.values())[0]
     outRanges = []
     desc = ""
     for (minV, maxV) in inRanges:
@@ -235,7 +235,7 @@
                                for x in proto.indexToCategory}
     encoder.categoryToIndex = {category: index
                                for index, category
-                               in encoder.indexToCategory.items()
+                               in list(encoder.indexToCategory.items())
                                if category != UNKNOWN}
     encoder._topDownMappingM = None
     encoder._bucketValues = None
@@ -247,7 +247,7 @@
     proto.width = self.width
     proto.indexToCategory = [
       {"index": index, "category": category}
-      for index, category in self.indexToCategory.items()
+      for index, category in list(self.indexToCategory.items())
     ]
     proto.name = self.name
     proto.verbosity = self.verbosity
--- d:\nupic\src\python\python27\nupic\encoders\coordinate.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\coordinate.py	(refactored)
@@ -124,7 +124,7 @@
 
     @return (numpy.array) List of coordinates
     """
-    ranges = (xrange(n-radius, n+radius+1) for n in coordinate.tolist())
+    ranges = (range(n-radius, n+radius+1) for n in coordinate.tolist())
     return numpy.array(list(itertools.product(*ranges)))
 
 
--- d:\nupic\src\python\python27\nupic\encoders\date.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\date.py	(refactored)
@@ -357,7 +357,7 @@
 
       # Encoder each sub-field
       result = []
-      for i in xrange(len(self.encoders)):
+      for i in range(len(self.encoders)):
         (name, encoder, offset) = self.encoders[i]
         result.extend(encoder.getBucketIndices(scalars[i]))
       return result
@@ -376,7 +376,7 @@
       # Get the scalar values for each sub-field
       scalars = self.getScalars(input)
       # Encoder each sub-field
-      for i in xrange(len(self.encoders)):
+      for i in range(len(self.encoders)):
         (name, encoder, offset) = self.encoders[i]
         encoder.encodeIntoArray(scalars[i], output[offset:])
 
--- d:\nupic\src\python\python27\nupic\encoders\logarithm.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\logarithm.py	(refactored)
@@ -166,8 +166,8 @@
       self.encoder.encodeIntoArray(scaledVal, output)
 
       if self.verbosity >= 2:
-        print "input:", inpt, "scaledVal:", scaledVal, "output:", output
-        print "decoded:", self.decodedToStr(self.decode(output))
+        print("input:", inpt, "scaledVal:", scaledVal, "output:", output)
+        print("decoded:", self.decodedToStr(self.decode(output)))
 
 
   def decode(self, encoded, parentFieldName=''):
@@ -184,7 +184,7 @@
     assert(len(fieldsDict) == 1)
 
     # Convert each range into normal space
-    (inRanges, inDesc) = fieldsDict.values()[0]
+    (inRanges, inDesc) = list(fieldsDict.values())[0]
     outRanges = []
     for (minV, maxV) in inRanges:
       outRanges.append((math.pow(10, minV),
@@ -193,7 +193,7 @@
     # Generate a text description of the ranges
     desc = ""
     numRanges = len(outRanges)
-    for i in xrange(numRanges):
+    for i in range(numRanges):
       if outRanges[i][0] != outRanges[i][1]:
         desc += "%.2f-%.2f" % (outRanges[i][0], outRanges[i][1])
       else:
--- d:\nupic\src\python\python27\nupic\encoders\multi.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\multi.py	(refactored)
@@ -52,7 +52,7 @@
 }
 
 # Invert for fast lookup in MultiEncoder.read()
-_ATTR_CLASS_MAP = {value: key for key, value in _CLASS_ATTR_MAP.items()}
+_ATTR_CLASS_MAP = {value: key for key, value in list(_CLASS_ATTR_MAP.items())}
 
 
 class MultiEncoder(Encoder):
@@ -171,10 +171,10 @@
         encoderName = fieldParams.pop('type')
         try:
           self.addEncoder(fieldName, eval(encoderName)(**fieldParams))
-        except TypeError, e:
-          print ("#### Error in constructing %s encoder. Possibly missing "
+        except TypeError as e:
+          print(("#### Error in constructing %s encoder. Possibly missing "
                 "some required constructor parameters. Parameters "
-                "that were provided are: %s" %  (encoderName, fieldParams))
+                "that were provided are: %s" %  (encoderName, fieldParams)))
           raise
 
 
--- d:\nupic\src\python\python27\nupic\encoders\pass_through.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\pass_through.py	(refactored)
@@ -86,8 +86,8 @@
     outputVal[:] = inputVal[:]
 
     if self.verbosity >= 2:
-      print "input:", inputVal, "output:", outputVal
-      print "decoded:", self.decodedToStr(self.decode(outputVal))
+      print("input:", inputVal, "output:", outputVal)
+      print("decoded:", self.decodedToStr(self.decode(outputVal)))
 
 
   def decode(self, encoded, parentFieldName=""):
--- d:\nupic\src\python\python27\nupic\encoders\random_distributed_scalar.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\random_distributed_scalar.py	(refactored)
@@ -163,7 +163,7 @@
     # previous numpy random state
     randomState = state["random"]
     if isinstance(randomState, numpy.random.mtrand.RandomState):
-      self.random = NupicRandom(randomState.randint(sys.maxint))
+      self.random = NupicRandom(randomState.randint(sys.maxsize))
 
 
   def _seed(self, seed=-1):
@@ -227,9 +227,9 @@
     if index >= self._maxBuckets:
       index = self._maxBuckets-1
 
-    if not self.bucketMap.has_key(index):
+    if index not in self.bucketMap:
       if self.verbosity >= 2:
-        print "Adding additional buckets to handle index=", index
+        print("Adding additional buckets to handle index=", index)
       self._createBucket(index)
     return self.bucketMap[index]
 
@@ -365,7 +365,7 @@
     """
     Return the overlap between bucket indices i and j
     """
-    if self.bucketMap.has_key(i) and self.bucketMap.has_key(j):
+    if i in self.bucketMap and j in self.bucketMap:
       iRep = self.bucketMap[i]
       jRep = self.bucketMap[j]
       return self._countOverlap(iRep, jRep)
@@ -489,4 +489,4 @@
     proto.minIndex = self.minIndex
     proto.maxIndex = self.maxIndex
     proto.bucketMap = [{"key": key, "value": value.tolist()}
-                       for key, value in self.bucketMap.items()]
+                       for key, value in list(self.bucketMap.items())]
--- d:\nupic\src\python\python27\nupic\encoders\scalar.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\scalar.py	(refactored)
@@ -352,8 +352,8 @@
         # Don't clip periodic inputs. Out-of-range input is always an error
         if self.clipInput and not self.periodic:
           if self.verbosity > 0:
-            print "Clipped input %s=%.2f to minval %.2f" % (self.name, input,
-                                                            self.minval)
+            print("Clipped input %s=%.2f to minval %.2f" % (self.name, input,
+                                                            self.minval))
           input = self.minval
         else:
           raise Exception('input (%s) less than range (%s - %s)' %
@@ -368,8 +368,8 @@
         if input > self.maxval:
           if self.clipInput:
             if self.verbosity > 0:
-              print "Clipped input %s=%.2f to maxval %.2f" % (self.name, input,
-                                                              self.maxval)
+              print("Clipped input %s=%.2f to maxval %.2f" % (self.name, input,
+                                                              self.maxval))
             input = self.maxval
           else:
             raise Exception('input (%s) greater than range (%s - %s)' %
@@ -452,14 +452,14 @@
 
     # Debug the decode() method
     if self.verbosity >= 2:
-      print
-      print "input:", input
-      print "range:", self.minval, "-", self.maxval
-      print "n:", self.n, "w:", self.w, "resolution:", self.resolution, \
-            "radius", self.radius, "periodic:", self.periodic
-      print "output:",
+      print()
+      print("input:", input)
+      print("range:", self.minval, "-", self.maxval)
+      print("n:", self.n, "w:", self.w, "resolution:", self.resolution, \
+            "radius", self.radius, "periodic:", self.periodic)
+      print("output:", end=' ')
       self.pprint(output)
-      print "input desc:", self.decodedToStr(self.decode(output))
+      print("input desc:", self.decodedToStr(self.decode(output)))
 
 
   def decode(self, encoded, parentFieldName=''):
@@ -480,28 +480,28 @@
 
     # Search for portions of the output that have "holes"
     maxZerosInARow = self.halfwidth
-    for i in xrange(maxZerosInARow):
+    for i in range(maxZerosInARow):
       searchStr = numpy.ones(i + 3, dtype=encoded.dtype)
       searchStr[1:-1] = 0
       subLen = len(searchStr)
 
       # Does this search string appear in the output?
       if self.periodic:
-        for j in xrange(self.n):
+        for j in range(self.n):
           outputIndices = numpy.arange(j, j + subLen)
           outputIndices %= self.n
           if numpy.array_equal(searchStr, tmpOutput[outputIndices]):
             tmpOutput[outputIndices] = 1
 
       else:
-        for j in xrange(self.n - subLen + 1):
+        for j in range(self.n - subLen + 1):
           if numpy.array_equal(searchStr, tmpOutput[j:j + subLen]):
             tmpOutput[j:j + subLen] = 1
 
 
     if self.verbosity >= 2:
-      print "raw output:", encoded[:self.n]
-      print "filtered output:", tmpOutput
+      print("raw output:", encoded[:self.n])
+      print("filtered output:", tmpOutput)
 
     # ------------------------------------------------------------------------
     # Find each run of 1's.
@@ -586,7 +586,7 @@
     """generate description from a text description of the ranges"""
     desc = ""
     numRanges = len(ranges)
-    for i in xrange(numRanges):
+    for i in range(numRanges):
       if ranges[i][0] != ranges[i][1]:
         desc += "%.2f-%.2f" % (ranges[i][0], ranges[i][1])
       else:
@@ -622,7 +622,7 @@
       self._topDownMappingM = SM32(numCategories, self.n)
 
       outputSpace = numpy.zeros(self.n, dtype=GetNTAReal())
-      for i in xrange(numCategories):
+      for i in range(numCategories):
         value = self._topDownValues[i]
         value = max(value, self.minval)
         value = min(value, self.maxval)
--- d:\nupic\src\python\python27\nupic\encoders\sdr_category.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\sdr_category.py	(refactored)
@@ -128,7 +128,7 @@
     # previous numpy random state
     randomState = state["random"]
     if isinstance(randomState, numpy.random.mtrand.RandomState):
-      self.random = NupicRandom(randomState.randint(sys.maxint))
+      self.random = NupicRandom(randomState.randint(sys.maxsize))
 
 
   def _seed(self, seed=-1):
@@ -180,14 +180,14 @@
     of shape (n,). """
     maxAttempts = 1000
 
-    for _ in xrange(maxAttempts):
+    for _ in range(maxAttempts):
       foundUnique = True
       population = numpy.arange(self.n, dtype=numpy.uint32)
       choices = numpy.arange(self.w, dtype=numpy.uint32)
       oneBits = sorted(self.random.sample(population, choices))
       sdr =  numpy.zeros(self.n, dtype='uint8')
       sdr[oneBits] = 1
-      for i in xrange(self.ncategories):
+      for i in range(self.ncategories):
         if (sdr == self.sdrs[i]).all():
           foundUnique = False
           break
@@ -241,8 +241,8 @@
       output[0:self.n] = self.sdrs[index,:]
 
     if self.verbosity >= 2:
-      print "input:", input, "index:", index, "output:", output
-      print "decoded:", self.decodedToStr(self.decode(output))
+      print("input:", input, "index:", index, "output:", output)
+      print("decoded:", self.decodedToStr(self.decode(output)))
 
 
   def decode(self, encoded, parentFieldName=''):
@@ -257,9 +257,9 @@
     overlaps =  (self.sdrs * encoded[0:self.n]).sum(axis=1)
 
     if self.verbosity >= 2:
-      print "Overlaps for decoding:"
-      for i in xrange(0, self.ncategories):
-        print "%d %s" % (overlaps[i], self.categories[i])
+      print("Overlaps for decoding:")
+      for i in range(0, self.ncategories):
+        print("%d %s" % (overlaps[i], self.categories[i]))
 
     matchingCategories =  (overlaps > self.thresholdOverlap).nonzero()[0]
 
@@ -291,7 +291,7 @@
       self._topDownMappingM = SM32(self.ncategories, self.n)
 
       outputSpace = numpy.zeros(self.n, dtype=GetNTAReal())
-      for i in xrange(self.ncategories):
+      for i in range(self.ncategories):
         self.encodeIntoArray(self.categories[i], outputSpace)
         self._topDownMappingM.setRowFromDense(i, outputSpace)
 
--- d:\nupic\src\python\python27\nupic\encoders\utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\encoders\utils.py	(refactored)
@@ -26,7 +26,7 @@
 def bitsToString(arr):
   """Returns a string representing a numpy array of 0's and 1's"""
   s = array('c','.'*len(arr))
-  for i in xrange(len(arr)):
+  for i in range(len(arr)):
     if arr[i] == 1:
       s[i]='*'
   return s
--- d:\nupic\src\python\python27\nupic\engine\__init__.py	(original)
+++ d:\nupic\src\python\python27\nupic\engine\__init__.py	(refactored)
@@ -79,7 +79,7 @@
 # stack trace define this environment variable
 if not 'NTA_STANDARD_PYTHON_UNHANDLED_EXCEPTIONS' in os.environ:
   import traceback
-  import cStringIO
+  import io
 
   def customExceptionHandler(type, value, tb):
     """Catch unhandled Python exception
@@ -92,7 +92,7 @@
     file.
     """
     # Print the exception info to a string IO buffer for manipulation
-    buff = cStringIO.StringIO()
+    buff = io.StringIO()
     traceback.print_exception(type, value, tb, file=buff)
 
     text = buff.getvalue()
@@ -123,17 +123,17 @@
 
     failMessage = 'The program failed with the following error message:'
     dashes = '-' * len(failMessage)
-    print
-    print dashes
-    print 'Traceback (most recent call last):'
-    print '\n'.join(lines[:begin-2])
+    print()
+    print(dashes)
+    print('Traceback (most recent call last):')
+    print('\n'.join(lines[:begin-2]))
     if stacktrace:
-      print stacktrace
-    print dashes
-    print 'The program failed with the following error message:'
-    print dashes
-    print message
-    print
+      print(stacktrace)
+    print(dashes)
+    print('The program failed with the following error message:')
+    print(dashes)
+    print(message)
+    print()
 
   #sys.excepthook = customExceptionHandler
 
@@ -282,7 +282,7 @@
     return values
 
   def items(self):
-    return zip(self.keys(), self.values())
+    return list(zip(list(self.keys()), list(self.values())))
 
   def __cmp__(self, other):
     return self.collection == other.collection
@@ -449,14 +449,14 @@
     Returns list of input names in spec.
     """
     inputs = self.getSpec().inputs
-    return [inputs.getByIndex(i)[0] for i in xrange(inputs.getCount())]
+    return [inputs.getByIndex(i)[0] for i in range(inputs.getCount())]
 
   def getOutputNames(self):
     """
     Returns list of output names in spec.
     """
     outputs = self.getSpec().outputs
-    return [outputs.getByIndex(i)[0] for i in xrange(outputs.getCount())]
+    return [outputs.getByIndex(i)[0] for i in range(outputs.getCount())]
 
   def executeCommand(self, args):
     """
@@ -604,7 +604,7 @@
         setattr(Network, obj, property(prop.fget, prop.fset, prop.fdel,
                                        docString))
       else:
-        obj.im_func.__doc__ = docString
+        obj.__func__.__doc__ = docString
 
   def _getRegions(self):
     """Get the collection of regions in a network
@@ -729,7 +729,7 @@
     """
     regions = []
 
-    for region in self.regions.values():
+    for region in list(self.regions.values()):
       if type(region.getSelf()) is regionClass:
         regions.append(region)
 
@@ -773,23 +773,23 @@
 
 if __name__ == '__main__':
   n = Network()
-  print n.regions
-  print len(n.regions)
-  print Network.regions.__doc__
+  print(n.regions)
+  print(len(n.regions))
+  print(Network.regions.__doc__)
 
   d = Dimensions([3, 4, 5])
-  print len(d)
-  print d
+  print(len(d))
+  print(d)
 
   a = Array('Byte', 5)
-  print len(a)
+  print(len(a))
   for i in range(len(a)):
     a[i] = ord('A') + i
 
   for i in range(len(a)):
-    print a[i]
+    print(a[i])
 
   r = n.addRegion('r', 'TestNode', '')
-  print 'name:', r.name
-  print 'node type:', r.type
-  print 'node spec:', r.spec
+  print('name:', r.name)
+  print('node type:', r.type)
+  print('node spec:', r.spec)
--- d:\nupic\src\python\python27\nupic\frameworks\opf\client.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\client.py	(refactored)
@@ -65,15 +65,15 @@
       self.sink.writeRecord(modelResult)
     return modelResult
 
-  def next(self):
-    record = self.datasetReader.next()
+  def __next__(self):
+    record = next(self.datasetReader)
     return self._processRecord(record)
 
   def skipNRecords(self, n):
     for i in range(n):
-      self.datasetReader.next()
+      next(self.datasetReader)
   def nextTruthPrediction(self, field):
-    record = self.datasetReader.next()
+    record = next(self.datasetReader)
     prediction=self._processRecord(record).inferences['prediction'][0]
     truth=record[field]
     return truth, prediction
@@ -83,7 +83,7 @@
     result = None
     while True:
       try:
-        result = self.next()
+        result = next(self)
         #print result
       except StopIteration:
         break
--- d:\nupic\src\python\python27\nupic\frameworks\opf\exp_description_api.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\exp_description_api.py	(refactored)
@@ -58,7 +58,7 @@
 
 
 
-class DescriptionIface(object):
+class DescriptionIface(object, metaclass=ABCMeta):
   """
   This is the base interface class for description API classes which provide
   OPF configuration parameters.
@@ -77,7 +77,6 @@
       'environment' parameter, which specifies the context in which the model is
       being run.
   """
-  __metaclass__ = ABCMeta
 
 
   @abstractmethod
@@ -199,14 +198,13 @@
 
         taskLabel = task['taskLabel']
 
-        assert isinstance(taskLabel, types.StringTypes), \
+        assert isinstance(taskLabel, (str,)), \
                "taskLabel type: %r" % type(taskLabel)
         assert len(taskLabel) > 0, "empty string taskLabel not is allowed"
 
         taskLabelsList.append(taskLabel.lower())
 
-      taskLabelDuplicates = filter(lambda x: taskLabelsList.count(x) > 1,
-                                   taskLabelsList)
+      taskLabelDuplicates = [x for x in taskLabelsList if taskLabelsList.count(x) > 1]
       assert len(taskLabelDuplicates) == 0, \
              "Duplcate task labels are not allowed: %s" % taskLabelDuplicates
 
--- d:\nupic\src\python\python27\nupic\frameworks\opf\exp_description_helpers.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\exp_description_helpers.py	(refactored)
@@ -360,7 +360,7 @@
   # Resolve value-getters within dictionaries, tuples and lists
 
   if isinstance(currentObj, dict):
-    for (key, value) in currentObj.items():
+    for (key, value) in list(currentObj.items()):
       if isinstance(value, ValueGetterBase):
         currentObj[key] = value(container)
 
--- d:\nupic\src\python\python27\nupic\frameworks\opf\experiment_runner.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\experiment_runner.py	(refactored)
@@ -378,8 +378,8 @@
 
 def _reportCommandLineUsageErrorAndExit(parser, message):
   """Report usage error and exit program with error indication."""
-  print parser.get_usage()
-  print message
+  print(parser.get_usage())
+  print(message)
   sys.exit(1)
 
 
@@ -430,10 +430,10 @@
 
   # Handle listTasks
   if options.privateOptions['listTasks']:
-    print "Available tasks:"
+    print("Available tasks:")
 
     for label in [t['taskLabel'] for t in experimentTasks]:
-      print "\t", label
+      print("\t", label)
 
     return None
 
@@ -449,7 +449,7 @@
           newSerialization=newSerialization)
 
   elif model is not None:
-    print "Skipping creation of OPFExperiment instance: caller provided his own"
+    print("Skipping creation of OPFExperiment instance: caller provided his own")
   else:
     modelDescription = expIface.getModelDescription()
     model = ModelFactory.create(modelDescription)
@@ -467,7 +467,7 @@
   # Build the task list
 
   # Default task execution index list is in the natural list order of the tasks
-  taskIndexList = range(len(experimentTasks))
+  taskIndexList = list(range(len(experimentTasks)))
 
   customTaskExecutionLabelsList = options.privateOptions['taskLabels']
   if customTaskExecutionLabelsList:
@@ -485,8 +485,8 @@
     taskIndexList = [taskLabelsList.index(label) for label in
                      customTaskExecutionLabelsList]
 
-    print "#### Executing custom task list: %r" % [taskLabelsList[i] for
-                                                   i in taskIndexList]
+    print("#### Executing custom task list: %r" % [taskLabelsList[i] for
+                                                   i in taskIndexList])
 
   # Run all experiment tasks
   for taskIndex in taskIndexList:
@@ -586,27 +586,27 @@
   checkpointParentDir = getCheckpointParentDir(experimentDir)
 
   if not os.path.exists(checkpointParentDir):
-    print "No available checkpoints."
+    print("No available checkpoints.")
     return
 
   checkpointDirs = [x for x in os.listdir(checkpointParentDir)
                     if _isCheckpointDir(os.path.join(checkpointParentDir, x))]
   if not checkpointDirs:
-    print "No available checkpoints."
+    print("No available checkpoints.")
     return
 
-  print "Available checkpoints:"
+  print("Available checkpoints:")
   checkpointList = [_checkpointLabelFromCheckpointDir(x)
                     for x in checkpointDirs]
 
   for checkpoint in sorted(checkpointList):
-    print "\t", checkpoint
-
-  print
-  print "To start from a checkpoint:"
-  print "  python run_opf_experiment.py experiment --load <CHECKPOINT>"
-  print "For example, to start from the checkpoint \"MyCheckpoint\":"
-  print "  python run_opf_experiment.py experiment --load MyCheckpoint"
+    print("\t", checkpoint)
+
+  print()
+  print("To start from a checkpoint:")
+  print("  python run_opf_experiment.py experiment --load <CHECKPOINT>")
+  print("For example, to start from the checkpoint \"MyCheckpoint\":")
+  print("  python run_opf_experiment.py experiment --load MyCheckpoint")
 
 
 
@@ -696,7 +696,7 @@
       numIters = self.__task['iterationCount']
 
     if numIters >= 0:
-      iterTracker = iter(xrange(numIters))
+      iterTracker = iter(range(numIters))
     else:
       iterTracker = iter(itertools.count())
 
@@ -724,7 +724,7 @@
 
       # Read next input record
       try:
-        inputRecord = self.__datasetReader.next()
+        inputRecord = next(self.__datasetReader)
       except StopIteration:
         break
 
@@ -854,7 +854,7 @@
       act =   self.Activity(repeating=req.repeating,
                             period=req.period,
                             cb=req.cb,
-                            iteratorHolder=[iter(xrange(req.period-1))])
+                            iteratorHolder=[iter(range(req.period-1))])
       self.__activities.append(act)
 
 
@@ -875,7 +875,7 @@
       except StopIteration:
         act.cb()
         if act.repeating:
-          act.iteratorHolder[0] = iter(xrange(act.period-1))
+          act.iteratorHolder[0] = iter(range(act.period-1))
         else:
           act.iteratorHolder[0] = None
 
--- d:\nupic\src\python\python27\nupic\frameworks\opf\helpers.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\helpers.py	(refactored)
@@ -31,7 +31,7 @@
 import imp
 import os
 
-import exp_description_api
+from . import exp_description_api
 
 
 def loadExperiment(path):
--- d:\nupic\src\python\python27\nupic\frameworks\opf\htm_prediction_model.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\htm_prediction_model.py	(refactored)
@@ -611,7 +611,7 @@
     classification = classificationDist.argmax()
     probabilities = classifier.getOutputData('categoryProbabilitiesOut')
     numCategories = classifier.getParameter('activeOutputCount')
-    classConfidences = dict(zip(xrange(numCategories), probabilities))
+    classConfidences = dict(list(zip(list(range(numCategories)), probabilities)))
 
     inference[InferenceElement.classification] = classification
     inference[InferenceElement.classConfidences] = {0: classConfidences}
@@ -875,7 +875,7 @@
 
       # calculate likelihood for each bucket
       bucketLikelihood = {}
-      for k in likelihoodsDict.keys():
+      for k in list(likelihoodsDict.keys()):
         bucketLikelihood[self._classifierInputEncoder.getBucketIndices(k)[0]] = (
                                                                 likelihoodsDict[k])
 
@@ -897,7 +897,7 @@
         # an offset from the current absolute value
         sumDelta = sum(predHistory)
         offsetDict = dict()
-        for (k, v) in likelihoodsDict.iteritems():
+        for (k, v) in likelihoodsDict.items():
           if k is not None:
             # Reconstruct the absolute value based on the current actual value,
             # the best predicted values from the previous iterations,
@@ -906,7 +906,7 @@
 
         # calculate likelihood for each bucket
         bucketLikelihoodOffset = {}
-        for k in offsetDict.keys():
+        for k in list(offsetDict.keys()):
           bucketLikelihoodOffset[self._classifierInputEncoder.getBucketIndices(k)[0]] = (
                                                                             offsetDict[k])
 
@@ -957,7 +957,7 @@
     minLikelihoodThreshold, but don't leave an empty dict.
     """
     maxVal = (None, None)
-    for (k, v) in likelihoodsDict.items():
+    for (k, v) in list(likelihoodsDict.items()):
       if len(likelihoodsDict) <= 1:
         break
       if maxVal[0] is None or v >= maxVal[1]:
@@ -967,7 +967,7 @@
       elif v < minLikelihoodThreshold:
         del likelihoodsDict[k]
     # Limit the number of predictions to include.
-    likelihoodsDict = dict(sorted(likelihoodsDict.iteritems(),
+    likelihoodsDict = dict(sorted(iter(likelihoodsDict.items()),
                                   key=itemgetter(1),
                                   reverse=True)[:maxPredictionsPerStep])
     return likelihoodsDict
@@ -1010,10 +1010,10 @@
       fieldNames = list(fieldNames) + addFieldNames
       fieldTypes = list(fieldTypes) + addFieldTypes
 
-    fieldMetaList = map(FieldMetaInfo._make,
-                        zip(fieldNames,
+    fieldMetaList = list(map(FieldMetaInfo._make,
+                        list(zip(fieldNames,
                             fieldTypes,
-                            itertools.repeat(FieldMetaSpecial.none)))
+                            itertools.repeat(FieldMetaSpecial.none)))))
 
     return tuple(fieldMetaList)
 
@@ -1104,7 +1104,7 @@
     sensor = n.regions['sensor'].getSelf()
 
     enabledEncoders = copy.deepcopy(sensorParams['encoders'])
-    for name, params in enabledEncoders.items():
+    for name, params in list(enabledEncoders.items()):
       if params is not None:
         classifierOnly = params.pop('classifierOnly', False)
         if classifierOnly:
@@ -1114,7 +1114,7 @@
     # SP or TM Regions. This is to handle the case where the predicted field
     # is not fed through the SP/TM. We typically just have one of these now.
     disabledEncoders = copy.deepcopy(sensorParams['encoders'])
-    for name, params in disabledEncoders.items():
+    for name, params in list(disabledEncoders.items()):
       if params is None:
         disabledEncoders.pop(name)
       else:
--- d:\nupic\src\python\python27\nupic\frameworks\opf\htm_prediction_model_callbacks.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\htm_prediction_model_callbacks.py	(refactored)
@@ -26,7 +26,7 @@
 import os
 
 from nupic.support.fs_helpers import makeDirectoryFromAbsolutePath
-from htm_prediction_model import HTMPredictionModel
+from .htm_prediction_model import HTMPredictionModel
 
 
 
--- d:\nupic\src\python\python27\nupic\frameworks\opf\htm_prediction_model_classifier_helper.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\htm_prediction_model_classifier_helper.py	(refactored)
@@ -33,7 +33,7 @@
     return obj_slot_values
 
   def __setstate__(self, data_dict):
-    for (name, value) in data_dict.iteritems():
+    for (name, value) in data_dict.items():
       setattr(self, name, value)
 
 
@@ -638,7 +638,7 @@
     if '_autoDetectThreshold' not in state:
       self._autoDetectThreshold = 1.1
 
-    for attr, value in state.iteritems():
+    for attr, value in state.items():
       setattr(self, attr, value)
 
     self._version = HTMPredictionModelClassifierHelper.__VERSION__
--- d:\nupic\src\python\python27\nupic\frameworks\opf\metrics.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\metrics.py	(refactored)
@@ -88,7 +88,7 @@
 
 from collections import deque
 from operator import itemgetter
-from safe_interpreter import SafeInterpreter
+from .safe_interpreter import SafeInterpreter
 from io import StringIO
 from functools import partial
 
@@ -159,7 +159,7 @@
     params = self.params
     if params is not None:
 
-      sortedParams= params.keys()
+      sortedParams= list(params.keys())
       sortedParams.sort()
       for param in sortedParams:
         # Don't include the customFuncSource - it is too long an unwieldy
@@ -294,7 +294,7 @@
     if len(self._countDict) == 0:
       pred = ""
     else:
-      pred = max(self._countDict.items(), key = itemgetter(1))[0]
+      pred = max(list(self._countDict.items()), key = itemgetter(1))[0]
 
     # Update count dict and history buffer
     self._history.appendleft(value)
@@ -317,7 +317,7 @@
 
 
 
-class MetricsIface(object):
+class MetricsIface(object, metaclass=ABCMeta):
   """
   A Metrics module compares a prediction Y to corresponding ground truth X and 
   returns a single measure representing the "goodness" of the prediction. It is 
@@ -325,8 +325,6 @@
 
   :param metricSpec: (:class:`MetricSpec`) spec used to created the metric
   """
-
-  __metaclass__ = ABCMeta
 
   @abstractmethod
   def __init__(self, metricSpec):
@@ -543,8 +541,8 @@
       return self.aggregateError
 
     if self.verbosity > 0:
-      print "groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth,
-                                                prediction, self.getMetric())
+      print("groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth,
+                                                prediction, self.getMetric()))
 
     # Ignore if we've reached maxRecords
     if self._maxRecords is not None and self.steps >= self._maxRecords:
@@ -585,9 +583,9 @@
       # a manually set minimum prediction probability so that the log(LL) doesn't blow up
       minProb = 0.00001
       negLL = 0
-      for step in bucketll.keys():
+      for step in list(bucketll.keys()):
         outOfBucketProb = 1 - sum(bucketll[step].values())
-        if bucketIdxTruth in bucketll[step].keys():
+        if bucketIdxTruth in list(bucketll[step].keys()):
           prob = bucketll[step][bucketIdxTruth]
         else:
           prob = outOfBucketProb
@@ -715,8 +713,8 @@
     # Compute absolute error
     error = abs(groundTruth - prediction)
     if self.verbosity > 0:
-      print "MetricAltMAPE:\n  groundTruth: %s\n  Prediction: " \
-            "%s\n  Error: %s" % (groundTruth, prediction, error)
+      print("MetricAltMAPE:\n  groundTruth: %s\n  Prediction: " \
+            "%s\n  Error: %s" % (groundTruth, prediction, error))
 
     # Update the accumulated groundTruth and aggregate error
     if self.history is not None:
@@ -737,9 +735,9 @@
       self.aggregateError = 0
 
     if self.verbosity >= 1:
-      print "  accumGT:", self._accumulatedGroundTruth
-      print "  accumError:", self._accumulatedError
-      print "  aggregateError:", self.aggregateError
+      print("  accumGT:", self._accumulatedGroundTruth)
+      print("  accumError:", self._accumulatedError)
+      print("  aggregateError:", self.aggregateError)
 
     self.steps += 1
     return self.aggregateError
@@ -776,13 +774,13 @@
     else:
       # Ignore this sample
       if self.verbosity > 0:
-        print "Ignoring sample with groundTruth of 0"
+        print("Ignoring sample with groundTruth of 0")
       self.steps += 1
       return self.aggregateError
 
     if self.verbosity > 0:
-      print "MetricMAPE:\n  groundTruth: %s\n  Prediction: " \
-            "%s\n  Error: %s" % (groundTruth, prediction, pctError)
+      print("MetricMAPE:\n  groundTruth: %s\n  Prediction: " \
+            "%s\n  Error: %s" % (groundTruth, prediction, pctError))
 
     # Update the accumulated groundTruth and aggregate error
     if self.history is not None:
@@ -797,8 +795,8 @@
     self.aggregateError = 100.0 * self._accumulatedPctError / len(self.history)
 
     if self.verbosity >= 1:
-      print "  accumPctError:", self._accumulatedPctError
-      print "  aggregateError:", self.aggregateError
+      print("  accumPctError:", self._accumulatedPctError)
+      print("  aggregateError:", self.aggregateError)
 
     self.steps += 1
     return self.aggregateError
@@ -861,7 +859,7 @@
     assert (len(self._predictionSteps) == 1)
 
     self.mean_window = 10
-    if metricSpec.params.has_key('mean_window'):
+    if 'mean_window' in metricSpec.params:
       assert metricSpec.params['mean_window'] >= 1
       self.mean_window = metricSpec.params['mean_window']
 
@@ -878,7 +876,7 @@
       return self._subErrorMetrics[0].aggregateError
 
     if self.verbosity > 0:
-      print "groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth, prediction, self.getMetric())
+      print("groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth, prediction, self.getMetric()))
 
     # Use ground truth from 'steps' steps ago as our most recent ground truth
     lastGT = self._getShiftedGroundTruth(groundTruth)
@@ -1016,12 +1014,12 @@
         likely outcome given a probability distribution
     """
     if len(pred) == 1:
-      return pred.keys()[0]
+      return list(pred.keys())[0]
 
     mostLikelyOutcome = None
     maxProbability = 0
 
-    for prediction, probability in pred.items():
+    for prediction, probability in list(pred.items()):
       if probability > maxProbability:
         mostLikelyOutcome = prediction
         maxProbability = probability
@@ -1033,9 +1031,9 @@
         value of a probability distribution
     """
     if len(pred) == 1:
-      return pred.keys()[0]
-
-    return sum([x*p for x,p in pred.items()])
+      return list(pred.keys())[0]
+
+    return sum([x*p for x,p in list(pred.items())])
 
   def evalAbsErr(self,pred,ground):
     return abs(pred-ground)
@@ -1081,7 +1079,7 @@
     super(MetricMovingMode, self).__init__(metricSpec)
 
     self.mode_window = 100
-    if metricSpec.params.has_key('mode_window'):
+    if 'mode_window' in metricSpec.params:
       assert metricSpec.params['mode_window'] >= 1
       self.mode_window = metricSpec.params['mode_window']
 
@@ -1101,8 +1099,8 @@
       return self._subErrorMetrics[0].aggregateError
 
     if self.verbosity > 0:
-      print "groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth, prediction,
-                                                          self.getMetric())
+      print("groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth, prediction,
+                                                          self.getMetric()))
 
     # Use ground truth from 'steps' steps ago as our most recent ground truth
     lastGT = self._getShiftedGroundTruth(groundTruth)
@@ -1147,8 +1145,8 @@
     prediction = self._getShiftedGroundTruth(groundTruth)
 
     if self.verbosity > 0:
-      print "groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth,
-                                            prediction, self.getMetric())
+      print("groundTruth:\n%s\nPredictions:\n%s\n%s\n" % (groundTruth,
+                                            prediction, self.getMetric()))
     # If missing data,
     if groundTruth == SENTINEL_VALUE_FOR_MISSING_DATA:
       return self._subErrorMetrics[0].aggregateError
@@ -1177,7 +1175,7 @@
     assert len(self._predictionSteps) == 1
 
     # Must supply the predictionField
-    assert(metricSpec.params.has_key('predictionField'))
+    assert('predictionField' in metricSpec.params)
     self.predictionField = metricSpec.params['predictionField']
     self.twoGramDict = dict()
 
@@ -1232,13 +1230,13 @@
       # Find most often occurring 1-gram
       if isinstance(actualGroundTruth,str):
         # Get the most frequent category that followed the previous timestep
-        twoGramMax = max(self.twoGramDict[prevGTKey].items(), key=itemgetter(1))
+        twoGramMax = max(list(self.twoGramDict[prevGTKey].items()), key=itemgetter(1))
         pred = twoGramMax[0]
 
       else:
         # Get average of all possible values that followed the previous
         # timestep
-        pred = sum(self.twoGramDict[prevGTKey].iterkeys())
+        pred = sum(self.twoGramDict[prevGTKey].keys())
         pred /= len(self.twoGramDict[prevGTKey])
 
       # Add current ground truth to dict
@@ -1248,8 +1246,8 @@
         self.twoGramDict[prevGTKey][actualGroundTruth] = 1
 
     if self.verbosity > 0:
-      print "\nencoding:%s\nactual:%s\nprevEncoding:%s\nprediction:%s\nmetric:%s" % \
-          (groundTruth, actualGroundTruth, prevGTKey, pred, self.getMetric())
+      print("\nencoding:%s\nactual:%s\nprevEncoding:%s\nprediction:%s\nmetric:%s" % \
+          (groundTruth, actualGroundTruth, prevGTKey, pred, self.getMetric()))
 
     return self._subErrorMetrics[0].addInstance(actualGroundTruth, pred, record)
 
@@ -1377,11 +1375,11 @@
     # Print warning the first time this metric is asked to be computed on a
     #  problem with more than 2 classes
     if sorted(classes) != [0,1]:
-      print "WARNING: AUC only implemented for binary classifications where " \
+      print("WARNING: AUC only implemented for binary classifications where " \
           "the categories are category 0 and 1. In this network, the " \
-          "categories are: %s" % (classes)
-      print "WARNING: Computation of this metric is disabled for the remainder of " \
-            "this experiment."
+          "categories are: %s" % (classes))
+      print("WARNING: Computation of this metric is disabled for the remainder of " \
+            "this experiment.")
       self.disabled = True
       return 0.0
 
@@ -1395,14 +1393,14 @@
 
     # Debug?
     if False:
-      print
-      print "AUC metric debug info (%d steps):" % (steps)
-      print " actuals:", actuals
-      print " probabilities:", ["%.2f" % x for x in scores]
-      print " fpr:", fpr
-      print " tpr:", tpr
-      print " thresholds:", thresholds
-      print " AUC:", auc
+      print()
+      print("AUC metric debug info (%d steps):" % (steps))
+      print(" actuals:", actuals)
+      print(" probabilities:", ["%.2f" % x for x in scores])
+      print(" fpr:", fpr)
+      print(" tpr:", tpr)
+      print(" thresholds:", thresholds)
+      print(" AUC:", auc)
 
     return -1 * auc
 
@@ -1455,15 +1453,15 @@
         if isinstance(stepPrediction, dict) \
             and not isinstance(subErrorMetric, CustomErrorMetric):
           predictions = [(prob,value) for (value, prob) in \
-                                                    stepPrediction.iteritems()]
+                                                    stepPrediction.items()]
           predictions.sort()
           stepPrediction = predictions[-1][1]
 
         # Get sum of the errors
         aggErr = subErrorMetric.addInstance(groundTruth, stepPrediction, record, result)
         if self.verbosity >= 2:
-          print "MetricMultiStep %s: aggErr for stepSize %d: %s" % \
-                  (self._predictionSteps, step, aggErr)
+          print("MetricMultiStep %s: aggErr for stepSize %d: %s" % \
+                  (self._predictionSteps, step, aggErr))
 
         aggErrSum += aggErr
     except:
@@ -1473,14 +1471,14 @@
     # Return average aggregate error across all step sizes
     self.aggregateError = aggErrSum / len(self._subErrorMetrics)
     if self.verbosity >= 2:
-      print "MetricMultiStep %s: aggErrAvg: %s" % (self._predictionSteps,
-                                                   self.aggregateError)
+      print("MetricMultiStep %s: aggErrAvg: %s" % (self._predictionSteps,
+                                                   self.aggregateError))
     self.steps += 1
 
     if self.verbosity >= 1:
-      print "\nMetricMultiStep %s: \n  groundTruth:  %s\n  Predictions:  %s" \
+      print("\nMetricMultiStep %s: \n  groundTruth:  %s\n  Predictions:  %s" \
             "\n  Metric:  %s" % (self._predictionSteps, groundTruth, prediction,
-                                 self.getMetric())
+                                 self.getMetric()))
 
     return self.aggregateError
 
@@ -1536,9 +1534,9 @@
       return self.aggregateError
 
     if self.verbosity >= 1:
-      print "\nMetricMultiStepProbability %s: \n  groundTruth:  %s\n  " \
+      print("\nMetricMultiStepProbability %s: \n  groundTruth:  %s\n  " \
             "Predictions:  %s" % (self._predictionSteps, groundTruth,
-                                  prediction)
+                                  prediction))
 
     # Get the aggregateErrors for all requested step sizes and average them
     aggErrSum = 0
@@ -1552,7 +1550,7 @@
       if isinstance(stepPrediction, dict):
         expectedValue = 0
         # For every possible prediction multiply its error by its probability
-        for (pred, prob) in stepPrediction.iteritems():
+        for (pred, prob) in stepPrediction.items():
           error += subErrorMetric.addInstance(groundTruth, pred, record) \
                     * prob
       else:
@@ -1560,8 +1558,8 @@
                                             record)
 
       if self.verbosity >= 2:
-          print ("MetricMultiStepProbability %s: aggErr for stepSize %d: %s" %
-                 (self._predictionSteps, step, error))
+          print(("MetricMultiStepProbability %s: aggErr for stepSize %d: %s" %
+                 (self._predictionSteps, step, error)))
 
       aggErrSum += error
 
@@ -1570,15 +1568,15 @@
     avgAggErr = aggErrSum / len(self._subErrorMetrics)
     self.aggregateError = self._movingAverage(avgAggErr)
     if self.verbosity >= 2:
-      print ("MetricMultiStepProbability %s: aggErr over all steps, this "
-             "iteration (%d): %s" % (self._predictionSteps, self.steps, avgAggErr))
-      print ("MetricMultiStepProbability %s: aggErr moving avg: %s" %
-             (self._predictionSteps, self.aggregateError))
+      print(("MetricMultiStepProbability %s: aggErr over all steps, this "
+             "iteration (%d): %s" % (self._predictionSteps, self.steps, avgAggErr)))
+      print(("MetricMultiStepProbability %s: aggErr moving avg: %s" %
+             (self._predictionSteps, self.aggregateError)))
     self.steps += 1
 
     if self.verbosity >= 1:
-      print "MetricMultiStepProbability %s: \n  Error: %s\n  Metric:  %s" % \
-              (self._predictionSteps, avgAggErr, self.getMetric())
+      print("MetricMultiStepProbability %s: \n  Error: %s\n  Metric:  %s" % \
+              (self._predictionSteps, avgAggErr, self.getMetric()))
 
     return self.aggregateError
 
@@ -1621,7 +1619,7 @@
   def addInstance(self, groundTruth, prediction, record = None, result = None):
     err = 0.0
     subResults = [m.addInstance(groundTruth, prediction, record) for m in self.metrics]
-    for i in xrange(len(self.weights)):
+    for i in range(len(self.weights)):
       if subResults[i] is not None:
         err += subResults[i]*self.weights[i]
       else: # submetric returned None, propagate
@@ -1629,7 +1627,7 @@
         return None
 
     if self.verbosity > 2:
-      print "IN=",groundTruth," pred=",prediction,": w=",self.weights[i]," metric=",self.metrics[i]," value=",m," err=",err
+      print("IN=",groundTruth," pred=",prediction,": w=",self.weights[i]," metric=",self.metrics[i]," value=",m," err=",err)
     if self.movingAvg is not None:
       err=self.movingAvg(err)
     self.err = err
--- d:\nupic\src\python\python27\nupic\frameworks\opf\model.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\model.py	(refactored)
@@ -21,7 +21,7 @@
 
 """Module defining the OPF Model base class."""
 
-import cPickle as pickle
+import pickle as pickle
 import json
 import os
 import shutil
@@ -32,7 +32,7 @@
 from nupic.serializable import Serializable
 
 
-class Model(Serializable):
+class Model(Serializable, metaclass=ABCMeta):
   """ This is the base class that all OPF Model implementations should
   subclass.
 
@@ -42,8 +42,6 @@
   :param inferenceType: (:class:`~nupic.frameworks.opf.opf_utils.InferenceType`)
          A value that specifies the type of inference.
   """
-
-  __metaclass__ = ABCMeta
 
   def __init__(self, inferenceType=None, proto=None):
     """
--- d:\nupic\src\python\python27\nupic\frameworks\opf\model_factory.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\model_factory.py	(refactored)
@@ -29,10 +29,10 @@
 import nupic.frameworks.opf.opf_utils as opf_utils
 
 # Import models
-from htm_prediction_model import HTMPredictionModel
-from model import Model
-from two_gram_model import TwoGramModel
-from previous_value_model import PreviousValueModel
+from .htm_prediction_model import HTMPredictionModel
+from .model import Model
+from .two_gram_model import TwoGramModel
+from .previous_value_model import PreviousValueModel
 
 class ModelFactory(object):
   """
--- d:\nupic\src\python\python27\nupic\frameworks\opf\opf_basic_environment.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\opf_basic_environment.py	(refactored)
@@ -43,24 +43,23 @@
 import logging.handlers
 import os
 import shutil
-import StringIO
-
-import opf_utils
-import opf_environment as opfenv
+import io
+
+from . import opf_utils
+from . import opf_environment as opfenv
 from nupic.data.file_record_stream import FileRecordStream
 from nupic.data.stream_reader import StreamReader
 from nupic.data.field_meta import (FieldMetaInfo,
                                    FieldMetaType,
                                    FieldMetaSpecial)
 from nupic.data.inference_shifter import InferenceShifter
-from opf_utils import InferenceType, InferenceElement
-
-
-
-class PredictionMetricsLoggerIface(object):
+from .opf_utils import InferenceType, InferenceElement
+
+
+
+class PredictionMetricsLoggerIface(object, metaclass=ABCMeta):
   """ This is the interface for output of prediction metrics.
   """
-  __metaclass__ = ABCMeta
 
 
   @abstractmethod
@@ -86,10 +85,9 @@
 
 
 
-class DatasetReaderIface(object):
+class DatasetReaderIface(object, metaclass=ABCMeta):
   """ This is the interface class for a dataset readers
   """
-  __metaclass__ = ABCMeta
 
 
   @abstractmethod
@@ -116,11 +114,10 @@
 
 
 
-class PredictionWriterIface(object):
+class PredictionWriterIface(object, metaclass=ABCMeta):
   """ This class defines the interface for prediction writer implementation
   returned by an object factory conforming to PredictionWriterFactoryIface
   """
-  __metaclass__ = ABCMeta
 
 
   @abstractmethod
@@ -238,9 +235,9 @@
 
 
   def _emitJSONStringToStdout(self, jsonString):
-    print '<JSON>'
-    print jsonString
-    print '</JSON>'
+    print('<JSON>')
+    print(jsonString)
+    print('</JSON>')
 
 
 
@@ -260,7 +257,7 @@
     return self
 
 
-  def next(self):
+  def __next__(self):
     row = self._reader.getNextRecordDict()
     if row == None:
       raise StopIteration
@@ -337,7 +334,7 @@
     self.__checkpointCache = None
     if checkpointSource is not None:
       checkpointSource.seek(0)
-      self.__checkpointCache = StringIO.StringIO()
+      self.__checkpointCache = io.StringIO()
       shutil.copyfileobj(checkpointSource, self.__checkpointCache)
 
     return
@@ -358,7 +355,7 @@
     # -----------------------------------------------------------------------
     # Write each of the raw inputs that go into the encoders
     rawInput = modelResult.rawInput
-    rawFields = rawInput.keys()
+    rawFields = list(rawInput.keys())
     rawFields.sort()
     for field in rawFields:
       if field.startswith('_') or field == 'reset':
@@ -372,7 +369,7 @@
 
     # -----------------------------------------------------------------------
     # Handle each of the inference elements
-    for inferenceElement, value in modelResult.inferences.iteritems():
+    for inferenceElement, value in modelResult.inferences.items():
       inferenceLabel = InferenceElement.getLabel(inferenceElement)
 
       # TODO: Right now we assume list inferences are associated with
@@ -411,8 +408,8 @@
     self.__datasetPath = os.path.join(inferenceDir, filename)
 
     # Create the output dataset
-    print "OPENING OUTPUT FOR PREDICTION WRITER AT: %r" % self.__datasetPath
-    print "Prediction field-meta: %r" % ([tuple(i) for i in self.__outputFieldsMeta],)
+    print("OPENING OUTPUT FOR PREDICTION WRITER AT: %r" % self.__datasetPath)
+    print("Prediction field-meta: %r" % ([tuple(i) for i in self.__outputFieldsMeta],))
     self.__dataset = FileRecordStream(streamID=self.__datasetPath, write=True,
                                      fields=self.__outputFieldsMeta)
 
@@ -424,9 +421,9 @@
 
       # Skip header row
       try:
-        header = reader.next()
+        header = next(reader)
       except StopIteration:
-        print "Empty record checkpoint initializer for %r" % (self.__datasetPath,)
+        print("Empty record checkpoint initializer for %r" % (self.__datasetPath,))
       else:
         assert tuple(self.__dataset.getFieldNames()) == tuple(header), \
           "dataset.getFieldNames(): %r; predictionCheckpointFieldNames: %r" % (
@@ -436,7 +433,7 @@
       numRowsCopied = 0
       while True:
         try:
-          row = reader.next()
+          row = next(reader)
         except StopIteration:
           break
 
@@ -447,8 +444,8 @@
 
       self.__dataset.flush()
 
-      print "Restored %d rows from checkpoint for %r" % (
-        numRowsCopied, self.__datasetPath)
+      print("Restored %d rows from checkpoint for %r" % (
+        numRowsCopied, self.__datasetPath))
 
       # Dispose of our checkpoint cache
       self.__checkpointCache.close()
@@ -544,7 +541,7 @@
     inferences = modelResult.inferences
     hasInferences = False
     if inferences is not None:
-      for value in inferences.itervalues():
+      for value in inferences.values():
         hasInferences = hasInferences or (value is not None)
 
     if not hasInferences:
@@ -567,7 +564,7 @@
 
     # -----------------------------------------------------------------------
     # Write out the inference element info
-    for inferenceElement, outputVal in inferences.iteritems():
+    for inferenceElement, outputVal in inferences.items():
       inputElement = InferenceElement.getInputElement(inferenceElement)
       if inputElement:
         inputVal = getattr(inputData, inputElement)
@@ -655,8 +652,8 @@
 
     # Skip initial rows to get to the rows that we actually need to checkpoint
     numRowsToSkip = totalDataRows - numToWrite
-    for i in xrange(numRowsToSkip):
-      reader.next()
+    for i in range(numRowsToSkip):
+      next(reader)
 
     # Write the data rows to checkpoint sink
     numWritten = 0
@@ -800,7 +797,7 @@
     self.__checkpointCache = None
     if checkpointSource is not None:
       checkpointSource.seek(0)
-      self.__checkpointCache = StringIO.StringIO()
+      self.__checkpointCache = io.StringIO()
       shutil.copyfileobj(checkpointSource, self.__checkpointCache)
 
     return
--- d:\nupic\src\python\python27\nupic\frameworks\opf\opf_environment.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\opf_environment.py	(refactored)
@@ -36,11 +36,10 @@
 
 
 
-class PredictionLoggerIface(object):
+class PredictionLoggerIface(object, metaclass=ABCMeta):
   """
   This class defines the interface for OPF prediction logger implementations.
   """
-  __metaclass__ = ABCMeta
 
   @abstractmethod
   def close(self):
--- d:\nupic\src\python\python27\nupic\frameworks\opf\opf_task_driver.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\opf_task_driver.py	(refactored)
@@ -56,7 +56,7 @@
 import itertools
 import logging
 
-from prediction_metrics_manager import (
+from .prediction_metrics_manager import (
   MetricsManager,
   )
 
@@ -343,8 +343,7 @@
     self.__model = model
 
     # Instantiate Iteration Phase drivers
-    self.__phases = tuple(map(lambda x: x._getImpl(model=model),
-                              phaseSpecs))
+    self.__phases = tuple([x._getImpl(model=model) for x in phaseSpecs])
 
     # Init phase-management structures
     if self.__phases:
@@ -363,7 +362,7 @@
   def __advancePhase(self):
     """ Advance to the next iteration cycle phase
     """
-    self.__currentPhase = self.__phaseCycler.next()
+    self.__currentPhase = next(self.__phaseCycler)
     self.__currentPhase.enterPhase()
 
     return
@@ -396,11 +395,9 @@
 
 
 
-class _IterationPhase(object):
+class _IterationPhase(object, metaclass=ABCMeta):
   """ Interface for IterationPhaseXXXXX classes
   """
-
-  __metaclass__ = ABCMeta
 
   def __init__(self, nIters):
     """
@@ -419,10 +416,10 @@
     be called before handleInputRecord() at the beginning of each phase
     """
 
-    self.__iter = iter(xrange(self.__nIters))
+    self.__iter = iter(range(self.__nIters))
 
     # Prime the iterator
-    self.__iter.next()
+    next(self.__iter)
 
 
   def advance(self):
@@ -433,7 +430,7 @@
     """
     hasMore = True
     try:
-      self.__iter.next()
+      next(self.__iter)
     except StopIteration:
       self.__iter = None
       hasMore = False
--- d:\nupic\src\python\python27\nupic\frameworks\opf\opf_utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\opf_utils.py	(refactored)
@@ -141,9 +141,9 @@
              in the inference dictionary.
     """
     maxDelay = 0
-    for inferenceElement, inference in inferences.iteritems():
+    for inferenceElement, inference in inferences.items():
       if isinstance(inference, dict):
-        for key in inference.iterkeys():
+        for key in inference.keys():
           maxDelay = max(InferenceElement.getTemporalDelay(inferenceElement,
                                                             key),
                          maxDelay)
--- d:\nupic\src\python\python27\nupic\frameworks\opf\periodic.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\periodic.py	(refactored)
@@ -86,7 +86,7 @@
       except StopIteration:
         act.cb()
         if act.repeating:
-          act.iteratorHolder[0] = iter(xrange(act.period-1))
+          act.iteratorHolder[0] = iter(range(act.period-1))
         else:
           act.iteratorHolder[0] = None
 
@@ -102,7 +102,7 @@
       act =   self.Activity(repeating=req.repeating,
                             period=req.period,
                             cb=req.cb,
-                            iteratorHolder=[iter(xrange(req.period-1))])
+                            iteratorHolder=[iter(range(req.period-1))])
       self.__activities.append(act)
 
     return
--- d:\nupic\src\python\python27\nupic\frameworks\opf\prediction_metrics_manager.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\prediction_metrics_manager.py	(refactored)
@@ -34,7 +34,7 @@
 from nupic.data.inference_shifter import InferenceShifter
 from nupic.frameworks.opf import metrics
 
-from opf_utils import InferenceType, InferenceElement
+from .opf_utils import InferenceType, InferenceElement
 
 
 
@@ -306,7 +306,7 @@
 
 
 def _testMetricsMgr():
-  print "*Testing Metrics Managers*..."
+  print("*Testing Metrics Managers*...")
   from nupic.data.field_meta import (
     FieldMetaInfo,
     FieldMetaType,
@@ -317,7 +317,7 @@
   onlineMetrics = (MetricSpec(metric="aae", inferenceElement='', \
                               field="consumption", params={}),)
 
-  print "TESTING METRICS MANAGER (BASIC PLUMBING TEST)..."
+  print("TESTING METRICS MANAGER (BASIC PLUMBING TEST)...")
 
   modelFieldMetaInfo = (
     FieldMetaInfo(name='temperature',
@@ -336,10 +336,10 @@
     fieldInfo=modelFieldMetaInfo,
     inferenceType=InferenceType.TemporalNextStep)
   except ValueError:
-    print "Caught bad inference element: PASS"
-
-
-  print
+    print("Caught bad inference element: PASS")
+
+
+  print()
   onlineMetrics = (MetricSpec(metric="aae",
                               inferenceElement=InferenceElement.prediction,
                               field="consumption", params={}),)
@@ -398,10 +398,10 @@
 
     temporalMetrics.update(result)
 
-  assert temporalMetrics.getMetrics().values()[0] == 15.0 / 3.0, \
+  assert list(temporalMetrics.getMetrics().values())[0] == 15.0 / 3.0, \
           "Expected %f, got %f" %(15.0/3.0,
-                                  temporalMetrics.getMetrics().values()[0])
-  print "ok"
+                                  list(temporalMetrics.getMetrics().values())[0])
+  print("ok")
 
   return
 
@@ -411,7 +411,7 @@
   """ Test to see if the metrics manager correctly shifts records for multistep
   prediction cases
   """
-  print "*Testing Multistep temporal shift*..."
+  print("*Testing Multistep temporal shift*...")
   from nupic.data.field_meta import (
     FieldMetaInfo,
     FieldMetaType,
@@ -431,10 +431,10 @@
                        inferenceType=InferenceType.TemporalMultiStep)
 
   groundTruths = [{'consumption':i} for i in range(10)]
-  oneStepInfs = reversed(range(10))
-  threeStepInfs = range(5, 15)
-
-  for iterNum, gt, os, ts in zip(xrange(10), groundTruths,
+  oneStepInfs = reversed(list(range(10)))
+  threeStepInfs = list(range(5, 15))
+
+  for iterNum, gt, os, ts in zip(range(10), groundTruths,
                               oneStepInfs, threeStepInfs):
     inferences = {InferenceElement.multiStepPredictions:{1: os, 3: ts}}
     sensorInput = SensorInput(dataDict = [gt])
@@ -459,7 +459,7 @@
 
 
 def _testMetricLabels():
-  print "\n*Testing Metric Label Generation*..."
+  print("\n*Testing Metric Label Generation*...")
 
   from nupic.frameworks.opf.metrics import MetricSpec
 
@@ -489,11 +489,11 @@
     try:
       assert test[0].getLabel() == test[1]
     except:
-      print "Failed Creating label"
-      print "Expected %s \t Got %s" % (test[1], test[0].getLabel())
+      print("Failed Creating label")
+      print("Expected %s \t Got %s" % (test[1], test[0].getLabel()))
       return
 
-  print "ok"
+  print("ok")
 
 
 
--- d:\nupic\src\python\python27\nupic\frameworks\opf\previous_value_model.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\previous_value_model.py	(refactored)
@@ -26,7 +26,7 @@
 from nupic.data import field_meta
 from nupic.frameworks.opf import model
 from nupic.frameworks.opf import opf_utils
-from opf_utils import InferenceType
+from .opf_utils import InferenceType
 
 try:
   import capnp
@@ -114,7 +114,7 @@
 
   def getFieldInfo(self):
     return tuple(field_meta.FieldMetaInfo(*args) for args in
-                 itertools.izip(
+                 zip(
                      self._fieldNames, self._fieldTypes,
                      itertools.repeat(field_meta.FieldMetaSpecial.none)))
 
--- d:\nupic\src\python\python27\nupic\frameworks\opf\two_gram_model.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\two_gram_model.py	(refactored)
@@ -28,7 +28,7 @@
 from nupic.data import field_meta
 from nupic.frameworks.opf import model
 from nupic.frameworks.opf import opf_utils
-from opf_utils import InferenceType
+from .opf_utils import InferenceType
 
 try:
   import capnp
@@ -60,7 +60,7 @@
     self._encoder = encoders.MultiEncoder(encoderParams)
     self._fieldNames = self._encoder.getScalarNames()
     self._prevValues = [None] * len(self._fieldNames)
-    self._twoGramDicts = [dict() for _ in xrange(len(self._fieldNames))]
+    self._twoGramDicts = [dict() for _ in range(len(self._fieldNames))]
 
   def run(self, inputRecord):
     results = super(TwoGramModel, self).run(inputRecord)
@@ -77,11 +77,11 @@
 
     # Keep track of the last value associated with each encoded value for that
     # predictions can be returned in the original value format.
-    for value, bucket in itertools.izip(values, inputBuckets):
+    for value, bucket in zip(values, inputBuckets):
       self._hashToValueDict[bucket] = value
 
     # Update the two-gram dict if learning is enabled.
-    for bucket, prevValue, twoGramDict in itertools.izip(
+    for bucket, prevValue, twoGramDict in zip(
         inputBuckets, self._prevValues, self._twoGramDicts):
       if self._learningEnabled and not self._reset:
         if prevValue not in twoGramDict:
@@ -93,10 +93,10 @@
     predictions = []
     encodedPredictions = []
     for bucket, twoGramDict, default, fieldName in (
-        itertools.izip(inputBuckets, self._twoGramDicts, defaults,
+        zip(inputBuckets, self._twoGramDicts, defaults,
                        self._fieldNames)):
       if bucket in twoGramDict:
-        probabilities = twoGramDict[bucket].items()
+        probabilities = list(twoGramDict[bucket].items())
         prediction = self._hashToValueDict[
             max(probabilities, key=lambda x: x[1])[0]]
         predictions.append(prediction)
@@ -128,7 +128,7 @@
     assert len(self._fieldNames) == len(fieldTypes)
 
     return tuple(field_meta.FieldMetaInfo(*args) for args in
-                 itertools.izip(
+                 zip(
                      self._fieldNames, fieldTypes,
                      itertools.repeat(field_meta.FieldMetaSpecial.none)))
 
@@ -165,7 +165,7 @@
     instance._encoder = encoders.MultiEncoder.read(proto.encoder)
     instance._fieldNames = instance._encoder.getScalarNames()
     instance._prevValues = list(proto.prevValues)
-    instance._twoGramDicts = [dict() for _ in xrange(len(proto.twoGramDicts))]
+    instance._twoGramDicts = [dict() for _ in range(len(proto.twoGramDicts))]
     for idx, field in enumerate(proto.twoGramDicts):
       for entry in field:
         prev = None if entry.value == -1 else entry.value
@@ -187,14 +187,14 @@
     proto.prevValues = self._prevValues
     self._encoder.write(proto.encoder)
     proto.hashToValueDict = [{"hash": h, "value": v}
-                             for h, v in self._hashToValueDict.items()]
+                             for h, v in list(self._hashToValueDict.items())]
 
     twoGramDicts = []
     for items in self._twoGramDicts:
       twoGramArr = []
-      for prev, values in items.iteritems():
+      for prev, values in items.items():
         buckets = [{"index": index, "count": count}
-                   for index, count in values.iteritems()]
+                   for index, count in values.items()]
         if prev is None:
           prev = -1
         twoGramArr.append({"value": prev, "buckets": buckets})
--- d:\nupic\src\python\python27\nupic\frameworks\opf\common_models\cluster_params.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\opf\common_models\cluster_params.py	(refactored)
@@ -139,7 +139,7 @@
     params["modelConfig"]["modelParams"]["sensorParams"]["encoders"]
   )
 
-  for encoder in encodersDict.itervalues():
+  for encoder in encodersDict.values():
     if encoder is not None:
       if encoder["type"] == "RandomDistributedScalarEncoder":
         resolution = max(minResolution,
--- d:\nupic\src\python\python27\nupic\frameworks\viz\dot_renderer.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\viz\dot_renderer.py	(refactored)
@@ -44,8 +44,8 @@
 
   def render(self, graph):
 
-    self.outp.write(u"digraph structs {\n")
-    self.outp.write(u'rankdir = "LR";\n')
+    self.outp.write("digraph structs {\n")
+    self.outp.write('rankdir = "LR";\n')
 
 
     lookup = {}
@@ -56,10 +56,10 @@
       lookup.setdefault(edge[0], {"inputs": set(), "outputs": set()})
       lookup.setdefault(edge[1], {"inputs": set(), "outputs": set()})
 
-      for labels in data.values():
+      for labels in list(data.values()):
         lookup[edge[0]]["outputs"].add(labels["src"])
         lookup[edge[1]]["inputs"].add(labels["dest"])
-        self.outp.write(u'"{}":{} -> "{}":{};\n'.format(edge[0],
+        self.outp.write('"{}":{} -> "{}":{};\n'.format(edge[0],
                                                         labels["src"],
                                                         edge[1],
                                                         labels["dest"]))
@@ -67,10 +67,10 @@
     def _renderPorts(ports):
       return "{" + "|".join("<{}>{}".format(port, port) for port in ports) + "}"
 
-    for node, ports in lookup.items():
+    for node, ports in list(lookup.items()):
       def _renderNode():
         nodeAttrs = ",".join("{}={}".format(key, value)
-                             for key, value in self.node_attrs.items())
+                             for key, value in list(self.node_attrs.items()))
         nodeAttrs += "," if nodeAttrs else ""
 
         return ('{} [{}label="{}"];\n'
@@ -80,6 +80,6 @@
                                         node,
                                         _renderPorts(ports["outputs"])]) + "}"))
 
-      self.outp.write(unicode(_renderNode()))
+      self.outp.write(str(_renderNode()))
 
-    self.outp.write(u"}\n")
+    self.outp.write("}\n")
--- d:\nupic\src\python\python27\nupic\frameworks\viz\network_visualization.py	(original)
+++ d:\nupic\src\python\python27\nupic\frameworks\viz\network_visualization.py	(refactored)
@@ -62,7 +62,7 @@
     # Add regions to graph as nodes, annotated by name
     regions = self.network.getRegions()
 
-    for idx in xrange(regions.getCount()):
+    for idx in range(regions.getCount()):
       regionPair = regions.getByIndex(idx)
       regionName = regionPair[0]
       graph.add_node(regionName, label=regionName)
--- d:\nupic\src\python\python27\nupic\math\cross.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\cross.py	(refactored)
@@ -36,17 +36,17 @@
   From: http://book.opensourceproject.org.cn/lamp/python/pythoncook2/opensource/0596007973/pythoncook2-chp-19-sect-9.html
   """
   # visualize an odometer, with "wheels" displaying "digits"...:
-  wheels = map(iter, sequences)
-  digits = [it.next( ) for it in wheels]
+  wheels = list(map(iter, sequences))
+  digits = [next(it) for it in wheels]
   while True:
     yield tuple(digits)
     for i in range(len(digits)-1, -1, -1):
       try:
-        digits[i] = wheels[i].next( )
+        digits[i] = next(wheels[i])
         break
       except StopIteration:
         wheels[i] = iter(sequences[i])
-        digits[i] = wheels[i].next( )
+        digits[i] = next(wheels[i])
     else:
       break
 
@@ -54,38 +54,38 @@
   """
   Similar to cross(), but generates output dictionaries instead of tuples.
   """
-  keys = keywords.keys()
+  keys = list(keywords.keys())
   # Could use keywords.values(), but unsure whether the order
   # the values come out in is guaranteed to be the same as that of keys
   # (appears to be anecdotally true).
   sequences = [keywords[key] for key in keys]
 
-  wheels = map(iter, sequences)
-  digits = [it.next( ) for it in wheels]
+  wheels = list(map(iter, sequences))
+  digits = [next(it) for it in wheels]
   while True:
-    yield dict(zip(keys, digits))
+    yield dict(list(zip(keys, digits)))
     for i in range(len(digits)-1, -1, -1):
       try:
-        digits[i] = wheels[i].next( )
+        digits[i] = next(wheels[i])
         break
       except StopIteration:
         wheels[i] = iter(sequences[i])
-        digits[i] = wheels[i].next( )
+        digits[i] = next(wheels[i])
     else:
       break
 
 def combinations(n, c):
   m = n - 1
-  positions = range(c)
+  positions = list(range(c))
   while True:
     yield tuple(positions)
     success = False
     lastPi = m
-    for i in xrange(c-1, -1, -1):
+    for i in range(c-1, -1, -1):
       pi = positions[i]
       if pi < lastPi:
         pi += 1
-        for j in xrange(i, c):
+        for j in range(i, c):
           positions[j] = pi
           pi += 1
         success = True
@@ -97,6 +97,6 @@
   if len(x) > 1:
     for permutation in permutations(x[1:]):
       # Stick the first digit in every position.
-      for i in xrange(len(permutation)+1):
+      for i in range(len(permutation)+1):
         yield permutation[:i] + x[0:1] + permutation[i:]
   else: yield x
--- d:\nupic\src\python\python27\nupic\math\dist.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\dist.py	(refactored)
@@ -55,14 +55,14 @@
     # Use iteritems() repeatedly here (instead of iterkeys()) to
     # make sure the order is preserved.
     n = len(pmfIn)
-    keyMap = dict((item[0], i) for i, item in enumerate(pmfIn.iteritems()))
-    keys = [item[0] for item in pmfIn.iteritems()]
-    pmf = numpy.array([item[1] for item in pmfIn.iteritems()], dtype=float)
+    keyMap = dict((item[0], i) for i, item in enumerate(pmfIn.items()))
+    keys = [item[0] for item in pmfIn.items()]
+    pmf = numpy.array([item[1] for item in pmfIn.items()], dtype=float)
     if normalize:
       pmf *= (1.0 / pmf.sum())
     # Is there a faster way to accumulate?
     cdf = pmf.copy()
-    for i in xrange(1, n): cdf[i] += cdf[i-1]
+    for i in range(1, n): cdf[i] += cdf[i-1]
     self.keys = keys
     self.keyMap = keyMap
     self.pmf = pmf
@@ -150,7 +150,7 @@
     else: # Inefficient sum.
       if x != int(x): raise RuntimeError("Invalid value.")
       c = 0.0
-      for i in xrange(x+1):
+      for i in range(x+1):
         c += self.probability(i)
       return c
 
--- d:\nupic\src\python\python27\nupic\math\logarithms.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\logarithms.py	(refactored)
@@ -93,12 +93,12 @@
         pass
       else:
         import sys
-        print >>sys.stderr, "Warning: scaled log sum down axis 0 did not match."
-        print >>sys.stderr, "Scaled log result:"
-        print >>sys.stderr, result
-        print >>sys.stderr, "Conventional result:"
-        print >>sys.stderr, conventional
-  except FloatingPointError, e:
+        print("Warning: scaled log sum down axis 0 did not match.", file=sys.stderr)
+        print("Scaled log result:", file=sys.stderr)
+        print(result, file=sys.stderr)
+        print("Conventional result:", file=sys.stderr)
+        print(conventional, file=sys.stderr)
+  except FloatingPointError as e:
     # print "Scaled log sum down axis 0 avoided underflow or overflow."
     pass
 
--- d:\nupic\src\python\python27\nupic\math\mvn.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\mvn.py	(refactored)
@@ -36,7 +36,7 @@
   if prior is not None:
     assert len(prior) == 2, "Must be of the form (n, SS)"
     n += prior[0]
-    print prior[1].shape
+    print(prior[1].shape)
     xxt += prior[1]
   n += dfOffset
   assert n > 0
--- d:\nupic\src\python\python27\nupic\math\proposal.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\proposal.py	(refactored)
@@ -167,8 +167,8 @@
     if willAdaptForward or willAdaptBackward:
       if not "obsOdds" in locals():
         obsOdds = logit(self.accepted.get())
-      print " Adapting:", obsOdds, "->", targetOdds
-      print "  Adapted:", self.kernel, kernelForward, kernelBackward
+      print(" Adapting:", obsOdds, "->", targetOdds)
+      print("  Adapted:", self.kernel, kernelForward, kernelBackward)
       self.kernel = kernelForward
 
     # Perform the proposal.
--- d:\nupic\src\python\python27\nupic\math\roc_utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\roc_utils.py	(refactored)
@@ -187,8 +187,8 @@
 def _printNPArray(x, precision=2):
   format = "%%.%df" % (precision)
   for elem in x:
-    print format % (elem),
-  print
+    print(format % (elem), end=' ')
+  print()
 
 
 
@@ -250,26 +250,26 @@
   yScore = np.array([0.1, 0.4, 0.5, 0.3, 0.45])
   (fpr, tpr, thresholds) = ROCCurve(yTrue, yScore)
 
-  print "Actual:    ",
+  print("Actual:    ", end=' ')
   _printNPArray(yTrue)
 
-  print "Predicted: ",
+  print("Predicted: ", end=' ')
   _printNPArray(yScore)
-  print
-
-  print "Thresholds:",
+  print()
+
+  print("Thresholds:", end=' ')
   _printNPArray(thresholds[::-1])
 
-  print "FPR(x):    ",
+  print("FPR(x):    ", end=' ')
   _printNPArray(fpr)
 
-  print "TPR(y):    ",
+  print("TPR(y):    ", end=' ')
   _printNPArray(tpr)
 
 
-  print
+  print()
   area = AreaUnderCurve(fpr, tpr)
-  print "AUC: ", area
+  print("AUC: ", area)
 
 
 
--- d:\nupic\src\python\python27\nupic\math\stats.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\stats.py	(refactored)
@@ -29,6 +29,7 @@
 import numpy
 
 from nupic.bindings.math import GetNTAReal, SparseMatrix
+from functools import reduce
 
 
 dtype = GetNTAReal()
@@ -332,12 +333,12 @@
     has been called since the last clean_outcpd().
     """
     m = self.hist_.toDense()
-    for j in xrange(m.shape[1]): # For each column.
+    for j in range(m.shape[1]): # For each column.
       cmax = m[:,j].max()
       if cmax:
         m[:,j] = numpy.array(m[:,j] == cmax, dtype=dtype)
     self.hack_ = SparseMatrix(0, self.hist_.nCols())
-    for i in xrange(m.shape[0]):
+    for i in range(m.shape[0]):
       self.hack_.addRow(m[i,:])
 
 def ShannonEntropy(x):
@@ -373,9 +374,9 @@
        dMeans = [min(samples)+0.01 , fixCenter ,max(samples)-0.01]
     else:
        dMeans = [min(samples)+0.01 , mean(samples) ,max(samples)-0.01]
-    begDeg = map(None,numpy.zeros(len(samples)))
-    midDeg = map(None,numpy.zeros(len(samples)))
-    endDeg = map(None,numpy.zeros(len(samples)))
+    begDeg = list(numpy.zeros(len(samples)))
+    midDeg = list(numpy.zeros(len(samples)))
+    endDeg = list(numpy.zeros(len(samples)))
 
     for j in range(iter):
        for k in range(len(samples)):
--- d:\nupic\src\python\python27\nupic\math\topology.py	(original)
+++ d:\nupic\src\python\python27\nupic\math\topology.py	(refactored)
@@ -45,7 +45,7 @@
   coordinates = [0] * len(dimensions)
 
   shifted = index
-  for i in xrange(len(dimensions) - 1, 0, -1):
+  for i in range(len(dimensions) - 1, 0, -1):
     coordinates[i] = shifted % dimensions[i]
     shifted = shifted / dimensions[i]
 
@@ -113,7 +113,7 @@
   for i, dimension in enumerate(dimensions):
     left = max(0, centerPosition[i] - radius)
     right = min(dimension - 1, centerPosition[i] + radius)
-    intervals.append(xrange(left, right + 1))
+    intervals.append(range(left, right + 1))
 
   coords = numpy.array(list(itertools.product(*intervals)))
   return numpy.ravel_multi_index(coords.T, dimensions)
@@ -145,7 +145,7 @@
     left = centerPosition[i] - radius
     right = min(centerPosition[i] + radius,
                 left + dimensions[i] - 1)
-    interval = [v % dimension for v in xrange(left, right + 1)]
+    interval = [v % dimension for v in range(left, right + 1)]
     intervals.append(interval)
 
   coords = numpy.array(list(itertools.product(*intervals)))
--- d:\nupic\src\python\python27\nupic\regions\anomaly_region.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\anomaly_region.py	(refactored)
@@ -80,7 +80,7 @@
 
 
   def __eq__(self, other):
-    for k, v1 in self.__dict__.iteritems():
+    for k, v1 in self.__dict__.items():
       if not k in other.__dict__:
         return False
       v2 = getattr(other, k)
--- d:\nupic\src\python\python27\nupic\regions\knn_anomaly_classifier_region.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\knn_anomaly_classifier_region.py	(refactored)
@@ -29,7 +29,7 @@
 
 from nupic.algorithms import anomaly
 from nupic.bindings.regions.PyRegion import PyRegion
-from knn_classifier_region import KNNClassifierRegion
+from .knn_classifier_region import KNNClassifierRegion
 from nupic.bindings.math import Random
 from nupic.frameworks.opf.exceptions import (HTMPredictionModelInvalidRangeError,
                                              HTMPredictionModelInvalidArgument)
@@ -1090,7 +1090,7 @@
 
 
   def __setstate__(self, data_dict):
-    for (name, value) in data_dict.iteritems():
+    for (name, value) in data_dict.items():
       setattr(self, name, value)
 
 
--- d:\nupic\src\python\python27\nupic\regions\knn_classifier_region.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\knn_classifier_region.py	(refactored)
@@ -685,7 +685,7 @@
 
     n = self.confusion.shape[0]
     assert n == self.confusion.shape[1], "Confusion matrix is non-square."
-    return self.confusion[range(n), range(n)].sum(), self.confusion.sum()
+    return self.confusion[list(range(n)), list(range(n))].sum(), self.confusion.sum()
 
   accuracy = property(fget=_getAccuracy)
 
@@ -838,8 +838,8 @@
     if self._tapFileIn is not None:
       for input in inputs:
         for k in range(len(input)):
-          print >> self._tapFileIn, input[k],
-        print >> self._tapFileIn
+          print(input[k], end=' ', file=self._tapFileIn)
+        print(file=self._tapFileIn)
 
 
   def handleLogOutput(self, output):
@@ -851,8 +851,8 @@
     #raise Exception('MULTI-LINE DUMMY\nMULTI-LINE DUMMY')
     if self._tapFileOut is not None:
       for k in range(len(output)):
-        print >> self._tapFileOut, output[k],
-      print >> self._tapFileOut
+        print(output[k], end=' ', file=self._tapFileOut)
+      print(file=self._tapFileOut)
 
 
   def _storeSample(self, inputVector, trueCatIndex, partition=0):
@@ -905,7 +905,7 @@
       if self._useAuxiliary:
         #print "\n  Auxiliary input stream from Image Sensor enabled."
         if self._justUseAuxiliary == True:
-          print "  Warning: You have chosen to ignore the image data and instead just use the auxiliary data stream."
+          print("  Warning: You have chosen to ignore the image data and instead just use the auxiliary data stream.")
 
 
     # Format inputs
@@ -918,7 +918,7 @@
       #auxVector = inputs['auxDataIn'][0].wvector(0).array()
       auxVector = inputs['auxDataIn']
       if auxVector.dtype != numpy.float32:
-        raise RuntimeError, "KNNClassifierRegion expects numpy.float32 for the auxiliary data vector"
+        raise RuntimeError("KNNClassifierRegion expects numpy.float32 for the auxiliary data vector")
       if self._justUseAuxiliary == True:
         #inputVector = inputs['auxDataIn'][0].wvector(0).array()
         inputVector = inputs['auxDataIn']
@@ -1014,8 +1014,8 @@
       probabilitiesOut[0:nout] = probabilities[0:nout]
 
       if self.verbosity >= 1:
-        print "KNNRegion: categoriesOut: ", categoriesOut[0:nout]
-        print "KNNRegion: probabilitiesOut: ", probabilitiesOut[0:nout]
+        print("KNNRegion: categoriesOut: ", categoriesOut[0:nout])
+        print("KNNRegion: probabilitiesOut: ", probabilitiesOut[0:nout])
 
       if self._scanInfo is not None:
         self._scanResults = [tuple(inference[:nout])]
--- d:\nupic\src\python\python27\nupic\regions\record_sensor.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\record_sensor.py	(refactored)
@@ -319,7 +319,7 @@
     """
 
     if self.verbosity > 0:
-      print "RecordSensor got data: %s" % data
+      print("RecordSensor got data: %s" % data)
 
     allFiltersHaveEnoughData = True
     if len(self.preEncodingFilters) > 0:
@@ -435,27 +435,27 @@
         if self._iterNum == 0:
           self.encoder.pprintHeader(prefix="sensor:")
         if reset:
-          print "RESET - sequenceID:%d" % sequenceId
+          print("RESET - sequenceID:%d" % sequenceId)
         if self.verbosity >= 2:
-          print
+          print()
 
       # If verbosity >=2, print the record fields
       if self.verbosity >= 1:
         self.encoder.pprint(outputs["dataOut"], prefix="%7d:" % (self._iterNum))
         scalarValues = self.encoder.getScalars(data)
         nz = outputs["dataOut"].nonzero()[0]
-        print "     nz: (%d)" % (len(nz)), nz
-        print "  encIn:", self.encoder.scalarsToStr(scalarValues)
+        print("     nz: (%d)" % (len(nz)), nz)
+        print("  encIn:", self.encoder.scalarsToStr(scalarValues))
       if self.verbosity >= 2:
         # if hasattr(data, 'header'):
         #  header = data.header()
         # else:
         #  header = '     '.join(self.dataSource.names)
         # print "        ", header
-        print "   data:", str(data)
+        print("   data:", str(data))
       if self.verbosity >= 3:
         decoded = self.encoder.decode(outputs["dataOut"])
-        print "decoded:", self.encoder.decodedToStr(decoded)
+        print("decoded:", self.encoder.decodedToStr(decoded))
 
       self._iterNum += 1
 
--- d:\nupic\src\python\python27\nupic\regions\sdr_classifier_region.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\sdr_classifier_region.py	(refactored)
@@ -437,7 +437,7 @@
         #                   4 : [0.2, 0.4, 0.3, 0.5]}
         #   becomes: [0.1, 0.3, 0.2, 0.7, 0.2, 0.4, 0.3, 0.5]
         stepProbabilities = clResults[step]
-        for categoryIndex in xrange(self.maxCategoryCount):
+        for categoryIndex in range(self.maxCategoryCount):
           flatIndex = categoryIndex + stepIndex * self.maxCategoryCount
           if categoryIndex < len(stepProbabilities):
             outputs['probabilities'][flatIndex] = \
--- d:\nupic\src\python\python27\nupic\regions\sp_region.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\sp_region.py	(refactored)
@@ -398,8 +398,8 @@
             # TODO: Re-enable warning or turn into error in a future release.
             pass
           else:
-            print self.__class__.__name__, "contains base class member '%s'" % \
-                attrName
+            print(self.__class__.__name__, "contains base class member '%s'" % \
+                attrName)
     if not self._loaded:
       for attrName in self._getEphemeralMembersBase():
         if attrName != "_loaded":
@@ -484,7 +484,7 @@
 
       import hotshot
       if self._iterations == 10:
-        print "\n  Collecting and sorting internal node profiling stats generated by hotshot..."
+        print("\n  Collecting and sorting internal node profiling stats generated by hotshot...")
         stats = hotshot.stats.load("hotshot.stats")
         stats.strip_dirs()
         stats.sort_stats('time', 'calls')
@@ -493,7 +493,7 @@
       # The guts of the compute are contained in the _compute() call so that we
       # can profile it if requested.
       if self._profileObj is None:
-        print "\n  Preparing to capture profile using hotshot..."
+        print("\n  Preparing to capture profile using hotshot...")
         if os.path.exists('hotshot.stats'):
           # There is an old hotshot stats profile left over, remove it.
           os.remove('hotshot.stats')
@@ -594,14 +594,14 @@
       output = self._spatialPoolerOutput.reshape(-1)
       outputNZ = output.nonzero()[0]
       outStr = " ".join(["%d" % int(token) for token in outputNZ])
-      print >>self._fpLogSP, output.size, outStr
+      print(output.size, outStr, file=self._fpLogSP)
 
     # Direct logging of SP inputs
     if self._fpLogSPInput:
       output = rfInput.reshape(-1)
       outputNZ = output.nonzero()[0]
       outStr = " ".join(["%d" % int(token) for token in outputNZ])
-      print >>self._fpLogSPInput, output.size, outStr
+      print(output.size, outStr, file=self._fpLogSPInput)
 
     return self._spatialPoolerOutput
 
@@ -990,10 +990,10 @@
   def _checkEphemeralMembers(self):
     for attrName in self._getEphemeralMembersBase():
       if not hasattr(self, attrName):
-        print "Missing base class member:", attrName
+        print("Missing base class member:", attrName)
     for attrName in self._getEphemeralMembers():
       if not hasattr(self, attrName):
-        print "Missing derived class member:", attrName
+        print("Missing derived class member:", attrName)
 
     for attrName in self._getEphemeralMembersBase():
       assert hasattr(self, attrName)
--- d:\nupic\src\python\python27\nupic\regions\spec.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\spec.py	(refactored)
@@ -169,7 +169,7 @@
 
     # Verify all item dicts
     hasDefaultInput = False
-    for k, v in self.inputs.items():
+    for k, v in list(self.inputs.items()):
       assert isinstance(k, str)
       assert isinstance(v, InputSpec)
       v.invariant()
@@ -179,7 +179,7 @@
 
 
     hasDefaultOutput = False
-    for k, v in self.outputs.items():
+    for k, v in list(self.outputs.items()):
       assert isinstance(k, str)
       assert isinstance(v, OutputSpec)
       v.invariant()
@@ -187,12 +187,12 @@
         assert not hasDefaultOutput
         hasDefaultOutput = True
 
-    for k, v in self.parameters.items():
+    for k, v in list(self.parameters.items()):
       assert isinstance(k, str)
       assert isinstance(v, ParameterSpec)
       v.invariant()
 
-    for k, v in self.commands.items():
+    for k, v in list(self.commands.items()):
       assert isinstance(k, str)
       assert isinstance(v, CommandSpec)
       v.invariant()
@@ -214,7 +214,7 @@
       attributes. The entire items dict will become a dict of dicts (same keys).
       """
       d = {}
-      for k, v in items.items():
+      for k, v in list(items.items()):
         d[k] = v.__dict__
 
       return d
--- d:\nupic\src\python\python27\nupic\regions\test_region.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\test_region.py	(refactored)
@@ -32,11 +32,10 @@
 
 
 
-class RegionIdentityPolicyBase(object):
+class RegionIdentityPolicyBase(object, metaclass=ABCMeta):
   """ A base class that must be subclassed by users in order to define the
   TestRegion instance's specialization. See also setIdentityPolicyInstance().
   """
-  __metaclass__ = ABCMeta
 
   @abstractmethod
   def initialize(self, testRegionObj):
@@ -508,8 +507,8 @@
   global g_debug
   if g_debug:
     callerTraceback = whois_callers_caller()
-    print "TEST_REGION (f=%s;line=%s): %s" % \
-                          (callerTraceback.function, callerTraceback.lineno, msg,)
+    print("TEST_REGION (f=%s;line=%s): %s" % \
+                          (callerTraceback.function, callerTraceback.lineno, msg,))
     sys.stdout.flush()
 
   return
--- d:\nupic\src\python\python27\nupic\regions\tm_region.py	(original)
+++ d:\nupic\src\python\python27\nupic\regions\tm_region.py	(refactored)
@@ -384,8 +384,8 @@
             # TODO: Re-enable warning or turn into error in a future release.
             pass
           else:
-            print self.__class__.__name__, "contains base class member '%s'" % \
-                attrName
+            print(self.__class__.__name__, "contains base class member '%s'" % \
+                attrName)
     if not self._loaded:
       for attrName in self._getEphemeralMembersBase():
         if attrName != "_loaded":
@@ -457,7 +457,7 @@
 
       import hotshot
       if self._iterations == 10:
-        print "\n  Collecting and sorting internal node profiling stats generated by hotshot..."
+        print("\n  Collecting and sorting internal node profiling stats generated by hotshot...")
         stats = hotshot.stats.load("hotshot.stats")
         stats.strip_dirs()
         stats.sort_stats('time', 'calls')
@@ -466,7 +466,7 @@
       # The guts of the compute are contained in the _compute() call so that
       # we can profile it if requested.
       if self._profileObj is None:
-        print "\n  Preparing to capture profile using hotshot..."
+        print("\n  Preparing to capture profile using hotshot...")
         if os.path.exists('hotshot.stats'):
           # There is an old hotshot stats profile left over, remove it.
           os.remove('hotshot.stats')
@@ -524,7 +524,7 @@
       output = tpOutput.reshape(-1)
       outputNZ = tpOutput.nonzero()[0]
       outStr = " ".join(["%d" % int(token) for token in outputNZ])
-      print >>self._fpLogTPOutput, output.size, outStr
+      print(output.size, outStr, file=self._fpLogTPOutput)
 
     # Write the bottom up out to our node outputs
     outputs['bottomUpOut'][:] = tpOutput.flat
@@ -952,10 +952,10 @@
   def _checkEphemeralMembers(self):
     for attrName in self._getEphemeralMembersBase():
       if not hasattr(self, attrName):
-        print "Missing base class member:", attrName
+        print("Missing base class member:", attrName)
     for attrName in self._getEphemeralMembers():
       if not hasattr(self, attrName):
-        print "Missing derived class member:", attrName
+        print("Missing derived class member:", attrName)
 
     for attrName in self._getEphemeralMembersBase():
       assert hasattr(self, attrName)
--- d:\nupic\src\python\python27\nupic\support\__init__.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\__init__.py	(refactored)
@@ -25,7 +25,7 @@
 These modules should NOT be used by client applications.
 """
 
-from __future__ import with_statement
+
 
 # Standard imports
 import os
@@ -36,13 +36,13 @@
 import logging.handlers
 from platform import python_version
 import struct
-from StringIO import StringIO
+from io import StringIO
 import time
 import traceback
 
 from pkg_resources import resource_string, resource_filename
 
-from configuration import Configuration
+from .configuration import Configuration
 from nupic.support.fs_helpers import makeDirectoryFromAbsolutePath
 
 
@@ -139,9 +139,9 @@
       s = class_name + '.' + callable_name
   lines = (s + additional).split('\n')
   length = max(len(line) for line in lines)
-  print >> stream, '-' * length
-  print >> stream, s + additional
-  print >> stream, '-' * length
+  print('-' * length, file=stream)
+  print(s + additional, file=stream)
+  print('-' * length, file=stream)
 
 
 
@@ -276,7 +276,7 @@
   global gLoggingInitialized
   if gLoggingInitialized:
     if verbose:
-      print >> sys.stderr, "Logging already initialized, doing nothing."
+      print("Logging already initialized, doing nothing.", file=sys.stderr)
     return
 
   consoleStreamMappings = {
@@ -287,7 +287,7 @@
   consoleLogLevels = ['DEBUG', 'INFO', 'WARNING', 'WARN', 'ERROR', 'CRITICAL',
                       'FATAL']
 
-  assert console is None or console in consoleStreamMappings.keys(), (
+  assert console is None or console in list(consoleStreamMappings.keys()), (
     'Unexpected console arg value: %r') % (console,)
 
   assert consoleLevel in consoleLogLevels, (
@@ -304,8 +304,8 @@
 
   # Load in the logging configuration file
   if verbose:
-    print >> sys.stderr, (
-      "Using logging configuration file: %s") % (configFilePath)
+    print((
+      "Using logging configuration file: %s") % (configFilePath), file=sys.stderr)
 
   # This dict will hold our replacement strings for logging configuration
   replacements = dict()
@@ -365,7 +365,7 @@
 
   for lineNum, line in enumerate(loggingFileContents.splitlines()):
     if "$$" in line:
-      for (key, value) in replacements.items():
+      for (key, value) in list(replacements.items()):
         line = line.replace(key, value)
 
     # If there is still a replacement string in the line, we're missing it
@@ -395,7 +395,7 @@
     'numenta-logs-%s' % (os.environ['USER'],),
     appName))
   appLogFileName = '%s-%s-%s.log' % (
-    appName, long(time.mktime(time.gmtime())), os.getpid())
+    appName, int(time.mktime(time.gmtime())), os.getpid())
   return os.path.join(appLogDir, appLogFileName)
 
 
--- d:\nupic\src\python\python27\nupic\support\configuration_base.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\configuration_base.py	(refactored)
@@ -25,7 +25,7 @@
 """
 
 
-from __future__ import with_statement
+
 
 import os
 import logging
@@ -186,9 +186,8 @@
     # Make a copy so we can update any current values obtained from environment
     #  variables
     result = dict(cls._properties)
-    keys = os.environ.keys()
-    replaceKeys = filter(lambda x: x.startswith(cls.envPropPrefix),
-                         keys)
+    keys = list(os.environ.keys())
+    replaceKeys = [x for x in keys if x.startswith(cls.envPropPrefix)]
     for envKey in replaceKeys:
       key = envKey[len(cls.envPropPrefix):]
       key = key.replace('_', '.')
--- d:\nupic\src\python\python27\nupic\support\configuration_custom.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\configuration_custom.py	(refactored)
@@ -25,7 +25,7 @@
 """
 
 
-from __future__ import with_statement
+
 
 from copy import copy
 import errno
@@ -92,7 +92,7 @@
 
     _CustomConfigurationFileWrapper.edit(properties)
 
-    for propertyName, value in properties.iteritems():
+    for propertyName, value in properties.items():
       cls.set(propertyName, value)
 
 
@@ -179,7 +179,7 @@
     if persistent:
       try:
         os.unlink(cls.getPath())
-      except OSError, e:
+      except OSError as e:
         if e.errno != errno.ENOENT:
           _getLogger().exception("Error %s while trying to remove dynamic " \
             "configuration file: %s", e.errno, cls.getPath())
@@ -223,7 +223,7 @@
     try:
       with open(configFilePath, 'r') as fp:
         contents = fp.read()
-    except IOError, e:
+    except IOError as e:
       if e.errno != errno.ENOENT:
         _getLogger().exception("Error %s reading custom configuration store "
           "from %s, while editing properties %s.",
@@ -234,14 +234,14 @@
     try:
       elements = ElementTree.XML(contents)
       ElementTree.tostring(elements)
-    except Exception, e:
+    except Exception as e:
       # Raising error as RuntimeError with custom message since ElementTree
       # exceptions aren't clear.
       msg = "File contents of custom configuration is corrupt.  File " \
         "location: %s; Contents: '%s'. Original Error (%s): %s." % \
         (configFilePath, contents, type(e), e)
       _getLogger().exception(msg)
-      raise RuntimeError(msg), None, sys.exc_info()[2]
+      raise RuntimeError(msg).with_traceback(sys.exc_info()[2])
 
 
     if elements.tag != 'configuration':
@@ -267,7 +267,7 @@
           raise RuntimeError(e)
 
     # Add unmatched remaining properties to custom config store
-    for propertyName, value in copyOfProperties.iteritems():
+    for propertyName, value in copyOfProperties.items():
       newProp = ElementTree.Element('property')
       nameTag = ElementTree.Element('name')
       nameTag.text = propertyName
@@ -283,7 +283,7 @@
       makeDirectoryFromAbsolutePath(os.path.dirname(configFilePath))
       with open(configFilePath,'w') as fp:
         fp.write(ElementTree.tostring(elements))
-    except Exception, e:
+    except Exception as e:
       _getLogger().exception("Error while saving custom configuration "
         "properties %s in %s.", properties, configFilePath)
       raise
--- d:\nupic\src\python\python27\nupic\support\console_printer.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\console_printer.py	(refactored)
@@ -72,22 +72,22 @@
       return
 
     if len(kw) > 1:
-      raise KeyError("Invalid keywords for cPrint: %s" % str(kw.keys()))
+      raise KeyError("Invalid keywords for cPrint: %s" % str(list(kw.keys())))
 
     newline = kw.get("newline", True)
     if len(kw) == 1 and 'newline' not in kw:
-      raise KeyError("Invalid keyword for cPrint: %s" % kw.keys()[0])
+      raise KeyError("Invalid keyword for cPrint: %s" % list(kw.keys())[0])
 
     if len(args) == 0:
       if newline:
-        print message
+        print(message)
       else:
-        print message,
+        print(message, end=' ')
     else:
       if newline:
-        print message % args
+        print(message % args)
       else:
-        print message % args,
+        print(message % args, end=' ')
 
 class Tee(object):
   """This class captures standard output and writes it to a file
--- d:\nupic\src\python\python27\nupic\support\decorators.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\decorators.py	(refactored)
@@ -116,7 +116,7 @@
       if logArgs:
         argsRepr = ', '.join(
           [repr(a) for a in args] +
-          ['%s=%r' % (k,v,) for k,v in kwargs.iteritems()])
+          ['%s=%r' % (k,v,) for k,v in kwargs.items()])
       else:
         argsRepr = ''
 
@@ -206,7 +206,7 @@
         numAttempts += 1
         try:
           result = func(*args, **kwargs)
-        except retryExceptions, e:
+        except retryExceptions as e:
           if not retryFilter(e, args, kwargs):
             if logger.isEnabledFor(logging.DEBUG):
               logger.debug(
--- d:\nupic\src\python\python27\nupic\support\enum.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\enum.py	(refactored)
@@ -75,7 +75,7 @@
     return cls.__labels[label]
 
 
-  for arg in list(args)+kwargs.keys():
+  for arg in list(args)+list(kwargs.keys()):
     if type(arg) is not str:
       raise TypeError("Enum arg {0} must be a string".format(arg))
 
@@ -84,10 +84,10 @@
                        "'{0}' is not a valid identifier".format(arg))
 
   #kwargs.update(zip(args, range(len(args))))
-  kwargs.update(zip(args, args))
+  kwargs.update(list(zip(args, args)))
   newType = type("Enum", (object,), kwargs)
 
-  newType.__labels = dict( (v,k) for k,v in kwargs.iteritems())
+  newType.__labels = dict( (v,k) for k,v in kwargs.items())
   newType.__values = set(newType.__labels.keys())
   newType.getLabel = functools.partial(getLabel, newType)
   newType.validate = functools.partial(validate, newType)
--- d:\nupic\src\python\python27\nupic\support\fs_helpers.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\fs_helpers.py	(refactored)
@@ -40,7 +40,7 @@
 
   try:
     os.makedirs(absDirPath)
-  except OSError, e:
+  except OSError as e:
     if e.errno != os.errno.EEXIST:
       raise
 
--- d:\nupic\src\python\python27\nupic\support\group_by.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\group_by.py	(refactored)
@@ -53,7 +53,7 @@
   advanceList = []
 
   # populate above lists
-  for i in xrange(0, len(args), 2):
+  for i in range(0, len(args), 2):
     listn = args[i]
     fn = args[i + 1]
     if listn is not None:
@@ -68,10 +68,10 @@
   nextList = [None] * n
   # while all lists aren't exhausted walk through each group in order
   while True:
-    for i in xrange(n):
+    for i in range(n):
       if advanceList[i]:
         try:
-          nextList[i] = generatorList[i].next()
+          nextList[i] = next(generatorList[i])
         except StopIteration:
           nextList[i] = None
 
@@ -85,7 +85,7 @@
 
     # populate the tuple to return based on minKeyVal
     retGroups = [minKeyVal]
-    for i in xrange(n):
+    for i in range(n):
       if nextList[i] is not None and nextList[i][0] == minKeyVal:
         retGroups.append(nextList[i][1])
         advanceList[i] = True
--- d:\nupic\src\python\python27\nupic\support\lock_attributes.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\lock_attributes.py	(refactored)
@@ -159,7 +159,7 @@
       if method is not None:
         setattr(cls, name, _allow_new_attributes(method))
 
-class LockAttributesMixin(object):
+class LockAttributesMixin(object, metaclass=LockAttributesMetaclass):
   """This class serves as a base (or mixin) for classes that want to enforce
   the locked attributes pattern (all attributes should be defined in
   ``__init__`` or ``__setstate__``.
@@ -171,4 +171,3 @@
   and the lock attributes machinery will be injected (unless the
   deactivation_key is defined in the environment)
   """
-  __metaclass__ = LockAttributesMetaclass
--- d:\nupic\src\python\python27\nupic\support\unittesthelpers\abstract_temporal_memory_test.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\unittesthelpers\abstract_temporal_memory_test.py	(refactored)
@@ -27,9 +27,7 @@
 
 
 
-class AbstractTemporalMemoryTest(object):
-  __metaclass__ = ABCMeta
-
+class AbstractTemporalMemoryTest(object, metaclass=ABCMeta):
   VERBOSITY = 1
 
   @abstractmethod
--- d:\nupic\src\python\python27\nupic\support\unittesthelpers\algorithm_test_helpers.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\unittesthelpers\algorithm_test_helpers.py	(refactored)
@@ -41,9 +41,9 @@
   """
   if seed is None:
     seed = int((time.time()%10000)*10)
-  print "Numpy seed set to:", seed, "called by",
+  print("Numpy seed set to:", seed, "called by", end=' ')
   callStack = traceback.extract_stack(limit=3)
-  print callStack[0][2], "line", callStack[0][1], "->", callStack[1][2]
+  print(callStack[0][2], "line", callStack[0][1], "->", callStack[1][2])
   return numpy.random.RandomState(seed)
 
 
@@ -55,7 +55,7 @@
   """
   numColumns = sourceSP.getNumColumns()
   numInputs = sourceSP.getNumInputs()
-  for i in xrange(numColumns):
+  for i in range(numColumns):
     potential = numpy.zeros(numInputs).astype(uintType)
     sourceSP.getPotential(i, potential)
     destSP.setPotential(i, potential)
@@ -69,9 +69,9 @@
 def getSeed():
   """Generate and log a 32-bit compatible seed value."""
   seed = int((time.time()%10000)*10)
-  print "New seed generated as:", seed, "called by",
+  print("New seed generated as:", seed, "called by", end=' ')
   callStack = traceback.extract_stack(limit=3)
-  print callStack[0][2], "line", callStack[0][1], "->", callStack[1][2]
+  print(callStack[0][2], "line", callStack[0][1], "->", callStack[1][2])
   return seed
 
 
@@ -128,7 +128,7 @@
   pySp.getMinOverlapDutyCycles(minOverlapDuty)
   cppSp.setMinOverlapDutyCycles(minOverlapDuty)
 
-  for i in xrange(numColumns):
+  for i in range(numColumns):
     potential = numpy.zeros(numInputs).astype(uintType)
     pySp.getPotential(i, potential)
     cppSp.setPotential(i, potential)
@@ -163,7 +163,7 @@
   else:
     raise RuntimeError("unrecognized implementation")
 
-  print params
+  print(params)
   sp = spClass(**params)
 
   return sp
--- d:\nupic\src\python\python27\nupic\support\unittesthelpers\testcasebase.py	(original)
+++ d:\nupic\src\python\python27\nupic\support\unittesthelpers\testcasebase.py	(refactored)
@@ -54,11 +54,11 @@
   def printTestHeader(self):
     """ Print out what test we are running """
 
-    print
-    print "###############################################################"
-    print "Running %s..." % (self,)
-    print "[%s UTC]" % (datetime.utcnow())
-    print "###############################################################"
+    print()
+    print("###############################################################")
+    print("Running %s..." % (self,))
+    print("[%s UTC]" % (datetime.utcnow()))
+    print("###############################################################")
     sys.stdout.flush()
     return
 
@@ -66,11 +66,11 @@
   def printBanner(self, msg, *args):
     """ Print out a banner """
 
-    print
-    print "==============================================================="
-    print msg % args
-    print >> sys.stdout, "[%s UTC; %s]" % (datetime.utcnow(), self,)
-    print "==============================================================="
+    print()
+    print("===============================================================")
+    print(msg % args)
+    print("[%s UTC; %s]" % (datetime.utcnow(), self,), file=sys.stdout)
+    print("===============================================================")
     sys.stdout.flush()
     return
 
--- d:\nupic\src\python\python27\nupic\swarming\ModelRunner.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\ModelRunner.py	(refactored)
@@ -25,7 +25,7 @@
 import os
 import sys
 import shutil
-import StringIO
+import io
 import threading
 import traceback
 from collections import deque
@@ -345,7 +345,7 @@
         inputRecord = self._inputSource.getNextRecordDict()
         if self._currentRecordIndex < 0:
           self._inputSource.setTimeout(10)
-      except Exception, e:
+      except Exception as e:
         raise utils.JobFailException(ErrorCodes.streamReading, str(e.args),
                                      traceback.format_exc())
 
@@ -432,7 +432,7 @@
     if self._predictionLogger is None:
       self._createPredictionLogger()
 
-    predictions = StringIO.StringIO()
+    predictions = io.StringIO()
     self._predictionLogger.checkpoint(
       checkpointSink=predictions,
       maxRows=int(Configuration.get('nupic.model.checkpoint.maxPredictionRows')))
@@ -586,7 +586,7 @@
     # Update model results
     results = json.dumps((metrics , optimizeDict))
     self._jobsDAO.modelUpdateResults(self._modelID,  results=results,
-                              metricValue=optimizeDict.values()[0],
+                              metricValue=list(optimizeDict.values())[0],
                               numRecords=(self._currentRecordIndex + 1))
 
     self._logger.debug(
@@ -879,7 +879,7 @@
 
     # Update a hadoop job counter at least once every 600 seconds so it doesn't
     #  think our map task is dead
-    print >>sys.stderr, "reporter:counter:HypersearchWorker,numRecords,50"
+    print("reporter:counter:HypersearchWorker,numRecords,50", file=sys.stderr)
 
     # See if the job got cancelled
     jobCancel = self._jobsDAO.jobGetFields(self._jobID, ['cancel'])[0]
--- d:\nupic\src\python\python27\nupic\swarming\dummy_model_runner.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\dummy_model_runner.py	(refactored)
@@ -341,7 +341,7 @@
     specified as lists, read the appropriate value for this model using the model
     index """
 
-    for key, value in params.iteritems():
+    for key, value in params.items():
       if type(value) == list:
         index = self.modelIndex % len(params[key])
         self._params[key] = params[key][index]
@@ -377,7 +377,7 @@
     if self._sleepModelRange is not None:
       range, delay = self._sleepModelRange.split(':')
       delay = float(delay)
-      range = map(int, range.split(','))
+      range = list(map(int, range.split(',')))
       modelIDs = self._jobsDAO.jobGetModelIDs(self._jobID)
       modelIDs.sort()
 
@@ -425,7 +425,7 @@
     # Create our top-level loop-control iterator
     # =========================================================================
     if self._iterations >= 0:
-      iterTracker = iter(xrange(self._iterations))
+      iterTracker = iter(range(self._iterations))
     else:
       iterTracker = iter(itertools.count())
 
@@ -601,9 +601,8 @@
     modelIDs = [e[0] for e in results]
     modelNums = [json.loads(e[1][0])['structuredParams']['__model_num'] for e in results]
 
-    sameModelNumbers = filter(lambda x: x[1] == self.modelIndex,
-                              zip(modelIDs, modelNums))
-
-    firstModelID = min(zip(*sameModelNumbers)[0])
+    sameModelNumbers = [x for x in zip(modelIDs, modelNums) if x[1] == self.modelIndex]
+
+    firstModelID = min(list(zip(*sameModelNumbers))[0])
 
     return firstModelID == self._modelID
--- d:\nupic\src\python\python27\nupic\swarming\experiment_utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\experiment_utils.py	(refactored)
@@ -126,9 +126,9 @@
     inferences:   A dictionary where the keys are InferenceElements
     """
     maxDelay = 0
-    for inferenceElement, inference in inferences.iteritems():
+    for inferenceElement, inference in inferences.items():
       if isinstance(inference, dict):
-        for key in inference.iterkeys():
+        for key in inference.keys():
           maxDelay = max(InferenceElement.getTemporalDelay(inferenceElement,
                                                             key),
                          maxDelay)
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch_v2.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch_v2.py	(refactored)
@@ -26,7 +26,7 @@
 import json
 import hashlib
 import itertools
-import StringIO
+import io
 import shutil
 import tempfile
 import copy
@@ -466,7 +466,7 @@
     if swarmId is not None:
       entryIdxs = self._swarmIdToIndexes.get(swarmId, [])
     else:
-      entryIdxs = range(len(self._allResults))
+      entryIdxs = list(range(len(self._allResults)))
     if len(entryIdxs) == 0:
       return ([], [], [], [], [])
 
@@ -538,7 +538,7 @@
               matured: list of matured booleans
     """
 
-    entryIdxs = range(len(self._allResults))
+    entryIdxs = list(range(len(self._allResults)))
     if len(entryIdxs) == 0:
       return ([], [], [], [], [])
 
@@ -709,7 +709,7 @@
     (allParticles, _, resultErrs, _, _) = self.getParticleInfos(swarmId,
                                               genIdx=None, matured=True)
 
-    for particleState, resultErr in itertools.izip(allParticles, resultErrs):
+    for particleState, resultErr in zip(allParticles, resultErrs):
       # Consider this generation?
       if maxGenIdx is not None:
         if particleState['genIdx'] > maxGenIdx:
@@ -1054,14 +1054,14 @@
 
       # If at DEBUG log level, print out permutations info to the log
       if self.logger.getEffectiveLevel() <= logging.DEBUG:
-        msg = StringIO.StringIO()
-        print >> msg, "Permutations file specifications: "
+        msg = io.StringIO()
+        print("Permutations file specifications: ", file=msg)
         info = dict()
         for key in ['_predictedField', '_permutations',
                     '_flattenedPermutations', '_encoderNames',
                     '_reportKeys', '_optimizeKey', '_maximize']:
           info[key] = getattr(self, key)
-        print >> msg, pprint.pformat(info)
+        print(pprint.pformat(info), file=msg)
         self.logger.debug(msg.getvalue())
         msg.close()
 
@@ -1119,7 +1119,7 @@
     # Honor any overrides provided in the stream definition
     aggFunctionsDict = {}
     if 'aggregation' in modelDescription['streamDef']:
-      for key in aggregationPeriod.keys():
+      for key in list(aggregationPeriod.keys()):
         if key in modelDescription['streamDef']['aggregation']:
           aggregationPeriod[key] = modelDescription['streamDef']['aggregation'][key]
       if 'fields' in modelDescription['streamDef']['aggregation']:
@@ -1128,13 +1128,13 @@
 
     # Do we have any aggregation at all?
     hasAggregation = False
-    for v in aggregationPeriod.values():
+    for v in list(aggregationPeriod.values()):
       if v != 0:
         hasAggregation = True
         break
 
     # Convert the aggFunctionsDict to a list
-    aggFunctionList = aggFunctionsDict.items()
+    aggFunctionList = list(aggFunctionsDict.items())
     aggregationInfo = dict(aggregationPeriod)
     aggregationInfo['fields'] = aggFunctionList
 
@@ -1199,7 +1199,7 @@
     # Open and execute the permutations file
     vars = {}
 
-    permFile = execfile(filename, globals(), vars)
+    permFile = exec(compile(open(filename).read(), filename, 'exec'), globals(), vars)
 
 
     # Read in misc info.
@@ -1315,8 +1315,8 @@
       # If it does not have a separate encoder for the predicted field that
       #  goes to the classifier, it is a legacy multi-step network
       classifierOnlyEncoder = None
-      for encoder in modelDescription["modelParams"]["sensorParams"]\
-                    ["encoders"].values():
+      for encoder in list(modelDescription["modelParams"]["sensorParams"]\
+                    ["encoders"].values()):
         if encoder.get("classifierOnly", False) \
              and encoder["fieldname"] == vars.get('predictedField', None):
           classifierOnlyEncoder = encoder
@@ -1375,7 +1375,7 @@
 
         # Store the flattened representations of the variables within the
         # encoder.
-        for encKey, encValue in value.kwArgs.iteritems():
+        for encKey, encValue in value.kwArgs.items():
           if isinstance(encValue, PermuteVariable):
             self._flattenedPermutations['%s:%s' % (flatKey, encKey)] = encValue
       elif isinstance(value, PermuteVariable):
@@ -1920,7 +1920,7 @@
       # Sort the swarms in priority order, trying the ones with the least
       #  number of models first
       swarmSizes = numpy.array([self._resultsDB.numModels(x) for x in swarmIds])
-      swarmSizeAndIdList = zip(swarmSizes, swarmIds)
+      swarmSizeAndIdList = list(zip(swarmSizes, swarmIds))
       swarmSizeAndIdList.sort()
       for (_, swarmId) in swarmSizeAndIdList:
 
@@ -2032,7 +2032,7 @@
     """
     # Send an update status periodically to the JobTracker so that it doesn't
     # think this worker is dead.
-    print >> sys.stderr, "reporter:status:In hypersearchV2: _okToExit"
+    print("reporter:status:In hypersearchV2: _okToExit", file=sys.stderr)
 
     # Any immature models still running?
     if not self._jobCancelled:
@@ -2177,7 +2177,7 @@
     self._checkForOrphanedModels()
 
     modelResults = []
-    for _ in xrange(numModels):
+    for _ in range(numModels):
       candidateParticle = None
 
       # If we've reached the max # of model to evaluate, we're done.
@@ -2201,7 +2201,7 @@
         else:
           # Send an update status periodically to the JobTracker so that it doesn't
           # think this worker is dead.
-          print >> sys.stderr, "reporter:status:In hypersearchV2: speculativeWait"
+          print("reporter:status:In hypersearchV2: speculativeWait", file=sys.stderr)
           time.sleep(self._speculativeWaitSecondsMax * random.random())
           return (False, [])
       useEncoders = candidateSwarm.split('.')
@@ -2342,7 +2342,7 @@
     if results is None:
       metricResult = None
     else:
-      metricResult = results[1].values()[0]
+      metricResult = list(results[1].values())[0]
 
     # Update our database.
     errScore = self._resultsDB.update(modelID=modelID,
@@ -2463,5 +2463,5 @@
                             cpuTime = time.clock() - cpuTimeStart)
 
 
-    except InvalidConnectionException, e:
+    except InvalidConnectionException as e:
       self.logger.warn("%s", e)
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch_worker.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch_worker.py	(refactored)
@@ -28,7 +28,7 @@
 import json
 import hashlib
 import itertools
-import StringIO
+import io
 import traceback
 
 from nupic.support import initLogging
@@ -37,7 +37,7 @@
 from nupic.swarming.hypersearch.error_codes import ErrorCodes
 from nupic.swarming.utils import clippedObj, validate
 from nupic.database.client_jobs_dao import ClientJobsDAO
-from hypersearch_v2 import HypersearchV2
+from .hypersearch_v2 import HypersearchV2
 
 
 
@@ -151,9 +151,8 @@
     # Each item in the list we are filtering contains:
     #  (idxIntoModelIDCtrList, (modelID, curCtr), (modelID, oldCtr))
     # We only want to keep the ones where the oldCtr != curCtr
-    changedEntries = filter(lambda x:x[1][1] != x[2][1],
-                      itertools.izip(xrange(numItems), curModelIDCtrList,
-                                     self._modelIDCtrList))
+    changedEntries = [x for x in zip(range(numItems), curModelIDCtrList,
+                                     self._modelIDCtrList) if x[1][1] != x[2][1]]
 
     if len(changedEntries) > 0:
       # Update values in our cache
@@ -200,7 +199,7 @@
       modelParamsAndHashs = cjDAO.modelsGetParams(newModelIDs)
       modelParamsAndHashs.sort()
 
-      for (mResult, mParamsAndHash) in itertools.izip(modelInfos,
+      for (mResult, mParamsAndHash) in zip(modelInfos,
                                                   modelParamsAndHashs):
 
         modelID = mResult.modelId
@@ -314,7 +313,7 @@
     try:
       exit = False
       numModelsTotal = 0
-      print >>sys.stderr, "reporter:status:Evaluating first model..."
+      print("reporter:status:Evaluating first model...", file=sys.stderr)
       while not exit:
 
         # ------------------------------------------------------------------
@@ -451,9 +450,9 @@
 
         self.logger.info("COMPLETED MODEL GID=%d; EVALUATED %d MODELs",
           modelIDToRun, numModelsTotal)
-        print >>sys.stderr, "reporter:status:Evaluated %d models..." % \
-                                    (numModelsTotal)
-        print >>sys.stderr, "reporter:counter:HypersearchWorker,numModels,1"
+        print("reporter:status:Evaluated %d models..." % \
+                                    (numModelsTotal), file=sys.stderr)
+        print("reporter:counter:HypersearchWorker,numModels,1", file=sys.stderr)
 
         if options.modelID is not None:
           exit = True
@@ -464,7 +463,7 @@
       self._hs.close()
 
     self.logger.info("FINISHED. Evaluated %d models." % (numModelsTotal))
-    print >>sys.stderr, "reporter:status:Finished, evaluated %d models" % (numModelsTotal)
+    print("reporter:status:Finished, evaluated %d models" % (numModelsTotal), file=sys.stderr)
     return options.jobID
 
 
@@ -543,11 +542,11 @@
     try:
       jobID = hst.run()
 
-    except Exception, e:
+    except Exception as e:
       jobID = options.jobID
-      msg = StringIO.StringIO()
-      print >>msg, "%s: Exception occurred in Hypersearch Worker: %r" % \
-         (ErrorCodes.hypersearchLogicErr, e)
+      msg = io.StringIO()
+      print("%s: Exception occurred in Hypersearch Worker: %r" % \
+         (ErrorCodes.hypersearchLogicErr, e), file=msg)
       traceback.print_exc(None, msg)
 
       completionReason = ClientJobsDAO.CMPL_REASON_ERROR
@@ -576,7 +575,7 @@
 
     try:
       jobID = hst.run()
-    except Exception, e:
+    except Exception as e:
       jobID = hst._options.jobID
       completionReason = ClientJobsDAO.CMPL_REASON_ERROR
       completionMsg = "ERROR: %s" % (e,)
--- d:\nupic\src\python\python27\nupic\swarming\permutations_runner.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\permutations_runner.py	(refactored)
@@ -26,7 +26,7 @@
 import imp
 import csv
 from datetime import datetime, timedelta
-import cPickle as pickle
+import pickle as pickle
 import time
 import subprocess
 
@@ -74,9 +74,9 @@
     jobrunner = gCurrentSearch
     jobID = jobrunner._HyperSearchRunner__searchJob.getJobID()
   except Exception as exc:
-    print exc
+    print(exc)
   else:
-    print "Canceling jobs due to receiving SIGTERM"
+    print("Canceling jobs due to receiving SIGTERM")
     cjdao.ClientJobsDAO.get().jobCancel(jobID)
 
 
@@ -94,7 +94,7 @@
 
 def _emit(verbosityLevel, info):
   if _verbosityEnabled(verbosityLevel):
-    print info
+    print(info)
 
 
 
@@ -158,9 +158,9 @@
   secs -= hours * (60 * 60)
   minutes = int(secs) / 60
   secs -= minutes * 60
-  print "Elapsed time (h:mm:ss): %d:%02d:%02d" % (hours, minutes, int(secs))
+  print("Elapsed time (h:mm:ss): %d:%02d:%02d" % (hours, minutes, int(secs)))
   jobID = search.peekSearchJob().getJobID()
-  print "Hypersearch ClientJobs job ID: ", jobID
+  print("Hypersearch ClientJobs job ID: ", jobID)
 
   return modelParams
 
@@ -360,7 +360,7 @@
   if  exports is None:
     return ret
   exportDict = json.loads(exports)
-  for key in exportDict.keys():
+  for key in list(exportDict.keys()):
     if (sys.platform.startswith('win')):
       ret+= "set \"%s=%s\" & " % (str(key), str(exportDict[key]))
     else:
@@ -529,7 +529,7 @@
               # Update the set of all encountered metrics keys (we will use
               # these to print column names in reports.csv)
               metrics = modelInfo.getReportMetrics()
-              self.__foundMetrcsKeySet.update(metrics.keys())
+              self.__foundMetrcsKeySet.update(list(metrics.keys()))
 
         numFinished = len(finishedModelIDs)
 
@@ -543,7 +543,7 @@
             expModelsStr = "of %s" % (expectedNumModels)
 
           stats = finishedModelStats
-          print ("<jobID: %s> %s %s models finished [success: %s; %s: %s; %s: "
+          print(("<jobID: %s> %s %s models finished [success: %s; %s: %s; %s: "
                  "%s; %s: %s; %s: %s; %s: %s; %s: %s]" % (
                      jobID,
                      numFinished,
@@ -561,38 +561,38 @@
                      "ORPHANED" if stats.numCompletedError else "orphaned",
                      stats.numCompletedOrphaned,
                      "UNKNOWN" if stats.numCompletedOther else "unknown",
-                     stats.numCompletedOther))
+                     stats.numCompletedOther)))
 
           # Print the first error message from the latest batch of completed
           # models
           if errorCompletionMsg:
-            print "ERROR MESSAGE: %s" % errorCompletionMsg
+            print("ERROR MESSAGE: %s" % errorCompletionMsg)
 
         # Print the new worker state, if it changed
         workerState = jobInfo.getWorkerState()
         if workerState != lastWorkerState:
-          print "##>> UPDATED WORKER STATE: \n%s" % (pprint.pformat(workerState,
-                                                           indent=4))
+          print("##>> UPDATED WORKER STATE: \n%s" % (pprint.pformat(workerState,
+                                                           indent=4)))
           lastWorkerState = workerState
 
         # Print the new job results, if it changed
         jobResults = jobInfo.getResults()
         if jobResults != lastJobResults:
-          print "####>> UPDATED JOB RESULTS: \n%s (elapsed time: %g secs)" \
-              % (pprint.pformat(jobResults, indent=4), time.time()-startTime)
+          print("####>> UPDATED JOB RESULTS: \n%s (elapsed time: %g secs)" \
+              % (pprint.pformat(jobResults, indent=4), time.time()-startTime))
           lastJobResults = jobResults
 
         # Print the new model milestones if they changed
         modelMilestones = jobInfo.getModelMilestones()
         if modelMilestones != lastModelMilestones:
-          print "##>> UPDATED MODEL MILESTONES: \n%s" % (
-              pprint.pformat(modelMilestones, indent=4))
+          print("##>> UPDATED MODEL MILESTONES: \n%s" % (
+              pprint.pformat(modelMilestones, indent=4)))
           lastModelMilestones = modelMilestones
 
         # Print the new engine status if it changed
         engStatus = jobInfo.getEngStatus()
         if engStatus != lastEngStatus:
-          print "##>> UPDATED STATUS: \n%s" % (engStatus)
+          print("##>> UPDATED STATUS: \n%s" % (engStatus))
           lastEngStatus = engStatus
 
       # Sleep before next check
@@ -600,18 +600,18 @@
         if self._options["timeout"] != None:
           if ((datetime.now() - lastUpdateTime) >
               timedelta(minutes=self._options["timeout"])):
-            print "Timeout reached, exiting"
+            print("Timeout reached, exiting")
             self.__cjDAO.jobCancel(jobID)
             sys.exit(1)
         time.sleep(1)
 
     # Tabulate results
     modelIDs = self.__searchJob.queryModelIDs()
-    print "Evaluated %s models" % len(modelIDs)
-    print "HyperSearch finished!"
+    print("Evaluated %s models" % len(modelIDs))
+    print("HyperSearch finished!")
 
     jobInfo = self.__searchJob.getJobStatus(self._workers)
-    print "Worker completion message: %s" % (jobInfo.getWorkerCompletionMsg())
+    print("Worker completion message: %s" % (jobInfo.getWorkerCompletionMsg()))
 
 
 
@@ -649,10 +649,10 @@
     if self._options["action"] == "dryRun":
       args = [sys.argv[0], "--params=%s" % (json.dumps(params))]
 
-      print
-      print "=================================================================="
-      print "RUNNING PERMUTATIONS INLINE as \"DRY RUN\"..."
-      print "=================================================================="
+      print()
+      print("==================================================================")
+      print("RUNNING PERMUTATIONS INLINE as \"DRY RUN\"...")
+      print("==================================================================")
       jobID = hypersearch_worker.main(args)
 
     else:
@@ -683,9 +683,9 @@
       hyperSearchJob=searchJob)
 
     if self._options["action"] == "dryRun":
-      print "Successfully executed \"dry-run\" hypersearch, jobID=%d" % (jobID)
+      print("Successfully executed \"dry-run\" hypersearch, jobID=%d" % (jobID))
     else:
-      print "Successfully submitted new HyperSearch job, jobID=%d" % (jobID)
+      print("Successfully submitted new HyperSearch job, jobID=%d" % (jobID))
       _emit(Verbosity.DEBUG,
             "Each worker executing the command line: %s" % (cmdLine,))
 
@@ -732,7 +732,7 @@
     options:        NupicRunPermutations options dict
     retval:         nothing
     """
-    print "Generating experiment requests..."
+    print("Generating experiment requests...")
 
     searchParams = _ClientJobUtils.makeSearchJobParamsDict(options=options)
 
@@ -777,10 +777,10 @@
     searchVar = set()
     for modelInfo in _iterModels(modelIDs):
       if modelInfo.isFinished():
-        vars = modelInfo.getParamLabels().keys()
+        vars = list(modelInfo.getParamLabels().keys())
         searchVar.update(vars)
         metrics = modelInfo.getReportMetrics()
-        metricstmp.update(metrics.keys())
+        metricstmp.update(list(metrics.keys()))
     if metricsKeys is None:
       metricsKeys = metricstmp
     # Create a csv report writer
@@ -795,8 +795,8 @@
     modelStats = _ModelStats()
     #numCompletedOther = long(0)
 
-    print "\nResults from all experiments:"
-    print "----------------------------------------------------------------"
+    print("\nResults from all experiments:")
+    print("----------------------------------------------------------------")
 
     # Get common optimization metric info from permutations script
     searchParams = hyperSearchJob.getParams()
@@ -820,12 +820,12 @@
 
     try:
       results = json.loads(jobInfo.results)
-    except Exception, e:
-      print "json.loads(jobInfo.results) raised an exception.  " \
-            "Here is some info to help with debugging:"
-      print "jobInfo: ", jobInfo
-      print "jobInfo.results: ", jobInfo.results
-      print "EXCEPTION: ", e
+    except Exception as e:
+      print("json.loads(jobInfo.results) raised an exception.  " \
+            "Here is some info to help with debugging:")
+      print("jobInfo: ", jobInfo)
+      print("jobInfo.results: ", jobInfo.results)
+      print("EXCEPTION: ", e)
       raise
 
     bestModelNum = results["bestModel"]
@@ -863,7 +863,7 @@
       if modelInfo.getModelID() == bestModelNum:
         bestModel = modelInfo
         bestModelIterIndex=i
-        bestMetric = optimizationMetrics.values()[0]
+        bestMetric = list(optimizationMetrics.values())[0]
 
       # Keep track of the best-performing model
       if optimizationMetrics:
@@ -873,20 +873,20 @@
 
       # Append to our list of modelIDs and scores
       if modelInfo.getCompletionReason().isEOF():
-        scoreModelIDDescList.append((optimizationMetrics.values()[0],
+        scoreModelIDDescList.append((list(optimizationMetrics.values())[0],
                                     modelInfo.getModelID(),
                                     modelInfo.getGeneratedDescriptionFile(),
                                     modelInfo.getParamLabels()))
 
-      print "[%d] Experiment %s\n(%s):" % (i, modelInfo, expDesc)
+      print("[%d] Experiment %s\n(%s):" % (i, modelInfo, expDesc))
       if (modelInfo.isFinished() and
           not (modelInfo.getCompletionReason().isStopped or
                modelInfo.getCompletionReason().isEOF())):
-        print ">> COMPLETION MESSAGE: %s" % modelInfo.getCompletionMsg()
+        print(">> COMPLETION MESSAGE: %s" % modelInfo.getCompletionMsg())
 
       if reportMetrics:
         # Update our metrics key set and format string
-        foundMetricsKeySet.update(reportMetrics.iterkeys())
+        foundMetricsKeySet.update(iter(reportMetrics.keys()))
         if len(sortedMetricsKeys) != len(foundMetricsKeySet):
           sortedMetricsKeys = sorted(foundMetricsKeySet)
 
@@ -900,13 +900,13 @@
               m = "%r (*)" % reportMetrics[key]
             else:
               m = "%r" % reportMetrics[key]
-            print formatStr % (key+":"), m
-        print
+            print(formatStr % (key+":"), m)
+        print()
 
     # Summarize results
-    print "--------------------------------------------------------------"
+    print("--------------------------------------------------------------")
     if len(modelIDs) > 0:
-      print "%d experiments total (%s).\n" % (
+      print("%d experiments total (%s).\n" % (
           len(modelIDs),
           ("all completed successfully"
            if (modelStats.numCompletedKilled + modelStats.numCompletedEOF) ==
@@ -914,23 +914,23 @@
            else "WARNING: %d models have not completed or there were errors" % (
                len(modelIDs) - (
                    modelStats.numCompletedKilled + modelStats.numCompletedEOF +
-                   modelStats.numCompletedStopped))))
+                   modelStats.numCompletedStopped)))))
 
       if modelStats.numStatusOther > 0:
-        print "ERROR: models with unexpected status: %d" % (
-            modelStats.numStatusOther)
-
-      print "WaitingToStart: %d" % modelStats.numStatusWaitingToStart
-      print "Running: %d" % modelStats.numStatusRunning
-      print "Completed: %d" % modelStats.numStatusCompleted
+        print("ERROR: models with unexpected status: %d" % (
+            modelStats.numStatusOther))
+
+      print("WaitingToStart: %d" % modelStats.numStatusWaitingToStart)
+      print("Running: %d" % modelStats.numStatusRunning)
+      print("Completed: %d" % modelStats.numStatusCompleted)
       if modelStats.numCompletedOther > 0:
-        print "    ERROR: models with unexpected completion reason: %d" % (
-            modelStats.numCompletedOther)
-      print "    ran to EOF: %d" % modelStats.numCompletedEOF
-      print "    ran to stop signal: %d" % modelStats.numCompletedStopped
-      print "    were orphaned: %d" % modelStats.numCompletedOrphaned
-      print "    killed off: %d" % modelStats.numCompletedKilled
-      print "    failed: %d" % modelStats.numCompletedError
+        print("    ERROR: models with unexpected completion reason: %d" % (
+            modelStats.numCompletedOther))
+      print("    ran to EOF: %d" % modelStats.numCompletedEOF)
+      print("    ran to stop signal: %d" % modelStats.numCompletedStopped)
+      print("    were orphaned: %d" % modelStats.numCompletedOrphaned)
+      print("    killed off: %d" % modelStats.numCompletedKilled)
+      print("    failed: %d" % modelStats.numCompletedError)
 
       assert modelStats.numStatusOther == 0, "numStatusOther=%s" % (
           modelStats.numStatusOther)
@@ -938,43 +938,43 @@
           modelStats.numCompletedOther)
 
     else:
-      print "0 experiments total."
+      print("0 experiments total.")
 
     # Print out the field contributions
-    print
+    print()
     global gCurrentSearch
     jobStatus = hyperSearchJob.getJobStatus(gCurrentSearch._workers)
     jobResults = jobStatus.getResults()
     if "fieldContributions" in jobResults:
-      print "Field Contributions:"
+      print("Field Contributions:")
       pprint.pprint(jobResults["fieldContributions"], indent=4)
     else:
-      print "Field contributions info not available"
+      print("Field contributions info not available")
 
     # Did we have an optimize key?
     if bestModel is not None:
       maxKeyLen = max([len(k) for k in sortedMetricsKeys])
       maxKeyLen = max(maxKeyLen, len(optimizationMetricKey))
       formatStr = "  %%-%ds" % (maxKeyLen+2)
-      bestMetricValue = bestModel.getOptimizationMetrics().values()[0]
-      optimizationMetricName = bestModel.getOptimizationMetrics().keys()[0]
-      print
-      print "Best results on the optimization metric %s (maximize=%s):" % (
-          optimizationMetricName, maximizeMetric)
-      print "[%d] Experiment %s (%s):" % (
-          bestModelIterIndex, bestModel, bestModel.getModelDescription())
-      print formatStr % (optimizationMetricName+":"), bestMetricValue
-      print
-      print "Total number of Records processed: %d"  % totalRecords
-      print
-      print "Total wall time for all models: %d" % totalWallTime
+      bestMetricValue = list(bestModel.getOptimizationMetrics().values())[0]
+      optimizationMetricName = list(bestModel.getOptimizationMetrics().keys())[0]
+      print()
+      print("Best results on the optimization metric %s (maximize=%s):" % (
+          optimizationMetricName, maximizeMetric))
+      print("[%d] Experiment %s (%s):" % (
+          bestModelIterIndex, bestModel, bestModel.getModelDescription()))
+      print(formatStr % (optimizationMetricName+":"), bestMetricValue)
+      print()
+      print("Total number of Records processed: %d"  % totalRecords)
+      print()
+      print("Total wall time for all models: %d" % totalWallTime)
 
       hsJobParams = hyperSearchJob.getParams()
 
     # Were we asked to write out the top N model description files?
     if options["genTopNDescriptions"] > 0:
-      print "\nGenerating description files for top %d models..." % (
-              options["genTopNDescriptions"])
+      print("\nGenerating description files for top %d models..." % (
+              options["genTopNDescriptions"]))
       scoreModelIDDescList.sort()
       scoreModelIDDescList = scoreModelIDDescList[
           0:options["genTopNDescriptions"]]
@@ -983,8 +983,8 @@
       for (score, modelID, description, paramLabels) in scoreModelIDDescList:
         i += 1
         outDir = os.path.join(options["permWorkDir"], "model_%d" % (i))
-        print "Generating description file for model %s at %s" % \
-          (modelID, outDir)
+        print("Generating description file for model %s at %s" % \
+          (modelID, outDir))
         if not os.path.exists(outDir):
           os.makedirs(outDir)
 
@@ -1006,14 +1006,14 @@
         # Generate a csv file with the parameter settings in it
         fd = open(os.path.join(outDir, "params.csv"), "wb")
         writer = csv.writer(fd)
-        colNames = paramLabels.keys()
+        colNames = list(paramLabels.keys())
         colNames.sort()
         writer.writerow(colNames)
         row = [paramLabels[x] for x in colNames]
         writer.writerow(row)
         fd.close()
 
-        print "Generating model params file..."
+        print("Generating model params file...")
         # Generate a model params file alongside the description.py
         mod = imp.load_source("description", os.path.join(outDir,
                                                           "description.py"))
@@ -1023,7 +1023,7 @@
                                             pprint.pformat(model_description)))
         fd.close()
 
-      print
+      print()
 
     reportWriter.finalize()
     return model_description
@@ -1123,17 +1123,17 @@
 
   def __init__(self):
     # Tallies of experiment dispositions
-    self.numStatusWaitingToStart = long(0)
-    self.numStatusRunning = long(0)
-    self.numStatusCompleted = long(0)
-    self.numStatusOther = long(0)
+    self.numStatusWaitingToStart = int(0)
+    self.numStatusRunning = int(0)
+    self.numStatusCompleted = int(0)
+    self.numStatusOther = int(0)
     #self.numCompletedSuccess = long(0)
-    self.numCompletedKilled = long(0)
-    self.numCompletedError = long(0)
-    self.numCompletedStopped = long(0)
-    self.numCompletedEOF = long(0)
-    self.numCompletedOther = long(0)
-    self.numCompletedOrphaned = long(0)
+    self.numCompletedKilled = int(0)
+    self.numCompletedError = int(0)
+    self.numCompletedStopped = int(0)
+    self.numCompletedEOF = int(0)
+    self.numCompletedOther = int(0)
+    self.numCompletedOrphaned = int(0)
 
 
 
@@ -1223,46 +1223,46 @@
     csv = self.__csvFileObj
 
     # Emit model info row to report.csv
-    print >> csv, "%s, " % (self.__searchJobID),
-    print >> csv, "%s, " % (modelInfo.getModelID()),
-    print >> csv, "%s, " % (modelInfo.statusAsString()),
+    print("%s, " % (self.__searchJobID), end=' ', file=csv)
+    print("%s, " % (modelInfo.getModelID()), end=' ', file=csv)
+    print("%s, " % (modelInfo.statusAsString()), end=' ', file=csv)
     if modelInfo.isFinished():
-      print >> csv, "%s, " % (modelInfo.getCompletionReason()),
+      print("%s, " % (modelInfo.getCompletionReason()), end=' ', file=csv)
     else:
-      print >> csv, "NA, ",
+      print("NA, ", end=' ', file=csv)
     if not modelInfo.isWaitingToStart():
-      print >> csv, "%s, " % (modelInfo.getStartTime()),
+      print("%s, " % (modelInfo.getStartTime()), end=' ', file=csv)
     else:
-      print >> csv, "NA, ",
+      print("NA, ", end=' ', file=csv)
     if modelInfo.isFinished():
       dateFormat = "%Y-%m-%d %H:%M:%S"
       startTime = modelInfo.getStartTime()
       endTime = modelInfo.getEndTime()
-      print >> csv, "%s, " % endTime,
+      print("%s, " % endTime, end=' ', file=csv)
       st = datetime.strptime(startTime, dateFormat)
       et = datetime.strptime(endTime, dateFormat)
-      print >> csv, "%s, " % (str((et - st).seconds)),
+      print("%s, " % (str((et - st).seconds)), end=' ', file=csv)
     else:
-      print >> csv, "NA, ",
-      print >> csv, "NA, ",
-    print >> csv, "%s, " % str(modelInfo.getModelDescription()),
-    print >> csv, "%s, " % str(modelInfo.getNumRecords()),
+      print("NA, ", end=' ', file=csv)
+      print("NA, ", end=' ', file=csv)
+    print("%s, " % str(modelInfo.getModelDescription()), end=' ', file=csv)
+    print("%s, " % str(modelInfo.getNumRecords()), end=' ', file=csv)
     paramLabelsDict = modelInfo.getParamLabels()
     for key in self.__sortedVariableNames:
       # Some values are complex structures,.. which need to be represented as
       # strings
       if key in paramLabelsDict:
-        print >> csv, "%s, " % (paramLabelsDict[key]),
+        print("%s, " % (paramLabelsDict[key]), end=' ', file=csv)
       else:
-        print >> csv, "None, ",
+        print("None, ", end=' ', file=csv)
     metrics = modelInfo.getReportMetrics()
     for key in self.__sortedMetricsKeys:
       value = metrics.get(key, "NA")
       value = str(value)
       value = value.replace("\n", " ")
-      print >> csv, "%s, " % (value),
-
-    print >> csv
+      print("%s, " % (value), end=' ', file=csv)
+
+    print(file=csv)
 
 
 
@@ -1278,13 +1278,13 @@
       self.__csvFileObj.close()
       self.__csvFileObj = None
 
-      print "Report csv saved in %s" % (self.__reportCSVPath,)
+      print("Report csv saved in %s" % (self.__reportCSVPath,))
 
       if self.__backupCSVPath:
-        print "Previous report csv file was backed up to %s" % \
-                (self.__backupCSVPath,)
+        print("Previous report csv file was backed up to %s" % \
+                (self.__backupCSVPath,))
     else:
-      print "Nothing was written to report csv file."
+      print("Nothing was written to report csv file.")
 
 
 
@@ -1324,25 +1324,25 @@
 
     # If we are appending, add some blank line separators
     if not self.__replaceReport and backupCSVPath:
-      print >> csv
-      print >> csv
+      print(file=csv)
+      print(file=csv)
 
     # Print the column names
-    print >> csv, "jobID, ",
-    print >> csv, "modelID, ",
-    print >> csv, "status, " ,
-    print >> csv, "completionReason, ",
-    print >> csv, "startTime, ",
-    print >> csv, "endTime, ",
-    print >> csv, "runtime(s), " ,
-    print >> csv, "expDesc, ",
-    print >> csv, "numRecords, ",
+    print("jobID, ", end=' ', file=csv)
+    print("modelID, ", end=' ', file=csv)
+    print("status, ", end=' ', file=csv)
+    print("completionReason, ", end=' ', file=csv)
+    print("startTime, ", end=' ', file=csv)
+    print("endTime, ", end=' ', file=csv)
+    print("runtime(s), ", end=' ', file=csv)
+    print("expDesc, ", end=' ', file=csv)
+    print("numRecords, ", end=' ', file=csv)
 
     for key in self.__sortedVariableNames:
-      print >> csv, "%s, " % key,
+      print("%s, " % key, end=' ', file=csv)
     for key in self.__sortedMetricsKeys:
-      print >> csv, "%s, " % key,
-    print >> csv
+      print("%s, " % key, end=' ', file=csv)
+    print(file=csv)
 
 
 
@@ -1887,7 +1887,7 @@
   nupicModelID:      Nupic modelID
   retval:           _NupicModelInfo instance for the given nupicModelID.
   """
-  return _iterModels([nupicModelID]).next()
+  return next(_iterModels([nupicModelID]))
 
 
 
@@ -1949,7 +1949,7 @@
 
 
 
-    def next(self):
+    def __next__(self):
       """Iterator Protocol function
 
       Parameters:
@@ -2124,7 +2124,7 @@
       paramSettings = self.getParamLabels()
       # Form a csv friendly string representation of this model
       items = []
-      for key, value in paramSettings.items():
+      for key, value in list(paramSettings.items()):
         items.append("%s_%s" % (key, value))
       return ".".join(items)
 
@@ -2165,7 +2165,7 @@
     if "particleState" in params:
       retval = dict()
       queue = [(pair, retval) for pair in
-               params["particleState"]["varStates"].iteritems()]
+               params["particleState"]["varStates"].items()]
       while len(queue) > 0:
         pair, output = queue.pop()
         k, v = pair
@@ -2175,7 +2175,7 @@
         else:
           if k not in output:
             output[k] = dict()
-          queue.extend((pair, output[k]) for pair in v.iteritems())
+          queue.extend((pair, output[k]) for pair in v.items())
       return retval
 
 
--- d:\nupic\src\python\python27\nupic\swarming\utils.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\utils.py	(refactored)
@@ -27,7 +27,7 @@
 import logging
 import re
 import traceback
-import StringIO
+import io
 from collections import namedtuple
 import pprint
 import shutil
@@ -140,7 +140,7 @@
   results:      dictionary of results at this level.
   """
 
-  allKeys = results.keys()
+  allKeys = list(results.keys())
   allKeys.sort()
   for key in allKeys:
     if hasattr(results[key], 'keys'):
@@ -285,7 +285,7 @@
   Returns:  a quoted string with characters that are represented in python via
             escape sequences converted to those escape sequences
   """
-  assert type(string) in types.StringTypes
+  assert type(string) in (str,)
   return pprint.pformat(string)
 
 
@@ -306,9 +306,9 @@
   retval:               (completionReason, completionMsg)
   """
 
-  msg = StringIO.StringIO()
-  print >>msg, "Exception occurred while running model %s: %r (%s)" % (
-    modelID, e, type(e))
+  msg = io.StringIO()
+  print("Exception occurred while running model %s: %r (%s)" % (
+    modelID, e, type(e)), file=msg)
   traceback.print_exc(None, msg)
 
   completionReason = jobsDAO.CMPL_REASON_ERROR
@@ -388,11 +388,11 @@
     paramsFile = open(paramsFilePath, 'wb')
     paramsFile.write(_paramsFileHead())
 
-    items = params.items()
+    items = list(params.items())
     items.sort()
     for (key,value) in items:
       quotedKey = _quoteAndEscape(key)
-      if isinstance(value, basestring):
+      if isinstance(value, str):
 
         paramsFile.write("  %s : '%s',\n" % (quotedKey , value))
       else:
@@ -436,7 +436,7 @@
 
     except InvalidConnectionException:
       raise
-    except Exception, e:
+    except Exception as e:
 
       (completionReason, completionMsg) = _handleModelRunnerException(jobID,
                                      modelID, jobsDAO, experimentDir, logger, e)
@@ -484,7 +484,7 @@
     sys.exit(1)
   except InvalidConnectionException:
     raise
-  except Exception, e:
+  except Exception as e:
     (completionReason, completionMsg) = _handleModelRunnerException(jobID,
                                    modelID, jobsDAO, "NA",
                                    logger, e)
@@ -528,7 +528,7 @@
       act =   self.Activity(repeating=req.repeating,
                             period=req.period,
                             cb=req.cb,
-                            iteratorHolder=[iter(xrange(req.period))])
+                            iteratorHolder=[iter(range(req.period))])
       self.__activities.append(act)
     return
 
@@ -549,7 +549,7 @@
       except StopIteration:
         act.cb()
         if act.repeating:
-          act.iteratorHolder[0] = iter(xrange(act.period))
+          act.iteratorHolder[0] = iter(range(act.period))
         else:
           act.iteratorHolder[0] = None
 
@@ -594,14 +594,14 @@
     d = copy.deepcopy(d)
 
   newDict = {}
-  toCopy = [(k, v, newDict, ()) for k, v in d.iteritems()]
+  toCopy = [(k, v, newDict, ()) for k, v in d.items()]
   while len(toCopy) > 0:
     k, v, d, prevKeys = toCopy.pop()
     prevKeys = prevKeys + (k,)
     if isinstance(v, dict):
       d[k] = dict()
       toCopy[0:0] = [(innerK, innerV, d[k], prevKeys)
-                     for innerK, innerV in v.iteritems()]
+                     for innerK, innerV in v.items()]
     else:
       #print k, v, prevKeys
       newV = f(v, prevKeys)
@@ -622,7 +622,7 @@
   remainingDicts = [(d, ())]
   while len(remainingDicts) > 0:
     current, prevKeys = remainingDicts.pop()
-    for k, v in current.iteritems():
+    for k, v in current.items():
       keys = prevKeys + (k,)
       if isinstance(v, dict):
         remainingDicts.insert(0, (v, keys))
@@ -651,7 +651,7 @@
   # Printing a dict?
   if isinstance(obj, dict):
     objOut = dict()
-    for key,val in obj.iteritems():
+    for key,val in obj.items():
       objOut[key] = clippedObj(val)
 
   # Printing a list?
@@ -694,7 +694,7 @@
           ValidationError when value fails json validation
   """
 
-  assert len(kwds.keys()) >= 1
+  assert len(list(kwds.keys())) >= 1
   assert 'schemaPath' in kwds or 'schemaDict' in kwds
 
   schemaDict = None
@@ -740,7 +740,7 @@
   itemStrs = []
 
   if isinstance(obj, dict):
-    items = obj.items()
+    items = list(obj.items())
     items.sort()
     for key, value in items:
       itemStrs.append('%s: %s' % (json.dumps(key), sortedJSONDumpS(value)))
--- d:\nupic\src\python\python27\nupic\swarming\exp_generator\experiment_generator.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\exp_generator\experiment_generator.py	(refactored)
@@ -150,9 +150,9 @@
 def _handleShowSchemaOption():
   """ Displays command schema to stdout and exit program
   """
-  print "\n============== BEGIN INPUT SCHEMA for --description =========>>"
-  print(json.dumps(_getExperimentDescriptionSchema(), indent=_INDENT_STEP*2))
-  print "\n<<============== END OF INPUT SCHEMA for --description ========"
+  print("\n============== BEGIN INPUT SCHEMA for --description =========>>")
+  print((json.dumps(_getExperimentDescriptionSchema(), indent=_INDENT_STEP*2)))
+  print("\n<<============== END OF INPUT SCHEMA for --description ========")
   return
 
 
@@ -178,7 +178,7 @@
   # convert --description arg from JSON string to dict
   try:
     args = json.loads(cmdArgStr)
-  except Exception, e:
+  except Exception as e:
     raise _InvalidCommandArgException(
       _makeUsageErrorStr(
         ("JSON arg parsing failed for --description: %s\n" + \
@@ -217,7 +217,7 @@
     JSONStringFromFile = fileHandle.read().splitlines()
     JSONStringFromFile = ''.join(JSONStringFromFile)
 
-  except Exception, e:
+  except Exception as e:
     raise _InvalidCommandArgException(
       _makeUsageErrorStr(
         ("File open failed for --descriptionFromFile: %s\n" + \
@@ -253,7 +253,7 @@
   returns whether or not the object is a string
   """
 
-  return type(obj) in types.StringTypes
+  return type(obj) in (str,)
 
 
 
@@ -344,7 +344,7 @@
 
   metricSpecAsString = "MetricSpec(%s)" % \
     ', '.join(['%s=%r' % (item[0],item[1])
-              for item in metricSpecArgs.iteritems()])
+              for item in metricSpecArgs.items()])
 
   if not returnLabel:
     return metricSpecAsString
@@ -391,13 +391,13 @@
     inputFile.close()
 
 
-  print "Writing ", len(inputLines), "lines..."
+  print("Writing ", len(inputLines), "lines...")
 
   for line in inputLines:
     tempLine = line
 
     # Enumerate through each key in replacementDict and replace with value
-    for k, v in replacementDict.iteritems():
+    for k, v in replacementDict.items():
       if v is None:
         v = "None"
       tempLine = re.sub(k, v, tempLine)
@@ -657,7 +657,7 @@
   # PermuteEncoder().
   if encoderDict.get('classifierOnly', False):
     permStr = "dict("
-    for key, value in encoderDict.items():
+    for key, value in list(encoderDict.items()):
       if key == "name":
         continue
 
@@ -665,7 +665,7 @@
         permStr += "n=PermuteInt(%d, %d), " % (encoderDict["w"] + 7,
                                                encoderDict["w"] + 500)
       else:
-        if issubclass(type(value), basestring):
+        if issubclass(type(value), str):
           permStr += "%s='%s', " % (key, value)
         else:
           permStr += "%s=%s, " % (key, value)
@@ -677,7 +677,7 @@
     if encoderDict["type"] in ["ScalarSpaceEncoder", "AdaptiveScalarEncoder",
                              "ScalarEncoder", "LogEncoder"]:
       permStr = "PermuteEncoder("
-      for key, value in encoderDict.items():
+      for key, value in list(encoderDict.items()):
         if key == "fieldname":
           key = "fieldName"
         elif key == "type":
@@ -695,7 +695,7 @@
           encoderDict.pop("runDelta")
 
         else:
-          if issubclass(type(value), basestring):
+          if issubclass(type(value), str):
             permStr += "%s='%s', " % (key, value)
           else:
             permStr += "%s=%s, " % (key, value)
@@ -704,7 +704,7 @@
     # Category encoder
     elif encoderDict["type"] in ["SDRCategoryEncoder"]:
       permStr = "PermuteEncoder("
-      for key, value in encoderDict.items():
+      for key, value in list(encoderDict.items()):
         if key == "fieldname":
           key = "fieldName"
         elif key == "type":
@@ -712,7 +712,7 @@
         elif key == "name":
           continue
 
-        if issubclass(type(value), basestring):
+        if issubclass(type(value), str):
           permStr += "%s='%s', " % (key, value)
         else:
           permStr += "%s=%s, " % (key, value)
@@ -722,7 +722,7 @@
     # Datetime encoder
     elif encoderDict["type"] in ["DateEncoder"]:
       permStr = "PermuteEncoder("
-      for key, value in encoderDict.items():
+      for key, value in list(encoderDict.items()):
         if key == "fieldname":
           key = "fieldName"
         elif key == "type":
@@ -743,7 +743,7 @@
           permStr += "radius=PermuteChoices([1]),  "
           permStr += "w=%d, " % (value)
         else:
-          if issubclass(type(value), basestring):
+          if issubclass(type(value), str):
             permStr += "%s='%s', " % (key, value)
           else:
             permStr += "%s=%s, " % (key, value)
@@ -986,7 +986,7 @@
     prediction = options.get('prediction', {InferenceType.TemporalNextStep:
                                               {'optimize':True}})
     inferenceType = None
-    for infType, value in prediction.iteritems():
+    for infType, value in prediction.items():
       if value['optimize']:
         inferenceType = infType
         break
@@ -1070,7 +1070,7 @@
   # Validate JSON arg using JSON schema validator
   try:
     validictory.validate(options, _gExperimentDescriptionSchema)
-  except Exception, e:
+  except Exception as e:
     raise _InvalidCommandArgException(
       ("JSON arg validation failed for option --description: " + \
        "%s\nOPTION ARG=%s") % (str(e), pprint.pformat(options)))
@@ -1080,7 +1080,7 @@
                                            'stream_def.json'))
   try:
     validictory.validate(options['streamDef'], streamSchema)
-  except Exception, e:
+  except Exception as e:
     raise _InvalidCommandArgException(
       ("JSON arg validation failed for streamDef " + \
        "%s\nOPTION ARG=%s") % (str(e), json.dumps(options)))
@@ -1227,7 +1227,7 @@
   # Honor any overrides provided in the stream definition
   aggFunctionsDict = {}
   if 'aggregation' in options['streamDef']:
-    for key in aggregationPeriod.keys():
+    for key in list(aggregationPeriod.keys()):
       if key in options['streamDef']['aggregation']:
         aggregationPeriod[key] = options['streamDef']['aggregation'][key]
     if 'fields' in options['streamDef']['aggregation']:
@@ -1236,14 +1236,14 @@
 
   # Do we have any aggregation at all?
   hasAggregation = False
-  for v in aggregationPeriod.values():
+  for v in list(aggregationPeriod.values()):
     if v != 0:
       hasAggregation = True
       break
 
 
   # Convert the aggFunctionsDict to a list
-  aggFunctionList = aggFunctionsDict.items()
+  aggFunctionList = list(aggFunctionsDict.items())
   aggregationInfo = dict(aggregationPeriod)
   aggregationInfo['fields'] = aggFunctionList
 
@@ -1293,7 +1293,7 @@
     # Compute the predictAheadTime
     numSteps = predictionSteps[0]
     predictAheadTime = dict(aggregationPeriod)
-    for key in predictAheadTime.iterkeys():
+    for key in predictAheadTime.keys():
       predictAheadTime[key] *= numSteps
     predictAheadTimeStr = pprint.pformat(predictAheadTime,
                                          indent=2*_INDENT_STEP)
@@ -1516,7 +1516,7 @@
     #  N * M = maxPredictionSteps constraint
     mTimesN = float(predictionSteps[0])
     possibleNs = []
-    for n in xrange(1, int(mTimesN)+1):
+    for n in range(1, int(mTimesN)+1):
       m = mTimesN / n
       mInt = int(round(m))
       if mInt < 1:
@@ -1526,7 +1526,7 @@
       possibleNs.append(n)
 
     if debugAgg:
-      print "All integer factors of %d are: %s" % (mTimesN, possibleNs)
+      print("All integer factors of %d are: %s" % (mTimesN, possibleNs))
 
     # Now go through and throw out any N's that don't satisfy the constraint:
     #  computeInterval = K * (minAggregation * N)
@@ -1534,7 +1534,7 @@
     for n in possibleNs:
       # Compute minAggregation * N
       agg = dict(aggregationPeriod)
-      for key in agg.iterkeys():
+      for key in agg.keys():
         agg[key] *= n
 
       # Make sure computeInterval is an integer multiple of the aggregation
@@ -1552,10 +1552,10 @@
     aggChoices = aggChoices[-5:]
 
     if debugAgg:
-      print "Aggregation choices that will be evaluted during swarming:"
+      print("Aggregation choices that will be evaluted during swarming:")
       for agg in aggChoices:
-        print "  ==>", agg
-      print
+        print("  ==>", agg)
+      print()
 
     tokenReplacements['\$PERM_AGGREGATION_CHOICES'] = (
         "PermuteChoices(%s)" % (
@@ -1591,7 +1591,7 @@
   if not os.path.exists(outputDirPath):
     os.makedirs(outputDirPath)
 
-  print "Generating experiment files in directory: %s..." % (outputDirPath)
+  print("Generating experiment files in directory: %s..." % (outputDirPath))
   descriptionPyPath = os.path.join(outputDirPath, "description.py")
   _generateFileFromTemplates([claDescriptionTemplateFile, controlTemplate],
                               descriptionPyPath,
@@ -1609,10 +1609,9 @@
     _generateFileFromTemplates(['permutationsTemplateV2.tpl'],permutationsPyPath,
                             tokenReplacements)
   else:
-    raise(ValueError("This permutation version is not supported yet: %s" %
-                        hsVersion))
-
-  print "done."
+    raise ValueError
+
+  print("done.")
 
 
 
@@ -1863,7 +1862,7 @@
   results = []
   for metric in options['metrics']:
 
-    for propertyName in _metricSpecSchema['properties'].keys():
+    for propertyName in list(_metricSpecSchema['properties'].keys()):
       _getPropertyValue(_metricSpecSchema, propertyName, metric)
 
 
@@ -2004,8 +2003,7 @@
   # -----------------------------------------------------------------
   # Check for use of mutually-exclusive options
   #
-  activeOptions = filter(lambda x: getattr(options, x) != None,
-                         ('description', 'showSchema'))
+  activeOptions = [x for x in ('description', 'showSchema') if getattr(options, x) != None]
   if len(activeOptions) > 1:
     raise _InvalidCommandArgException(
       _makeUsageErrorStr(("The specified command options are " + \
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch\hs_state.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch\hs_state.py	(refactored)
@@ -123,7 +123,7 @@
       # Fast Swarm, first and only sprint has one swarm for each field
       # in fixedFields
       if self._hsObj._fixedFields is not None:
-        print self._hsObj._fixedFields
+        print(self._hsObj._fixedFields)
         encoderSet = []
         for field in self._hsObj._fixedFields:
             if field =='_classifierInput':
@@ -201,7 +201,7 @@
         #  for easier reference when viewing the state as presented by
         #  log messages and prints of the hsState data structure (by
         #  permutations_runner).
-        activeSwarms = swarms.keys(),
+        activeSwarms = list(swarms.keys()),
 
         # All the swarms that have been created so far.
         swarms = swarms,
@@ -313,7 +313,7 @@
     # -----------------------------------------------------------------------
     # Collect all the single field scores
     fieldScores = []
-    for swarmId, info in self._state['swarms'].iteritems():
+    for swarmId, info in self._state['swarms'].items():
       encodersUsed = swarmId.split('.')
       if len(encodersUsed) != 1:
         continue
@@ -337,7 +337,7 @@
       assert(len(fieldScores)==1)
       (baseErrScore, baseField) = fieldScores[0]
 
-      for swarmId, info in self._state['swarms'].iteritems():
+      for swarmId, info in self._state['swarms'].items():
         encodersUsed = swarmId.split('.')
         if len(encodersUsed) != 2:
           continue
@@ -397,7 +397,7 @@
     retval:   list of active swarm Ids in the given sprint
     """
     swarmIds = []
-    for swarmId, info in self._state['swarms'].iteritems():
+    for swarmId, info in self._state['swarms'].items():
       if info['sprintIdx'] == sprintIdx:
         swarmIds.append(swarmId)
 
@@ -414,7 +414,7 @@
     retval:   list of active swarm Ids in the given sprint
     """
     swarmIds = []
-    for swarmId, info in self._state['swarms'].iteritems():
+    for swarmId, info in self._state['swarms'].items():
       if sprintIdx is not None and info['sprintIdx'] != sprintIdx:
         continue
       if info['status'] == 'active':
@@ -433,7 +433,7 @@
     retval:   list of active swarm Ids in the given sprint
     """
     swarmIds = []
-    for swarmId, info in self._state['swarms'].iteritems():
+    for swarmId, info in self._state['swarms'].items():
       if info['sprintIdx'] == sprintIdx and info['status'] != 'killed':
         swarmIds.append(swarmId)
 
@@ -447,7 +447,7 @@
     retval:   list of active swarm Ids
     """
     swarmIds = []
-    for swarmId, info in self._state['swarms'].iteritems():
+    for swarmId, info in self._state['swarms'].items():
       if info['status'] == 'completed':
         swarmIds.append(swarmId)
 
@@ -461,7 +461,7 @@
     retval:   list of active swarm Ids
     """
     swarmIds = []
-    for swarmId, info in self._state['swarms'].iteritems():
+    for swarmId, info in self._state['swarms'].items():
       if info['status'] == 'completing':
         swarmIds.append(swarmId)
 
@@ -565,7 +565,7 @@
     statusCounts = dict(active=0, completing=0, completed=0, killed=0)
     bestModelIds = []
     bestErrScores = []
-    for info in self._state['swarms'].itervalues():
+    for info in self._state['swarms'].values():
       if info['sprintIdx'] != sprintIdx:
         continue
       statusCounts[info['status']] += 1
@@ -743,7 +743,7 @@
 
     # Mark the bad swarms as killed
     if len(toKill) > 0:
-      print "ParseMe: Killing encoders:" + str(toKill)
+      print("ParseMe: Killing encoders:" + str(toKill))
 
     for swarm in toKill:
       self.setSwarmState(swarm[0], "killed")
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch\object_json.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch\object_json.py	(refactored)
@@ -27,7 +27,7 @@
 import json
 import sys
 
-NON_OBJECT_TYPES = (type(None), bool, int, float, long, str, unicode)
+NON_OBJECT_TYPES = (type(None), bool, int, float, int, str, str)
 
 
 class Types(object):
@@ -46,9 +46,9 @@
 
 def convertDict(obj):
   obj = dict(obj)
-  for k, v in obj.items():
+  for k, v in list(obj.items()):
     del obj[k]
-    if not (isinstance(k, str) or isinstance(k, unicode)):
+    if not (isinstance(k, str) or isinstance(k, str)):
       k = dumps(k)
       # Keep track of which keys need to be decoded when loading.
       if Types.KEYS not in obj:
@@ -67,7 +67,7 @@
         newKey = loads(k)
         obj[newKey] = v
       del obj[Types.KEYS]
-    for k, v in obj.items():
+    for k, v in list(obj.items()):
       if isinstance(v, dict):
         obj[k] = restoreKeysPostDecoding(v)
   elif isinstance(obj, list):
@@ -94,8 +94,8 @@
     if hasattr(obj, '__getstate__'):
       state = obj.__getstate__()
     elif hasattr(obj, '__slots__'):
-      values = map(lambda x: getattr(obj, x), obj.__slots__)
-      state = dict(zip(obj.__slots__, values))
+      values = [getattr(obj, x) for x in obj.__slots__]
+      state = dict(list(zip(obj.__slots__, values)))
     elif hasattr(obj, '__dict__'):
       state = obj.__dict__
     else:
@@ -138,7 +138,7 @@
       if hasattr(instance, '__setstate__'):
         instance.__setstate__(attrs)
       else:
-        for k, v in attrs.iteritems():
+        for k, v in attrs.items():
           setattr(instance, k, v)
       return instance
   return obj
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch\particle.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch\particle.py	(refactored)
@@ -20,7 +20,7 @@
 # ----------------------------------------------------------------------
 
 import logging
-import StringIO
+import io
 import copy
 import pprint
 import random
@@ -141,7 +141,7 @@
       self.permuteVars = copy.deepcopy(flattenedPermuteVars)
 
       # Remove fields we don't want.
-      varNames = self.permuteVars.keys()
+      varNames = list(self.permuteVars.keys())
       for varName in varNames:
         # Remove encoders we're not using
         if ':' in varName:  # if an encoder
@@ -160,7 +160,7 @@
           resultsPerChoice = self._resultsDB.getResultsPerChoice(
             swarmId=self.swarmId, maxGenIdx=maxGenIdx, varName=varName)
           self.permuteVars[varName].setResultsPerChoice(
-            resultsPerChoice.values())
+            list(resultsPerChoice.values()))
 
     # Method #1
     # Create from scratch, optionally pushing away from others that already
@@ -185,7 +185,7 @@
 
       # Push away from other particles?
       if newFarFrom is not None:
-        for varName in self.permuteVars.iterkeys():
+        for varName in self.permuteVars.keys():
           otherPositions = []
           for particleState in newFarFrom:
             otherPositions.append(
@@ -259,7 +259,7 @@
     """Get the particle state as a dict. This is enough information to
     instantiate this particle on another worker."""
     varStates = dict()
-    for varName, var in self.permuteVars.iteritems():
+    for varName, var in self.permuteVars.items():
       varStates[varName] = var.getState()
 
     return dict(id=self.particleId,
@@ -286,7 +286,7 @@
     # Replace with the position and velocity of each variable from
     #  saved state
     varStates = particleState['varStates']
-    for varName in varStates.keys():
+    for varName in list(varStates.keys()):
       varState = copy.deepcopy(varStates[varName])
       if newBest:
         varState['bestResult'] = bestResult
@@ -375,7 +375,7 @@
     retval:     dict() of flattened permutation choices
     """
     result = dict()
-    for (varName, value) in self.permuteVars.iteritems():
+    for (varName, value) in self.permuteVars.items():
       result[varName] = value.getPosition()
 
     return result
@@ -390,7 +390,7 @@
                   values are their positions
     """
     result = dict()
-    for (varName, value) in pState['varStates'].iteritems():
+    for (varName, value) in pState['varStates'].items():
       result[varName] = value['position']
 
     return result
@@ -404,7 +404,7 @@
     --------------------------------------------------------------
     retval:               None
     """
-    for (varName, var) in self.permuteVars.iteritems():
+    for (varName, var) in self.permuteVars.items():
       var.agitate()
 
     self.newPosition()
@@ -440,7 +440,7 @@
         globalBestPosition = Particle.getPositionFromState(particleState)
 
     # Update each variable
-    for (varName, var) in self.permuteVars.iteritems():
+    for (varName, var) in self.permuteVars.items():
       if whichVars is not None and varName not in whichVars:
         continue
       if globalBestPosition is None:
@@ -453,12 +453,12 @@
 
     # Log the new position
     if self.logger.getEffectiveLevel() <= logging.DEBUG:
-      msg = StringIO.StringIO()
-      print >> msg, "New particle position: \n%s" % (pprint.pformat(position,
-                                                                    indent=4))
-      print >> msg, "Particle variables:"
-      for (varName, var) in self.permuteVars.iteritems():
-        print >> msg, "  %s: %s" % (varName, str(var))
+      msg = io.StringIO()
+      print("New particle position: \n%s" % (pprint.pformat(position,
+                                                                    indent=4)), file=msg)
+      print("Particle variables:", file=msg)
+      for (varName, var) in self.permuteVars.items():
+        print("  %s: %s" % (varName, str(var)), file=msg)
       self.logger.debug(msg.getvalue())
       msg.close()
 
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch\permutation_helpers.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch\permutation_helpers.py	(refactored)
@@ -469,7 +469,7 @@
   def __repr__(self):
     """See comments in base class."""
     suffix = ""
-    for key, value in self.kwArgs.items():
+    for key, value in list(self.kwArgs.items()):
       suffix += "%s=%s, " % (key, value)
 
     return "PermuteEncoder(fieldName=%s, encoderClass=%s, name=%s, %s)" % (
@@ -493,7 +493,7 @@
                    name=self.name)
 
     # Get the position of each encoder argument
-    for encoderArg, value in self.kwArgs.iteritems():
+    for encoderArg, value in self.kwArgs.items():
       # If a permuted variable, get its chosen value.
       if isinstance(value, PermuteVariable):
         value = flattenedChosenValues["%s:%s" % (encoderName, encoderArg)]
@@ -538,12 +538,12 @@
     rng.seed(42)
     var = varClass(min=minValue, max=maxValue, stepSize=stepSize,
                        inertia=inertia, cogRate=cogRate, socRate=socRate)
-    for _ in xrange(iterations):
+    for _ in range(iterations):
       pos = var.getPosition()
       if self.verbosity >= 1:
-        print "pos: %f" % (pos),
+        print("pos: %f" % (pos), end=' ')
       if self.verbosity >= 2:
-        print var
+        print(var)
       positions.add(pos)
 
       # Set the result so that the local best is at lBestPosition.
@@ -560,7 +560,7 @@
       var.newPosition(gBestPosition, rng)
 
     positions = sorted(positions)
-    print "Positions visited (%d):" % (len(positions)), positions
+    print("Positions visited (%d):" % (len(positions)), positions)
 
     # Validate positions.
     assert (max(positions) <= maxValue)
@@ -579,12 +579,12 @@
     rng.seed(42)
 
     var = varClass(min=minValue, max=maxValue)
-    for _ in xrange(iterations):
+    for _ in range(iterations):
       pos = var.getPosition()
       if self.verbosity >= 1:
-        print "pos: %f" % (pos),
+        print("pos: %f" % (pos), end=' ')
       if self.verbosity >= 2:
-        print var
+        print(var)
 
       # Set the result so that the local best is at lBestPosition.
       result = 1.0 - abs(pos - lBestPosition)
@@ -600,7 +600,7 @@
       var.newPosition(gBestPosition, rng)
 
     # Test that we reached the target.
-    print "Target: %f, Converged on: %f" % (targetValue, pos)
+    print("Target: %f, Converged on: %f" % (targetValue, pos))
     assert abs(pos-targetValue) < 0.001
 
   def _testChoices(self):
@@ -614,7 +614,7 @@
       counts[pos] += 1
     for count in counts:
       assert count < 270 and count > 230
-    print "No results permuteChoice test passed"
+    print("No results permuteChoice test passed")
 
     # Check that with some results the choices are chosen with the lower
     # errors being chosen more often.
@@ -638,7 +638,7 @@
     for choice in choices:
       assert prevCount > counts[choice]
       prevCount = counts[choice]
-    print "Results permuteChoice test passed"
+    print("Results permuteChoice test passed")
 
     # Check that with fixEarly as you see more data points you begin heavily
     # biasing the probabilities to the one with the lowest error.
@@ -657,7 +657,7 @@
       for choice in choices:
         resultsPerChoiceDict[choice][1].append(float(choice))
         counts[choice] = 0
-      pc.setResultsPerChoice(resultsPerChoiceDict.values())
+      pc.setResultsPerChoice(list(resultsPerChoiceDict.values()))
       rng = random.Random()
       rng.seed(42)
       # Check the without results the choices are chosen uniformly.
@@ -668,7 +668,7 @@
       # seen goes down.
       assert prevLowestErrorCount < counts['1']
       prevLowestErrorCount = counts['1']
-    print "Fix early permuteChoice test passed"
+    print("Fix early permuteChoice test passed")
 
   def run(self):
     """Run unit tests on this module."""
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch\support.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch\support.py	(refactored)
@@ -24,7 +24,7 @@
 Most of it is temporarily copied from nupic.support.
 """
 
-from __future__ import with_statement
+
 
 from copy import copy
 import errno
@@ -90,7 +90,7 @@
     return cls.__labels[label]
 
 
-  for arg in list(args)+kwargs.keys():
+  for arg in list(args)+list(kwargs.keys()):
     if type(arg) is not str:
       raise TypeError("Enum arg {0} must be a string".format(arg))
 
@@ -99,10 +99,10 @@
                        "'{0}' is not a valid identifier".format(arg))
 
   #kwargs.update(zip(args, range(len(args))))
-  kwargs.update(zip(args, args))
+  kwargs.update(list(zip(args, args)))
   newType = type("Enum", (object,), kwargs)
 
-  newType.__labels = dict( (v,k) for k,v in kwargs.iteritems())
+  newType.__labels = dict( (v,k) for k,v in kwargs.items())
   newType.__values = set(newType.__labels.keys())
   newType.getLabel = functools.partial(getLabel, newType)
   newType.validate = functools.partial(validate, newType)
@@ -129,7 +129,7 @@
 
   try:
     os.makedirs(absDirPath)
-  except OSError, e:
+  except OSError as e:
     if e.errno != os.errno.EEXIST:
       raise
 
@@ -297,9 +297,8 @@
     # Make a copy so we can update any current values obtained from environment
     #  variables
     result = dict(cls._properties)
-    keys = os.environ.keys()
-    replaceKeys = filter(lambda x: x.startswith(cls.envPropPrefix),
-                         keys)
+    keys = list(os.environ.keys())
+    replaceKeys = [x for x in keys if x.startswith(cls.envPropPrefix)]
     for envKey in replaceKeys:
       key = envKey[len(cls.envPropPrefix):]
       key = key.replace('_', '.')
@@ -582,7 +581,7 @@
 
     _CustomConfigurationFileWrapper.edit(properties)
 
-    for propertyName, value in properties.iteritems():
+    for propertyName, value in properties.items():
       cls.set(propertyName, value)
 
   @classmethod
@@ -663,7 +662,7 @@
     if persistent:
       try:
         os.unlink(cls.getPath())
-      except OSError, e:
+      except OSError as e:
         if e.errno != errno.ENOENT:
           _getLogger().exception("Error %s while trying to remove dynamic " \
                                  "configuration file: %s", e.errno,
@@ -706,7 +705,7 @@
     try:
       with open(configFilePath, 'r') as fp:
         contents = fp.read()
-    except IOError, e:
+    except IOError as e:
       if e.errno != errno.ENOENT:
         _getLogger().exception("Error %s reading custom configuration store "
                                "from %s, while editing properties %s.",
@@ -717,14 +716,14 @@
     try:
       elements = ElementTree.XML(contents)
       ElementTree.tostring(elements)
-    except Exception, e:
+    except Exception as e:
       # Raising error as RuntimeError with custom message since ElementTree
       # exceptions aren't clear.
       msg = "File contents of custom configuration is corrupt.  File " \
             "location: %s; Contents: '%s'. Original Error (%s): %s." % \
             (configFilePath, contents, type(e), e)
       _getLogger().exception(msg)
-      raise RuntimeError(msg), None, sys.exc_info()[2]
+      raise RuntimeError(msg).with_traceback(sys.exc_info()[2])
 
     if elements.tag != 'configuration':
       e = "Expected top-level element to be 'configuration' but got '%s'" % \
@@ -749,7 +748,7 @@
           raise RuntimeError(e)
 
     # Add unmatched remaining properties to custom config store
-    for propertyName, value in copyOfProperties.iteritems():
+    for propertyName, value in copyOfProperties.items():
       newProp = ElementTree.Element('property')
       nameTag = ElementTree.Element('name')
       nameTag.text = propertyName
@@ -765,7 +764,7 @@
       makeDirectoryFromAbsolutePath(os.path.dirname(configFilePath))
       with open(configFilePath, 'w') as fp:
         fp.write(ElementTree.tostring(elements))
-    except Exception, e:
+    except Exception as e:
       _getLogger().exception("Error while saving custom configuration "
                              "properties %s in %s.", properties,
                              configFilePath)
--- d:\nupic\src\python\python27\nupic\swarming\hypersearch\swarm_terminator.py	(original)
+++ d:\nupic\src\python\python27\nupic\swarming\hypersearch\swarm_terminator.py	(refactored)
@@ -37,7 +37,7 @@
   """
   MATURITY_WINDOW = None
   MAX_GENERATIONS = None
-  _DEFAULT_MILESTONES = [1.0 / (x + 1) for x in xrange(12)]
+  _DEFAULT_MILESTONES = [1.0 / (x + 1) for x in range(12)]
 
   def __init__(self, milestones=None, logLevel=None):
     # Set class constants.
@@ -126,7 +126,7 @@
   def _getTerminatedSwarms(self, generation):
     terminatedSwarms = []
     generationScores = dict()
-    for swarm, scores in self.swarmScores.iteritems():
+    for swarm, scores in self.swarmScores.items():
       if len(scores) > generation and swarm not in self.terminatedSwarms:
         generationScores[swarm] = scores[generation]
 
@@ -136,7 +136,7 @@
     bestScore = min(generationScores.values())
     tolerance = self.milestones[generation]
 
-    for swarm, score in generationScores.iteritems():
+    for swarm, score in generationScores.items():
       if score > (1 + tolerance) * bestScore:
         self._logger.info('Swarm %s is doing poorly at generation %d.\n'
                           'Current Score:%s \n'
--- d:\nupic\src\python\python27\scripts\run_nupic_tests.py	(original)
+++ d:\nupic\src\python\python27\scripts\run_nupic_tests.py	(refactored)
@@ -34,9 +34,9 @@
 try:
   pytestXdistAvailable = bool(get_distribution("pytest-xdist"))
 except DistributionNotFound:
-  print "ERROR: `pytest-xdist` is not installed.  Certain testing features" \
+  print("ERROR: `pytest-xdist` is not installed.  Certain testing features" \
     " are not available without it.  The complete list of python" \
-    " requirements can be found in requirements.txt."
+    " requirements can be found in requirements.txt.")
   sys.exit(1)
 
 
--- d:\nupic\src\python\python27\scripts\temporal_memory_performance_benchmark.py	(original)
+++ d:\nupic\src\python\python27\scripts\temporal_memory_performance_benchmark.py	(refactored)
@@ -43,12 +43,12 @@
   numberOfDots = lambda n: (n * nDots) // total
   completedDots = numberOfDots(completed)
   if completedDots != numberOfDots(completed - 1):
-    print "\r|" + ("." * completedDots) + (" " * (nDots - completedDots)) + "|",
+    print("\r|" + ("." * completedDots) + (" " * (nDots - completedDots)) + "|", end=' ')
     sys.stdout.flush()
 
 
 def clearProgressBar(nDots):
-  print "\r" + (" " * (nDots + 2))
+  print("\r" + (" " * (nDots + 2)))
 
 
 class TemporalMemoryPerformanceBenchmark(object):
@@ -68,7 +68,7 @@
   def _createInstances(self, cellsPerColumn):
     instances = []
 
-    for i in xrange(len(self.contestants)):
+    for i in range(len(self.contestants)):
       (constructor,
        paramsFn,
        computeFn,
@@ -93,7 +93,7 @@
     increment = 4
     sequenceLength = 25
     sequence = (i % (sequenceLength * 4)
-                for i in xrange(0, duration * increment, increment))
+                for i in range(0, duration * increment, increment))
     t = 0
 
     encodedValue = numpy.zeros(2048, dtype=numpy.int32)
@@ -102,7 +102,7 @@
       scalarEncoder.encodeIntoArray(value, output=encodedValue)
       activeBits = encodedValue.nonzero()[0]
 
-      for i in xrange(len(self.contestants)):
+      for i in range(len(self.contestants)):
         tmInstance = instances[i]
         computeFn = self.contestants[i][2]
 
@@ -120,7 +120,7 @@
     clearProgressBar(50)
 
     results = []
-    for i in xrange(len(self.contestants)):
+    for i in range(len(self.contestants)):
       name = self.contestants[i][3]
       results.append((name,
                       times[i],))
@@ -137,12 +137,12 @@
     t = 0
     duration = HOTGYM_LENGTH * repetitions
 
-    for _ in xrange(repetitions):
+    for _ in range(repetitions):
       with open(HOTGYM_PATH) as fin:
         reader = csv.reader(fin)
-        reader.next()
-        reader.next()
-        reader.next()
+        next(reader)
+        next(reader)
+        next(reader)
 
         encodedValue = numpy.zeros(2048, dtype=numpy.uint32)
 
@@ -151,7 +151,7 @@
           scalarEncoder.encodeIntoArray(value, output=encodedValue)
           activeBits = encodedValue.nonzero()[0]
 
-          for i in xrange(len(self.contestants)):
+          for i in range(len(self.contestants)):
             tmInstance = instances[i]
             computeFn = self.contestants[i][2]
 
@@ -165,7 +165,7 @@
     clearProgressBar(50)
 
     results = []
-    for i in xrange(len(self.contestants)):
+    for i in range(len(self.contestants)):
       name = self.contestants[i][3]
       results.append((name,
                       times[i],))
@@ -184,12 +184,12 @@
 
     encodedValue = numpy.zeros(2048, dtype=numpy.int32)
 
-    for _ in xrange(duration):
-      activeBits = random.sample(xrange(2048), 40)
+    for _ in range(duration):
+      activeBits = random.sample(range(2048), 40)
       encodedValue = numpy.zeros(2048, dtype=numpy.int32)
       encodedValue[activeBits] = 1
 
-      for i in xrange(len(self.contestants)):
+      for i in range(len(self.contestants)):
         tmInstance = instances[i]
         computeFn = self.contestants[i][2]
 
@@ -203,7 +203,7 @@
     clearProgressBar(50)
 
     results = []
-    for i in xrange(len(self.contestants)):
+    for i in range(len(self.contestants)):
       name = self.contestants[i][3]
       results.append((name,
                       times[i],))
@@ -373,31 +373,31 @@
     assert name not in allResults
 
     if name in args.tests:
-      print "Test: %s" % description
+      print("Test: %s" % description)
       if args.pause:
-        raw_input("Press enter to continue. ")
+        input("Press enter to continue. ")
 
       results = testFn()
       allResults[name] = results
 
       for implDescription, t in sorted(results, key=lambda x: x[1]):
-        print "%s: %fs" % (implDescription, t)
-      print
-      print
+        print("%s: %fs" % (implDescription, t))
+      print()
+      print()
 
   if args.output is not None and len(allResults) > 0:
-    print "Writing results to",args.output
-    print
+    print("Writing results to",args.output)
+    print()
     with open(args.output, "wb") as csvFile:
       writer = csv.writer(csvFile)
-      firstTestName, firstResults = allResults.iteritems().next()
+      firstTestName, firstResults = next(iter(allResults.items()))
       orderedImplNames = (implName for implName, t in firstResults)
 
       firstRow = ["test"]
       firstRow.extend(orderedImplNames)
       writer.writerow(firstRow)
 
-      for testName, results in allResults.iteritems():
+      for testName, results in allResults.items():
         row = [testName]
         for implDescription, t in results:
           row.append(t)
--- d:\nupic\src\python\python27\scripts\profiling\enc_profile.py	(original)
+++ d:\nupic\src\python\python27\scripts\profiling\enc_profile.py	(refactored)
@@ -42,7 +42,7 @@
     encScalar.encode(d)
     encRDSE.encode(d)
 
-  print "Scalar n=",encScalar.n," RDSE n=",encRDSE.n
+  print("Scalar n=",encScalar.n," RDSE n=",encRDSE.n)
 
 
 
--- d:\nupic\src\python\python27\scripts\profiling\profile_opf_memory.py	(original)
+++ d:\nupic\src\python\python27\scripts\profiling\profile_opf_memory.py	(refactored)
@@ -49,7 +49,7 @@
       continue
     history.append((time.time() - start, mem))
 
-  print 'Max memory: ', max([a[1] for a in history])
+  print('Max memory: ', max([a[1] for a in history]))
 
 
 
--- d:\nupic\src\python\python27\scripts\profiling\sp_profile.py	(original)
+++ d:\nupic\src\python\python27\scripts\profiling\sp_profile.py	(refactored)
@@ -69,7 +69,7 @@
   dataDim.append(nRuns)
   data = numpy.random.randint(0, 2, dataDim).astype('float32')
 
-  for i in xrange(nRuns):
+  for i in range(nRuns):
     # new data every time, this is the worst case performance
     # real performance would be better, as the input data would not be completely random
     d = data[:,:,:,i]
--- d:\nupic\src\python\python27\scripts\profiling\tm_profile.py	(original)
+++ d:\nupic\src\python\python27\scripts\profiling\tm_profile.py	(refactored)
@@ -45,7 +45,7 @@
   # generate input data
   data = numpy.random.randint(0, 2, [tmDim, nRuns]).astype('float32')
 
-  for i in xrange(nRuns):
+  for i in range(nRuns):
     # new data every time, this is the worst case performance
     # real performance would be better, as the input data would not be completely random
     d = data[:,i]
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\extensive_tm_cpp_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\extensive_tm_cpp_test.py	(refactored)
@@ -22,7 +22,7 @@
 import unittest
 
 import nupic.bindings.algorithms
-from extensive_tm_test_base import ExtensiveTemporalMemoryTest
+from .extensive_tm_test_base import ExtensiveTemporalMemoryTest
 
 
 
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\extensive_tm_py_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\extensive_tm_py_test.py	(refactored)
@@ -22,7 +22,7 @@
 import unittest
 
 import nupic.algorithms.temporal_memory
-from extensive_tm_test_base import ExtensiveTemporalMemoryTest
+from .extensive_tm_test_base import ExtensiveTemporalMemoryTest
 
 
 
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\extensive_tm_test_base.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\extensive_tm_test_base.py	(refactored)
@@ -29,7 +29,7 @@
 
 
 
-class ExtensiveTemporalMemoryTest(AbstractTemporalMemoryTest):
+class ExtensiveTemporalMemoryTest(AbstractTemporalMemoryTest, metaclass=ABCMeta):
   """
   ==============================================================================
                   Basic First Order Sequences
@@ -188,12 +188,11 @@
   when presented with the second A and B is different from the representation
   in the first presentation. [TODO]
   """
-  __metaclass__ = ABCMeta
 
   VERBOSITY = 1
 
   def getPatternMachine(self):
-    return PatternMachine(100, range(21, 26), num=300)
+    return PatternMachine(100, list(range(21, 26)), num=300)
 
   def getDefaultTMParams(self):
     return {
@@ -301,7 +300,7 @@
     numbers = self.sequenceMachine.generateNumbers(1, 100)
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
-    for _ in xrange(4):
+    for _ in range(4):
       self.feedTM(sequence)
 
     self._testTM(sequence)
@@ -320,7 +319,7 @@
     numbers = self.sequenceMachine.generateNumbers(1, 100)
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
-    for _ in xrange(4):
+    for _ in range(4):
       self.feedTM(sequence)
 
     self._testTM(sequence)
@@ -338,7 +337,7 @@
     numbers = self.sequenceMachine.generateNumbers(1, 100)
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
-    for _ in xrange(3):
+    for _ in range(3):
       self.feedTM(sequence)
 
     self._testTM(sequence)
@@ -400,7 +399,7 @@
     numbers = self.sequenceMachine.generateNumbers(2, 20, (10, 15))
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
-    for _ in xrange(10):
+    for _ in range(10):
       self.feedTM(sequence)
 
     self._testTM(sequence)
@@ -453,12 +452,12 @@
     self.init({"cellsPerColumn": 4})
 
     numbers = []
-    for _ in xrange(2):
+    for _ in range(2):
       numbers += self.sequenceMachine.generateNumbers(1, 20)
 
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
-    for _ in xrange(20):
+    for _ in range(20):
       self.feedTM(sequence)
 
     self._testTM(sequence)
@@ -476,14 +475,14 @@
 
     numbers = []
     shared = self.sequenceMachine.generateNumbers(1, 5)[:-1]
-    for _ in xrange(2):
+    for _ in range(2):
       sublist = self.sequenceMachine.generateNumbers(1, 20)
-      sublist = [x for x in sublist if x not in xrange(5)]
+      sublist = [x for x in sublist if x not in range(5)]
       numbers += sublist[0:10] + shared + sublist[10:]
 
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
-    for _ in xrange(20):
+    for _ in range(20):
       self.feedTM(sequence)
 
     self._testTM(sequence)
@@ -504,7 +503,7 @@
     numbers = self.sequenceMachine.generateNumbers(2, 20, (10, 15))
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
-    for _ in xrange(10):
+    for _ in range(10):
       self.feedTM(sequence)
 
     sequence = self.sequenceMachine.addSpatialNoise(sequence, 0.05)
@@ -530,7 +529,7 @@
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
 
     sequenceNoisy = dict()
-    for i in xrange(10):
+    for i in range(10):
       sequenceNoisy[i] = self.sequenceMachine.addSpatialNoise(sequence, 0.05)
       self.feedTM(sequenceNoisy[i])
     self.tm.mmClearHistory()
@@ -551,7 +550,7 @@
                "minThreshold": 8,
                "predictedSegmentDecrement": 0.04})
 
-    for i in xrange(10):
+    for i in range(10):
       self.feedTM(sequenceNoisy[i])
     self.tm.mmClearHistory()
 
@@ -576,12 +575,12 @@
   def setUp(self):
     super(ExtensiveTemporalMemoryTest, self).setUp()
 
-    print ("\n"
+    print(("\n"
            "======================================================\n"
            "Test: {0} \n"
            "{1}\n"
            "======================================================\n"
-    ).format(self.id(), self.shortDescription())
+    ).format(self.id(), self.shortDescription()))
 
 
   def feedTM(self, sequence, learn=True, num=1):
@@ -589,12 +588,12 @@
       sequence, learn=learn, num=num)
 
     if self.VERBOSITY >= 2:
-      print self.tm.mmPrettyPrintTraces(
-        self.tm.mmGetDefaultTraces(verbosity=self.VERBOSITY-1))
-      print
+      print(self.tm.mmPrettyPrintTraces(
+        self.tm.mmGetDefaultTraces(verbosity=self.VERBOSITY-1)))
+      print()
 
     if learn and self.VERBOSITY >= 3:
-      print self.tm.mmPrettyPrintConnections()
+      print(self.tm.mmPrettyPrintConnections())
 
 
   # ==============================
@@ -604,7 +603,7 @@
   def _testTM(self, sequence):
     self.feedTM(sequence, learn=False)
 
-    print self.tm.mmPrettyPrintMetrics(self.tm.mmGetDefaultMetrics())
+    print(self.tm.mmPrettyPrintMetrics(self.tm.mmGetDefaultMetrics()))
 
 
   def assertAllActiveWerePredicted(self):
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_likelihood_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_likelihood_test.py	(refactored)
@@ -65,7 +65,7 @@
 
   numCols = numOnes * numPatterns
   p = []
-  for i in xrange(numPatterns):
+  for i in range(numPatterns):
     x = numpy.zeros(numCols, dtype='float32')
     x[i*numOnes:(i+1)*numOnes] = 1
     p.append(x)
@@ -102,7 +102,7 @@
   globalDecay = 0.0
 
   if VERBOSITY > 1:
-    print "Creating BacktrackingTMCPP instance"
+    print("Creating BacktrackingTMCPP instance")
 
   cppTm = BacktrackingTMCPP(numberOfCols=numCols, cellsPerColumn=cellsPerColumn,
                             initialPerm=initialPerm, connectedPerm=connectedPerm,
@@ -115,7 +115,7 @@
                             pamLength=1000)
 
   if VERBOSITY > 1:
-    print "Creating PY TM instance"
+    print("Creating PY TM instance")
 
   pyTm = BacktrackingTM(numberOfCols=numCols, cellsPerColumn=cellsPerColumn,
                         initialPerm=initialPerm, connectedPerm=connectedPerm,
@@ -163,11 +163,11 @@
 
     seq = numpy.array(seq, dtype='uint32')
     if verbosity > 2:
-      print "--------------------------------------------------------"
+      print("--------------------------------------------------------")
     for i, inputPattern in enumerate(seq):
       if verbosity > 2:
-        print "sequence %d, element %d," % (seqIdx, i),
-        print "pattern", inputPattern
+        print("sequence %d, element %d," % (seqIdx, i), end=' ')
+        print("pattern", inputPattern)
 
 
       # Feed this input to the TM and get the stats
@@ -176,26 +176,26 @@
       if verbosity > 2:
         stats = tm.getStats()
         if stats['curPredictionScore'] > 0:
-          print "   patternConfidence=", stats['curPredictionScore2']
+          print("   patternConfidence=", stats['curPredictionScore2'])
 
 
       # Print some diagnostics for debugging
       if verbosity > 3:
-        print "\n\n"
+        print("\n\n")
         predOut = numpy.sum(tm.predictedState['t'], axis=1)
         actOut  = numpy.sum(tm.activeState['t'], axis=1)
         outout  = numpy.sum(y.reshape(tm.activeState['t'].shape), axis=1)
-        print "Prediction non-zeros: ", predOut.nonzero()
-        print "Activestate non-zero: ", actOut.nonzero()
-        print "input non-zeros:      ", inputPattern.nonzero()
-        print "Output non-zeros:     ", outout.nonzero()
+        print("Prediction non-zeros: ", predOut.nonzero())
+        print("Activestate non-zero: ", actOut.nonzero())
+        print("input non-zeros:      ", inputPattern.nonzero())
+        print("Output non-zeros:     ", outout.nonzero())
 
   # Print and return final stats
   stats = tm.getStats()
   datasetScore = stats['predictionScoreAvg2']
   numPredictions = stats['nPredictions']
-  print "Final results: datasetScore=", datasetScore,
-  print "numPredictions=", numPredictions
+  print("Final results: datasetScore=", datasetScore, end=' ')
+  print("numPredictions=", numPredictions)
 
   return datasetScore, numPredictions
 
@@ -207,7 +207,7 @@
 
   dataSet = []
   trainingCummulativeFrequencies = numpy.cumsum(relativeFrequencies)
-  for _ in xrange(numSequences):
+  for _ in range(numSequences):
     # Pick a training sequence to present, based on the given training
     # frequencies.
     whichSequence = numpy.searchsorted(trainingCummulativeFrequencies,
@@ -279,9 +279,9 @@
 
     # Learn
     if VERBOSITY > 1:
-      print "============= Learning ================="
-
-    for r in xrange(nSequencePresentations):
+      print("============= Learning =================")
+
+    for r in range(nSequencePresentations):
 
       # Pick a training sequence to present, based on the given training
       # frequencies.
@@ -290,34 +290,34 @@
       trainingSequence = trainingSequences[whichSequence]
 
       if VERBOSITY > 2:
-        print "=========Presentation #%d Sequence #%d==============" % \
-                                              (r, whichSequence)
+        print("=========Presentation #%d Sequence #%d==============" % \
+                                              (r, whichSequence))
       if doResets:
         tm.reset()
       for t, x in enumerate(trainingSequence):
         if VERBOSITY > 3:
-          print "Time step", t
-          print "Input: ", tm.printInput(x)
+          print("Time step", t)
+          print("Input: ", tm.printInput(x))
         tm.learn(x)
         if VERBOSITY > 4:
           tm.printStates(printPrevious=(VERBOSITY > 4))
-          print
+          print()
       if VERBOSITY > 4:
-        print "Sequence finished. Complete state after sequence"
+        print("Sequence finished. Complete state after sequence")
         tm.printCells()
-        print
+        print()
 
     tm.finishLearning()
     if VERBOSITY > 2:
-      print "Training completed. Complete state:"
+      print("Training completed. Complete state:")
       tm.printCells()
-      print
-      print "TM parameters:"
-      print tm.printParameters()
+      print()
+      print("TM parameters:")
+      print(tm.printParameters())
 
     # Infer
     if VERBOSITY > 1:
-      print "============= Inference ================="
+      print("============= Inference =================")
 
     testSequence = testSequences[0]
     slen = len(testSequence)
@@ -327,11 +327,11 @@
       tm.reset()
     for t, x in enumerate(testSequence):
       if VERBOSITY > 2:
-        print "Time step", t, '\nInput:', tm.printInput(x)
+        print("Time step", t, '\nInput:', tm.printInput(x))
       tm.infer(x)
       if VERBOSITY > 3:
         tm.printStates(printPrevious=(VERBOSITY > 4), printLearnState=False)
-        print
+        print()
 
       # We will exit with the confidence score for the last element
       if t == slen-2:
@@ -339,7 +339,7 @@
         predictionScore2 = tm._checkPrediction(tmNonZeros)[2]
 
     if VERBOSITY > 0:
-      print "predictionScore:", predictionScore2
+      print("predictionScore:", predictionScore2)
 
     # The following test tests that the prediction scores for each pattern
     # are within 10% of the its relative frequency.  Here we check only
@@ -363,8 +363,8 @@
   def _likelihoodTest1(self, numOnes=5, relativeFrequencies=None,
                        checkSynapseConsistency=True):
 
-    print "Sequence Likelihood test 1 with relativeFrequencies=",
-    print relativeFrequencies
+    print("Sequence Likelihood test 1 with relativeFrequencies=", end=' ')
+    print(relativeFrequencies)
 
     trainingSet = _buildLikelihoodTrainingSet(numOnes, relativeFrequencies)
     cppTm, pyTm = _createTMs(numCols=trainingSet[0][0][0].size,
@@ -379,8 +379,8 @@
 
   def _likelihoodTest2(self, numOnes=5, relativeFrequencies=None,
                        checkSynapseConsistency=True):
-    print "Sequence Likelihood test 2 with relativeFrequencies=",
-    print relativeFrequencies
+    print("Sequence Likelihood test 2 with relativeFrequencies=", end=' ')
+    print(relativeFrequencies)
 
     trainingSet = _buildLikelihoodTrainingSet(numOnes, relativeFrequencies)
 
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_overlapping_sequences_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_overlapping_sequences_test.py	(refactored)
@@ -63,13 +63,13 @@
 
 def printOneTrainingVector(x):
   "Print a single vector succinctly."
-  print ''.join('1' if k != 0 else '.' for k in x)
+  print(''.join('1' if k != 0 else '.' for k in x))
 
 
 
 def printAllTrainingSequences(trainingSequences, upTo = 99999):
   for i,trainingSequence in enumerate(trainingSequences):
-    print "============= Sequence",i,"================="
+    print("============= Sequence",i,"=================")
     for j,pattern in enumerate(trainingSequence):
       printOneTrainingVector(pattern)
 
@@ -95,7 +95,7 @@
   numCols = numNewBitsInEachPattern * numPatterns + patternOverlap
 
   p = []
-  for i in xrange(numPatterns):
+  for i in range(numPatterns):
     x = numpy.zeros(numCols, dtype='float32')
 
     startBit = i*numNewBitsInEachPattern
@@ -148,16 +148,16 @@
   # Create the training sequences
   trainingSequences = []
 
-  uniquePatternIndices = range(numSharedElements, numPatterns)
-  for i in xrange(numSequences):
+  uniquePatternIndices = list(range(numSharedElements, numPatterns))
+  for i in range(numSequences):
     sequence = []
 
     # pattern indices [0 ... numSharedElements-1] are reserved for the shared
     #  middle
-    sharedPatternIndices = range(numSharedElements)
+    sharedPatternIndices = list(range(numSharedElements))
 
     # Build up the sequence
-    for j in xrange(seqLen):
+    for j in range(seqLen):
       if j in sharedElements:
         patIdx = sharedPatternIndices.pop(0)
       else:
@@ -168,7 +168,7 @@
 
 
   if VERBOSITY >= 3:
-    print "\nTraining sequences"
+    print("\nTraining sequences")
     printAllTrainingSequences(trainingSequences)
 
   return (numCols, trainingSequences)
@@ -210,13 +210,13 @@
   # -----------------------------------------------------------------------
   # Create the training sequences
   trainingSequences = []
-  for i in xrange(numSequences):
+  for i in range(numSequences):
 
     # Build it up from patterns
     sequence = []
     length = random.choice(seqLen)
-    for j in xrange(length):
-      patIdx = random.choice(xrange(numPatterns))
+    for j in range(length):
+      patIdx = random.choice(range(numPatterns))
       sequence.append(patterns[patIdx])
 
     # Put it in
@@ -224,7 +224,7 @@
 
 
   if VERBOSITY >= 3:
-    print "\nTraining sequences"
+    print("\nTraining sequences")
     printAllTrainingSequences(trainingSequences)
 
   return (numCols, trainingSequences)
@@ -264,7 +264,7 @@
 
   if includeCPP:
     if VERBOSITY >= 2:
-      print "Creating BacktrackingTMCPP instance"
+      print("Creating BacktrackingTMCPP instance")
 
     cpp_tm = BacktrackingTMCPP(numberOfCols = numCols, cellsPerColumn = cellsPerCol,
                                initialPerm = initialPerm, connectedPerm = connectedPerm,
@@ -288,7 +288,7 @@
 
   if includePy:
     if VERBOSITY >= 2:
-      print "Creating PY TM instance"
+      print("Creating PY TM instance")
 
     py_tm = BacktrackingTM(numberOfCols = numCols, cellsPerColumn = cellsPerCol,
                            initialPerm = initialPerm, connectedPerm = connectedPerm,
@@ -325,7 +325,7 @@
   if len(tms) > 2:
     raise "Not implemented for more than 2 TMs"
 
-  same = fdrutils.tmDiff2(*tms.values(), verbosity=VERBOSITY)
+  same = fdrutils.tmDiff2(*list(tms.values()), verbosity=VERBOSITY)
   assert(same)
   return
 
@@ -358,23 +358,23 @@
 
   # First TM instance is used by default for verbose printing of input values,
   #  etc.
-  firstTP = tms.values()[0]
+  firstTP = list(tms.values())[0]
 
   assertNoTMDiffs(tms)
 
   # =====================================================================
   # Loop through the training set nTrainRepetitions times
   # ==========================================================================
-  for trainingNum in xrange(nTrainRepetitions):
+  for trainingNum in range(nTrainRepetitions):
     if VERBOSITY >= 2:
-      print "\n##############################################################"
-      print "################# Training round #%d of %d #################" \
-                % (trainingNum, nTrainRepetitions)
-      for (name,tm) in tms.iteritems():
-        print "TM parameters for %s: " % (name)
-        print "---------------------"
+      print("\n##############################################################")
+      print("################# Training round #%d of %d #################" \
+                % (trainingNum, nTrainRepetitions))
+      for (name,tm) in tms.items():
+        print("TM parameters for %s: " % (name))
+        print("---------------------")
         tm.printParameters()
-        print
+        print()
 
     # ======================================================================
     # Loop through the sequences in the training set
@@ -383,11 +383,11 @@
       numTimeSteps = len(trainingSequence)
 
       if VERBOSITY >= 2:
-        print "\n================= Sequence #%d of %d ================" \
-                  % (sequenceNum, numSequences)
+        print("\n================= Sequence #%d of %d ================" \
+                  % (sequenceNum, numSequences))
 
       if doResets:
-        for tm in tms.itervalues():
+        for tm in tms.values():
           tm.reset()
 
       # --------------------------------------------------------------------
@@ -396,62 +396,62 @@
 
         # Print Verbose info about this element
         if VERBOSITY >= 2:
-          print
+          print()
           if VERBOSITY >= 3:
-            print "------------------------------------------------------------"
-          print "--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
-                  % (sequenceNum, numSequences, t, numTimeSteps)
+            print("------------------------------------------------------------")
+          print("--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
+                  % (sequenceNum, numSequences, t, numTimeSteps))
           firstTP.printInput(x)
-          print "input nzs:", x.nonzero()
+          print("input nzs:", x.nonzero())
 
         # Train in this element
         x = numpy.array(x).astype('float32')
-        for tm in tms.itervalues():
+        for tm in tms.values():
           tm.learn(x, enableInference=True)
 
         # Print the input and output states
         if VERBOSITY >= 3:
-          for (name,tm) in tms.iteritems():
-            print "I/O states of %s TM:" % (name)
-            print "-------------------------------------",
+          for (name,tm) in tms.items():
+            print("I/O states of %s TM:" % (name))
+            print("-------------------------------------", end=' ')
             tm.printStates(printPrevious = (VERBOSITY >= 5))
-            print
+            print()
 
         assertNoTMDiffs(tms)
 
         # Print out number of columns that weren't predicted
         if VERBOSITY >= 2:
-          for (name,tm) in tms.iteritems():
+          for (name,tm) in tms.items():
             stats = tm.getStats()
-            print "# of unpredicted columns for %s TM: %d of %d" \
-                % (name, stats['curMissing'], x.sum())
+            print("# of unpredicted columns for %s TM: %d of %d" \
+                % (name, stats['curMissing'], x.sum()))
             numBurstingCols = tm.infActiveState['t'].min(axis=1).sum()
-            print "# of bursting columns for %s TM: %d of %d" \
-                % (name, numBurstingCols, x.sum())
+            print("# of bursting columns for %s TM: %d of %d" \
+                % (name, numBurstingCols, x.sum()))
 
 
       # Print the trained cells
       if VERBOSITY >= 4:
-        print "Sequence %d finished." % (sequenceNum)
-        for (name,tm) in tms.iteritems():
-          print "All cells of %s TM:" % (name)
-          print "-------------------------------------",
+        print("Sequence %d finished." % (sequenceNum))
+        for (name,tm) in tms.items():
+          print("All cells of %s TM:" % (name))
+          print("-------------------------------------", end=' ')
           tm.printCells()
-          print
+          print()
 
     # --------------------------------------------------------------------
     # Done training all sequences in this round, print the total number of
     #  missing, extra columns and make sure it's the same among the TMs
     if VERBOSITY >= 2:
-      print
+      print()
     prevResult = None
-    for (name,tm) in tms.iteritems():
+    for (name,tm) in tms.items():
       stats = tm.getStats()
       if VERBOSITY >= 1:
-        print "Stats for %s TM over all sequences for training round #%d of %d:" \
-                % (name, trainingNum, nTrainRepetitions)
-        print "   total missing:", stats['totalMissing']
-        print "   total extra:", stats['totalExtra']
+        print("Stats for %s TM over all sequences for training round #%d of %d:" \
+                % (name, trainingNum, nTrainRepetitions))
+        print("   total missing:", stats['totalMissing'])
+        print("   total extra:", stats['totalExtra'])
 
       if prevResult is None:
         prevResult = (stats['totalMissing'], stats['totalExtra'])
@@ -465,9 +465,9 @@
   # =====================================================================
   # Finish up learning
   if VERBOSITY >= 3:
-    print "Calling trim segments"
+    print("Calling trim segments")
   prevResult = None
-  for tm in tms.itervalues():
+  for tm in tms.values():
     nSegsRemoved, nSynsRemoved = tm.trimSegments()
     if prevResult is None:
       prevResult = (nSegsRemoved, nSynsRemoved)
@@ -478,22 +478,22 @@
   assertNoTMDiffs(tms)
 
   if VERBOSITY >= 4:
-    print "Training completed. Complete state:"
-    for (name,tm) in tms.iteritems():
-      print "%s:" % (name)
+    print("Training completed. Complete state:")
+    for (name,tm) in tms.items():
+      print("%s:" % (name))
       tm.printCells()
-      print
+      print()
 
 
   # ==========================================================================
   # Infer
   # ==========================================================================
   if VERBOSITY >= 2:
-    print "\n##############################################################"
-    print "########################## Inference #########################"
+    print("\n##############################################################")
+    print("########################## Inference #########################")
 
   # Reset stats in all TMs
-  for tm in tms.itervalues():
+  for tm in tms.values():
     tm.resetStats()
 
   # -------------------------------------------------------------------
@@ -504,12 +504,12 @@
 
     # Identify this sequence
     if VERBOSITY >= 2:
-      print "\n================= Sequence %d of %d ================" \
-                % (sequenceNum, numSequences)
+      print("\n================= Sequence %d of %d ================" \
+                % (sequenceNum, numSequences))
 
     # Send in the rest
     if doResets:
-      for tm in tms.itervalues():
+      for tm in tms.values():
         tm.reset()
 
     # -------------------------------------------------------------------
@@ -518,66 +518,66 @@
 
       # Print verbose info about this element
       if VERBOSITY >= 2:
-        print
+        print()
         if VERBOSITY >= 3:
-          print "------------------------------------------------------------"
-        print "--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
-                % (sequenceNum, numSequences, t, numTimeSteps)
+          print("------------------------------------------------------------")
+        print("--------- sequence: #%d of %d, timeStep: #%d of %d -----------" \
+                % (sequenceNum, numSequences, t, numTimeSteps))
         firstTP.printInput(x)
-        print "input nzs:", x.nonzero()
+        print("input nzs:", x.nonzero())
 
       # Infer on this element
-      for tm in tms.itervalues():
+      for tm in tms.values():
         tm.infer(x)
 
       assertNoTMDiffs(tms)
 
       # Print out number of columns that weren't predicted
       if VERBOSITY >= 2:
-        for (name,tm) in tms.iteritems():
+        for (name,tm) in tms.items():
           stats = tm.getStats()
-          print "# of unpredicted columns for %s TM: %d of %d" \
-              % (name, stats['curMissing'], x.sum())
+          print("# of unpredicted columns for %s TM: %d of %d" \
+              % (name, stats['curMissing'], x.sum()))
 
       # Debug print of internal state
       if VERBOSITY >= 3:
-        for (name,tm) in tms.iteritems():
-          print "I/O states of %s TM:" % (name)
-          print "-------------------------------------",
+        for (name,tm) in tms.items():
+          print("I/O states of %s TM:" % (name))
+          print("-------------------------------------", end=' ')
           tm.printStates(printPrevious = (VERBOSITY >= 5),
                          printLearnState = False)
-          print
+          print()
 
     # Done with this sequence
     # Debug print of all stats of the TMs
     if VERBOSITY >= 4:
-      print
-      for (name,tm) in tms.iteritems():
-        print "Interim internal stats for %s TM:" % (name)
-        print "---------------------------------"
+      print()
+      for (name,tm) in tms.items():
+        print("Interim internal stats for %s TM:" % (name))
+        print("---------------------------------")
         pprint.pprint(tm.getStats())
-        print
+        print()
 
 
   if VERBOSITY >= 2:
-    print "\n##############################################################"
-    print "####################### Inference Done #######################"
+    print("\n##############################################################")
+    print("####################### Inference Done #######################")
 
   # Get the overall stats for each TM and return them
   tpStats = dict()
-  for (name,tm) in tms.iteritems():
+  for (name,tm) in tms.items():
     tpStats[name] = stats = tm.getStats()
     if VERBOSITY >= 2:
-      print "Stats for %s TM over all sequences:" % (name)
-      print "   total missing:", stats['totalMissing']
-      print "   total extra:", stats['totalExtra']
-
-  for (name,tm) in tms.iteritems():
+      print("Stats for %s TM over all sequences:" % (name))
+      print("   total missing:", stats['totalMissing'])
+      print("   total extra:", stats['totalExtra'])
+
+  for (name,tm) in tms.items():
     if VERBOSITY >= 3:
-      print "\nAll internal stats for %s TM:" % (name)
-      print "-------------------------------------",
+      print("\nAll internal stats for %s TM:" % (name))
+      print("-------------------------------------", end=' ')
       pprint.pprint(tpStats[name])
-      print
+      print()
 
   return tpStats
 
@@ -626,16 +626,16 @@
 
   # -----------------------------------------------------------------------
   # Make sure there are the expected number of missing predictions
-  for (name, stats) in tpStats.iteritems():
-    print "Detected %d missing predictions overall during inference" \
-              % (stats['totalMissing'])
+  for (name, stats) in tpStats.items():
+    print("Detected %d missing predictions overall during inference" \
+              % (stats['totalMissing']))
     if expMissingMin is not None and stats['totalMissing'] < expMissingMin:
-      print "FAILURE: Expected at least %d total missing but got %d" \
-          % (expMissingMin, stats['totalMissing'])
+      print("FAILURE: Expected at least %d total missing but got %d" \
+          % (expMissingMin, stats['totalMissing']))
       assert False
     if expMissingMax is not None and stats['totalMissing'] > expMissingMax:
-      print "FAILURE: Expected at most %d total missing but got %d" \
-          % (expMissingMax, stats['totalMissing'])
+      print("FAILURE: Expected at most %d total missing but got %d" \
+          % (expMissingMax, stats['totalMissing']))
       assert False
 
 
@@ -683,13 +683,13 @@
     # ================================================================
     # Run various configs
     # No PAM, with 3 repetitions, still missing predictions
-    print "\nRunning without PAM, 3 repetitions of the training data..."
+    print("\nRunning without PAM, 3 repetitions of the training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=20,
                                 expMissingMax=None, pamLength=1,
                                 nTrainRepetitions=3))
 
     # With PAM, with only 3 repetitions, 0 missing predictions
-    print "\nRunning with PAM, 3 repetitions of the training data..."
+    print("\nRunning with PAM, 3 repetitions of the training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=0,
                                 expMissingMax=0, pamLength=5,
                                 nTrainRepetitions=3))
@@ -734,13 +734,13 @@
     # Run various configs
     # No PAM, requires 40 repetitions
     # No PAM, with 10 repetitions, still missing predictions
-    print "\nRunning without PAM, 10 repetitions of the training data..."
+    print("\nRunning without PAM, 10 repetitions of the training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=10,
                                 expMissingMax=None, pamLength=1,
                                 nTrainRepetitions=10))
 
     # With PAM, with only 10 repetitions, 0 missing predictions
-    print "\nRunning with PAM, 10 repetitions of the training data..."
+    print("\nRunning with PAM, 10 repetitions of the training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=0,
                                 expMissingMax=0, pamLength=6,
                                 nTrainRepetitions=10))
@@ -790,13 +790,13 @@
     # ================================================================
     # Run various configs
     # No PAM, with 10 repetitions, still missing predictions
-    print "\nRunning without PAM, 10 repetitions of the training data..."
+    print("\nRunning without PAM, 10 repetitions of the training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=10,
                                 expMissingMax=None, pamLength=1,
                                 nTrainRepetitions=10))
 
     # With PAM, with only 10 repetitions, 0 missing predictions
-    print "\nRunning with PAM, 10 repetitions of the training data..."
+    print("\nRunning with PAM, 10 repetitions of the training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=0,
                                 expMissingMax=0, pamLength=6,
                                 nTrainRepetitions=10))
@@ -850,29 +850,29 @@
     # Run various configs
     # Fast mode, no PAM
     # Fast mode, with PAM
-    print "\nRunning without PAM, fast learning, 2 repetitions of the " \
-          "training data..."
+    print("\nRunning without PAM, fast learning, 2 repetitions of the " \
+          "training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=50,
                                 expMissingMax=None, pamLength=1,
                                 nTrainRepetitions=2))
 
     # Fast mode, with PAM
-    print "\nRunning with PAM, fast learning, 2 repetitions of the " \
-          "training data..."
+    print("\nRunning with PAM, fast learning, 2 repetitions of the " \
+          "training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=0,
                                 expMissingMax=0, pamLength=5,
                                 nTrainRepetitions=2))
 
     # Slow mode, no PAM
-    print "\nRunning without PAM, slow learning, 8 repetitions of the " \
-          "training data..."
+    print("\nRunning without PAM, slow learning, 8 repetitions of the " \
+          "training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=1,
                                 expMissingMax=None, initialPerm=0.31,
                                 pamLength=1, nTrainRepetitions=8))
 
     # Fast mode, with PAM
-    print "\nRunning with PAM, slow learning, 8 repetitions of the " \
-          "training data..."
+    print("\nRunning with PAM, slow learning, 8 repetitions of the " \
+          "training data...")
     self.assertTrue(_testConfig(baseParams=baseParams, expMissingMin=0,
                                 expMissingMax=0, initialPerm=0.31, pamLength=5,
                                 nTrainRepetitions=8))
@@ -901,9 +901,9 @@
   random.seed(SEED)
 
   if not INCLUDE_CPP_TM:
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
-    print "!!  WARNING: C++ TM testing is DISABLED until it can be updated."
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
+    print("!!  WARNING: C++ TM testing is DISABLED until it can be updated.")
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
 
   # Form the command line for the unit test framework.
   args = [sys.argv[0]] + args
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_segment_learning.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_segment_learning.py	(refactored)
@@ -80,13 +80,13 @@
 
   def _printOneTrainingVector(self, x):
     """Print a single vector succinctly."""
-    print ''.join('1' if k != 0 else '.' for k in x)
+    print(''.join('1' if k != 0 else '.' for k in x))
 
 
   def _printAllTrainingSequences(self, trainingSequences):
     """Print all vectors"""
     for i, trainingSequence in enumerate(trainingSequences):
-      print "============= Sequence", i, "================="
+      print("============= Sequence", i, "=================")
       for pattern in trainingSequence:
         self._printOneTrainingVector(pattern)
 
@@ -128,7 +128,7 @@
 
     if g_testCPPTM:
       if g_options.verbosity > 1:
-        print "Creating BacktrackingTMCPP instance"
+        print("Creating BacktrackingTMCPP instance")
 
       cppTM = BacktrackingTMCPP(numberOfCols = numCols, cellsPerColumn = 4,
                                 initialPerm = initialPerm, connectedPerm = connectedPerm,
@@ -151,7 +151,7 @@
       cppTM = None
 
     if g_options.verbosity > 1:
-      print "Creating PY TM instance"
+      print("Creating PY TM instance")
     pyTM = BacktrackingTM(numberOfCols = numCols, cellsPerColumn = 4,
                           initialPerm = initialPerm,
                           connectedPerm = connectedPerm,
@@ -177,7 +177,7 @@
 
     numCols = numOnes * numPatterns
     p = []
-    for i in xrange(numPatterns):
+    for i in range(numPatterns):
       x = numpy.zeros(numCols, dtype='float32')
       x[i*numOnes:(i+1)*numOnes] = 1
       p.append(x)
@@ -205,9 +205,9 @@
 
     # Create noisy training sequence
     trainingSequences = []
-    for i in xrange(numRepetitions):
+    for i in range(numRepetitions):
       sequence = []
-      for j in xrange(numPatterns):
+      for j in range(numPatterns):
 
         # Make left half
         v = numpy.zeros(numCols)
@@ -222,16 +222,16 @@
 
     # Create a single clean test sequence
     testSequence = []
-    for j in xrange(numPatterns):
+    for j in range(numPatterns):
       # Make only left half
       v = numpy.zeros(numCols, dtype='float32')
       v[0:halfCols] = p[j]
       testSequence.append(v)
 
     if g_options.verbosity > 1:
-      print "\nTraining sequences"
+      print("\nTraining sequences")
       self.printAllTrainingSequences(trainingSequences)
-      print "\nTest sequence"
+      print("\nTest sequence")
       self.printAllTrainingSequences([testSequence])
 
     return (trainingSequences, [testSequence])
@@ -270,9 +270,9 @@
 
     # Create the noisy training sequence
     trainingSequences = []
-    for i in xrange(numRepetitions*numSequences):
+    for i in range(numRepetitions*numSequences):
       sequence = []
-      for j in xrange(numPatterns):
+      for j in range(numPatterns):
 
         # Make left half
         v = numpy.zeros(numCols, dtype='float32')
@@ -287,9 +287,9 @@
 
     # Create the clean test sequences
     testSequences = []
-    for i in xrange(numSequences):
+    for i in range(numSequences):
       sequence = []
-      for j in xrange(numPatterns):
+      for j in range(numPatterns):
         # Make only left half
         v = numpy.zeros(numCols, dtype='float32')
         v[0:halfCols] = p[indices[i % numSequences][j]]
@@ -297,9 +297,9 @@
       testSequences.append(sequence)
 
     if g_options.verbosity > 1:
-      print "\nTraining sequences"
+      print("\nTraining sequences")
       self.printAllTrainingSequences(trainingSequences)
-      print "\nTest sequences"
+      print("\nTest sequences")
       self.printAllTrainingSequences(testSequences)
 
     return (trainingSequences, testSequences)
@@ -327,18 +327,18 @@
     #--------------------------------------------------------------------------
     # Learn
     if g_options.verbosity > 0:
-      print "============= Training ================="
-      print "TM parameters:"
-      print "CPP"
+      print("============= Training =================")
+      print("TM parameters:")
+      print("CPP")
       if cppTM is not None:
-        print cppTM.printParameters()
-      print "\nPY"
-      print pyTM.printParameters()
+        print(cppTM.printParameters())
+      print("\nPY")
+      print(pyTM.printParameters())
 
     for sequenceNum, trainingSequence in enumerate(trainingSequences):
 
       if g_options.verbosity > 1:
-        print "============= New sequence ================="
+        print("============= New sequence =================")
 
       if doResets:
         if cppTM is not None:
@@ -348,9 +348,9 @@
       for t, x in enumerate(trainingSequence):
 
         if g_options.verbosity > 1:
-          print "Time step", t, "sequence number", sequenceNum
-          print "Input: ", pyTM.printInput(x)
-          print "NNZ:", x.nonzero()
+          print("Time step", t, "sequence number", sequenceNum)
+          print("Input: ", pyTM.printInput(x))
+          print("NNZ:", x.nonzero())
 
         x = numpy.array(x).astype('float32')
         if cppTM is not None:
@@ -363,23 +363,23 @@
 
         if g_options.verbosity > 2:
           if cppTM is not None:
-            print "CPP"
+            print("CPP")
             cppTM.printStates(printPrevious = (g_options.verbosity > 4))
-          print "\nPY"
+          print("\nPY")
           pyTM.printStates(printPrevious = (g_options.verbosity > 4))
-          print
+          print()
 
       if g_options.verbosity > 4:
-        print "Sequence finished. Complete state after sequence"
-        if cppTM is not None:
-          print "CPP"
+        print("Sequence finished. Complete state after sequence")
+        if cppTM is not None:
+          print("CPP")
           cppTM.printCells()
-        print "\nPY"
+        print("\nPY")
         pyTM.printCells()
-        print
+        print()
 
     if g_options.verbosity > 2:
-      print "Calling trim segments"
+      print("Calling trim segments")
 
     if cppTM is not None:
       nSegsRemovedCPP, nSynsRemovedCPP = cppTM.trimSegments()
@@ -391,22 +391,22 @@
     if cppTM is not None:
       assert fdrutils.tmDiff2(cppTM, pyTM, g_options.verbosity) == True
 
-    print "Training completed. Stats:"
+    print("Training completed. Stats:")
     info = pyTM.getSegmentInfo()
-    print "  nSegments:", info[0]
-    print "  nSynapses:", info[1]
+    print("  nSegments:", info[0])
+    print("  nSynapses:", info[1])
     if g_options.verbosity > 3:
-      print "Complete state:"
+      print("Complete state:")
       if cppTM is not None:
-        print "CPP"
+        print("CPP")
         cppTM.printCells()
-      print "\nPY"
+      print("\nPY")
       pyTM.printCells()
 
     #---------------------------------------------------------------------------
     # Infer
     if g_options.verbosity > 1:
-      print "============= Inference ================="
+      print("============= Inference =================")
 
     if cppTM is not None:
       cppTM.collectStats = True
@@ -418,7 +418,7 @@
     for sequenceNum, testSequence in enumerate(testSequences):
 
       if g_options.verbosity > 1:
-        print "============= New sequence ================="
+        print("============= New sequence =================")
 
       slen = len(testSequence)
 
@@ -430,7 +430,7 @@
       for t, x in enumerate(testSequence):
 
         if g_options.verbosity >= 2:
-          print "Time step", t, '\nInput:'
+          print("Time step", t, '\nInput:')
           pyTM.printInput(x)
 
         if cppTM is not None:
@@ -442,10 +442,10 @@
 
         if g_options.verbosity > 2:
           if cppTM is not None:
-            print "CPP"
+            print("CPP")
             cppTM.printStates(printPrevious = (g_options.verbosity > 4),
                            printLearnState = False)
-          print "\nPY"
+          print("\nPY")
           pyTM.printStates(printPrevious = (g_options.verbosity > 4),
                          printLearnState = False)
 
@@ -455,10 +455,10 @@
 
         if g_options.verbosity >= 2:
           if cppTM is not None:
-            print "CPP"
-            print cppScores
-          print "\nPY"
-          print pyScores
+            print("CPP")
+            print(cppScores)
+          print("\nPY")
+          print(pyScores)
 
         if t < slen-1 and t > pyTM.burnIn:
           nPredictions += 1
@@ -482,9 +482,9 @@
         passTest = True
 
     if not passTest:
-      print "CPP correct predictions:", cppNumCorrect
-      print "PY correct predictions:", pyNumCorrect
-      print "Total predictions:", nPredictions
+      print("CPP correct predictions:", cppNumCorrect)
+      print("PY correct predictions:", pyNumCorrect)
+      print("Total predictions:", nPredictions)
 
     return passTest
 
@@ -498,7 +498,7 @@
     else:
       testName = "TestSL1"
 
-    print "\nRunning %s..." % testName
+    print("\nRunning %s..." % testName)
 
     trainingSet, testSet = self._buildSegmentLearningTrainingSet(numOnes,
                                                                  numRepetitions)
@@ -510,10 +510,10 @@
     testResult = self._testSegmentLearningSequence(tms, trainingSet, testSet)
 
     if testResult:
-      print "%s PASS" % testName
+      print("%s PASS" % testName)
       return 1
     else:
-      print "%s FAILED" % testName
+      print("%s FAILED" % testName)
       return 0
 
 
@@ -526,7 +526,7 @@
     else:
       testName = "TestSL2"
 
-    print "\nRunning %s..." % testName
+    print("\nRunning %s..." % testName)
 
     trainingSet, testSet = self._buildSL2TrainingSet(numOnes, numRepetitions)
     numCols = len(trainingSet[0][0])
@@ -537,10 +537,10 @@
     testResult = self._testSegmentLearningSequence(tms, trainingSet, testSet)
 
     if testResult:
-      print "%s PASS" % testName
+      print("%s PASS" % testName)
       return 1
     else:
-      print "%s FAILED" % testName
+      print("%s FAILED" % testName)
       return 0
 
 
@@ -560,8 +560,8 @@
     """Test segment learning with fixed resources"""
 
     if not g_options.long:
-      print "Test %s only enabled with the --long option" % \
-                                (self._testMethodName)
+      print("Test %s only enabled with the --long option" % \
+                                (self._testMethodName))
       return
 
     self._testSL1(fixedResources=True,
@@ -572,8 +572,8 @@
     """Test segment learning without fixed resources"""
 
     if not g_options.long:
-      print "Test %s only enabled with the --long option" % \
-                                (self._testMethodName)
+      print("Test %s only enabled with the --long option" % \
+                                (self._testMethodName))
       return
 
     self._testSL2(fixedResources=False,
@@ -584,8 +584,8 @@
     """Test segment learning with fixed resources"""
 
     if not g_options.long:
-      print "Test %s only enabled with the --long option" % \
-                                (self._testMethodName)
+      print("Test %s only enabled with the --long option" % \
+                                (self._testMethodName))
       return
 
     self._testSL2(fixedResources=True,
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tm_test.py	(refactored)
@@ -433,7 +433,7 @@
 
 """
 
-import cPickle
+import pickle
 import numpy
 import pickle
 import pprint
@@ -462,16 +462,16 @@
 
 def printOneTrainingVector(x):
 
-    print ''.join('1' if k != 0 else '.' for k in x)
+    print(''.join('1' if k != 0 else '.' for k in x))
 
 
 
 def printAllTrainingSequences(trainingSequences, upTo = 99999):
 
-    for t in xrange(min(len(trainingSequences[0]), upTo)):
-        print 't=',t,
+    for t in range(min(len(trainingSequences[0]), upTo)):
+        print('t=',t, end=' ')
         for i,trainingSequence in enumerate(trainingSequences):
-            print "\tseq#",i,'\t',
+            print("\tseq#",i,'\t', end=' ')
             printOneTrainingVector(trainingSequences[i][t])
 
 
@@ -563,7 +563,7 @@
 
   sharedSequence = []
 
-  for i in xrange(sharedSequenceLength):
+  for i in range(sharedSequenceLength):
     if disjointConsecutive and i > 0:
       x = generatePattern(numCols, minOnes, maxOnes, colSet, sharedSequence[i-1])
     else:
@@ -579,18 +579,18 @@
   else:
       trailingLength = sequenceLength - sharedSequenceLength
 
-  for k,s in enumerate(xrange(numSequences)):
+  for k,s in enumerate(range(numSequences)):
 
     # TODO: implement no repetitions
     if len(trainingSequences) > 0 and 'shuffle' in seqGenMode:
 
-      r = range(subsequenceStartPos) \
-          + range(subsequenceStartPos + sharedSequenceLength, sequenceLength)
+      r = list(range(subsequenceStartPos)) \
+          + list(range(subsequenceStartPos + sharedSequenceLength, sequenceLength))
 
       rgen.shuffle(r)
 
       r = r[:subsequenceStartPos] \
-          + range(subsequenceStartPos, subsequenceStartPos + sharedSequenceLength) \
+          + list(range(subsequenceStartPos, subsequenceStartPos + sharedSequenceLength)) \
           + r[subsequenceStartPos:]
 
       sequence = [trainingSequences[k-1][j] for j in r]
@@ -599,7 +599,7 @@
         sequence = []
 
         if 'beginning' not in seqGenMode:
-          for i in xrange(subsequenceStartPos):
+          for i in range(subsequenceStartPos):
             if disjointConsecutive and i > 0:
               x = generatePattern(numCols, minOnes, maxOnes, colSet, sequence[i-1])
             else:
@@ -609,7 +609,7 @@
         if 'shared' in seqGenMode and 'no shared' not in seqGenMode:
           sequence.extend(sharedSequence)
 
-        for i in xrange(trailingLength):
+        for i in range(trailingLength):
           if disjointConsecutive and i > 0:
             x = generatePattern(numCols, minOnes, maxOnes, colSet, sequence[i-1])
           else:
@@ -623,7 +623,7 @@
   assert len(trainingSequences) == numSequences
 
   if VERBOSITY >= 2:
-    print "Training Sequences"
+    print("Training Sequences")
     pprint.pprint(trainingSequences)
 
   if sharedSequenceLength > 0:
@@ -638,7 +638,7 @@
 
   numCols = numOnes * numPatterns
   p = []
-  for i in xrange(numPatterns):
+  for i in range(numPatterns):
     x = numpy.zeros(numCols, dtype='float32')
     x[i*numOnes:(i+1)*numOnes] = 1
     p.append(x)
@@ -698,7 +698,7 @@
 
   s = []
   s.append(p[rgen.randint(3,23)])
-  for i in xrange(20):
+  for i in range(20):
     s.append(p[rgen.randint(3,23)])
     s.append(p[0])
     s.append(p[1])
@@ -724,25 +724,25 @@
 
   s = []
   s.append(p[rgen.randint(5,numPatterns)])
-  for i in xrange(50):
+  for i in range(50):
     r = rgen.randint(5,numPatterns)
-    print r,
+    print(r, end=' ')
     s.append(p[r])
     if rgen.binomial(1, 0.5) > 0:
-      print "S1",
+      print("S1", end=' ')
       s.append(p[0])
       s.append(p[1])
       s.append(p[2])
       s.append(p[4])
     else:
-      print "S2",
+      print("S2", end=' ')
       s.append(p[1])
       s.append(p[2])
       s.append(p[3])
     r = rgen.randint(5,numPatterns)
     s.append(p[r])
-    print r,
-  print
+    print(r, end=' ')
+  print()
 
   return ([s], [ [p[0], p[1], p[2], p[4]],  [p[1], p[2], p[3]] ])
 
@@ -780,7 +780,7 @@
           pamLength = 1000,
           checkSynapseConsistency=checkSynapseConsistency)
 
-  print "Creation ok"
+  print("Creation ok")
 
   #--------------------------------------------------------------------------------
   # Save and reload
@@ -789,7 +789,7 @@
 
   assert tm2.numberOfCols == numberOfCols
   assert tm2.cellsPerColumn == cellsPerColumn
-  print tm2.initialPerm
+  print(tm2.initialPerm)
   assert tm2.initialPerm == numpy.float32(.2)
   assert tm2.connectedPerm == numpy.float32(.8)
   assert tm2.minThreshold == minThreshold
@@ -804,11 +804,11 @@
   assert tm2.seed == SEED
   assert tm2.verbosity == verbosity
 
-  print "Save/load ok"
+  print("Save/load ok")
 
   #--------------------------------------------------------------------------------
   # Learn
-  for i in xrange(5):
+  for i in range(5):
     xi = rgen.randint(0,2,(numberOfCols))
     x = numpy.array(xi, dtype="uint32")
     y = tm.learn(x)
@@ -816,14 +816,14 @@
   #--------------------------------------------------------------------------------
   # Infer
   patterns = rgen.randint(0,2,(4,numberOfCols))
-  for i in xrange(10):
+  for i in range(10):
     xi = rgen.randint(0,2,(numberOfCols))
     x = numpy.array(xi, dtype="uint32")
     y = tm.infer(x)
     if i > 0:
         p = tm._checkPrediction([pattern.nonzero()[0] for pattern in patterns])
 
-  print "basicTest ok"
+  print("basicTest ok")
 
 #---------------------------------------------------------------------------------
 # Figure out acceptable patterns if none were passed to us.
@@ -884,7 +884,7 @@
 
     # Add patterns going forward
     acceptablePatterns += [trainingSequences[whichSequence][t] \
-                           for t in xrange(t,upTo)]
+                           for t in range(t,upTo)]
 
     return acceptablePatterns
 
@@ -986,12 +986,12 @@
 
   #--------------------------------------------------------------------------------
   # Learn
-  for r in xrange(nTrainingReps):
+  for r in range(nTrainingReps):
     if VERBOSITY > 1:
-      print "============= Learning round",r,"================="
+      print("============= Learning round",r,"=================")
     for sequenceNum, trainingSequence in enumerate(trainingSequences):
       if VERBOSITY > 1:
-        print "============= New sequence ================="
+        print("============= New sequence =================")
       if doResets:
           tm.reset()
           if compareToPy:
@@ -1003,9 +1003,9 @@
             noise_vector = rgen.binomial(len(x), noiseLevel, (len(x)))
             x = logical_xor(x, noise_vector)
         if VERBOSITY > 2:
-          print "Time step",t, "learning round",r, "sequence number", sequenceNum
-          print "Input: ",tm.printInput(x)
-          print "NNZ:", x.nonzero()
+          print("Time step",t, "learning round",r, "sequence number", sequenceNum)
+          print("Input: ",tm.printInput(x))
+          print("NNZ:", x.nonzero())
         x = numpy.array(x).astype('float32')
         y = tm.learn(x)
         if compareToPy:
@@ -1015,25 +1015,25 @@
 
         if VERBOSITY > 3:
           tm.printStates(printPrevious = (VERBOSITY > 4))
-          print
+          print()
       if VERBOSITY > 3:
-        print "Sequence finished. Complete state after sequence"
+        print("Sequence finished. Complete state after sequence")
         tm.printCells()
-        print
+        print()
 
   numPerfectAtHub = 0
 
   if compareToPy:
-      print "End of training"
+      print("End of training")
       assert fdrutils.tmDiff(tm, py_tm, VERBOSITY) == True
 
   #--------------------------------------------------------------------------------
   # Infer
-  if VERBOSITY > 1: print "============= Inference ================="
+  if VERBOSITY > 1: print("============= Inference =================")
 
   for s,testSequence in enumerate(testSequences):
 
-    if VERBOSITY > 1: print "============= New sequence ================="
+    if VERBOSITY > 1: print("============= New sequence =================")
 
     if doResets:
         tm.reset()
@@ -1051,7 +1051,7 @@
         noise_vector = rgen.binomial(len(x), noiseLevel, (len(x)))
         x = logical_xor(x, noise_vector)
 
-      if VERBOSITY > 2: print "Time step",t, '\nInput:', tm.printInput(x)
+      if VERBOSITY > 2: print("Time step",t, '\nInput:', tm.printInput(x))
 
       x = numpy.array(x).astype('float32')
       y = tm.infer(x)
@@ -1066,7 +1066,7 @@
       #     print ''.join('.' if z[i] == 0 else '1' for i in xrange(len(z)))
 
       if VERBOSITY > 3: tm.printStates(printPrevious = (VERBOSITY > 4),
-                                       printLearnState = False); print
+                                       printLearnState = False); print()
 
 
       if nMultiStepPrediction > 0:
@@ -1074,9 +1074,9 @@
         y_ms = tm.predict(nSteps=nMultiStepPrediction)
 
         if VERBOSITY > 3:
-          print "Multi step prediction at Time step", t
+          print("Multi step prediction at Time step", t)
           for i in range(nMultiStepPrediction):
-            print "Prediction at t+", i+1
+            print("Prediction at t+", i+1)
             tm.printColConfidence(y_ms[i])
 
         # Error Checking
@@ -1092,34 +1092,34 @@
             falsePositives = missingFromInput
 
             if VERBOSITY > 2:
-              print "Predition from %d to %d" % (t, t+i+1)
-              print "\t\tFalse Negatives:", falseNegatives
-              print "\t\tFalse Positivies:", falsePositives
+              print("Predition from %d to %d" % (t, t+i+1))
+              print("\t\tFalse Negatives:", falseNegatives)
+              print("\t\tFalse Positivies:", falsePositives)
 
             if falseNegatives > 0 or falsePositives > 0:
               numStrictErrors += 1
 
               if falseNegatives > 0 and VERBOSITY > 1:
-                print "Multi step prediction from t=", t, "to t=", t+i+1,\
-                      "false negative with error=",falseNegatives,
-                print "out of", totalActiveInInput,"ones"
+                print("Multi step prediction from t=", t, "to t=", t+i+1,\
+                      "false negative with error=",falseNegatives, end=' ')
+                print("out of", totalActiveInInput,"ones")
 
               if falsePositives > 0 and VERBOSITY > 1:
-                print "Multi step prediction from t=", t, "to t=", t+i+1,\
-                    "false positive with error=",falsePositives,
-                print "out of",totalActiveInInput,"ones"
+                print("Multi step prediction from t=", t, "to t=", t+i+1,\
+                    "false positive with error=",falsePositives, end=' ')
+                print("out of",totalActiveInInput,"ones")
 
               if falsePositives > 3 or falseNegatives > 3:
                 numFailures += 1
 
                 # Analyze the failure if we care about it
                 if VERBOSITY > 1 and not shouldFail:
-                  print 'Input at t=', t
-                  print '\t\t',; printOneTrainingVector(testSequence[t])
-                  print 'Prediction for t=', t+i+1
-                  print '\t\t',; printOneTrainingVector(y_ms[i])
-                  print 'Actual input at t=', t+i+1
-                  print '\t\t',; printOneTrainingVector(testSequence[t+i+1])
+                  print('Input at t=', t)
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t])
+                  print('Prediction for t=', t+i+1)
+                  print('\t\t', end=' '); printOneTrainingVector(y_ms[i])
+                  print('Actual input at t=', t+i+1)
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t+i+1])
 
 
       if t < slen-1:
@@ -1145,14 +1145,14 @@
           numStrictErrors += 1
 
           if falseNegatives > 0 and VERBOSITY > 1:
-            print "Pattern",s,"time",t,\
-                  "prediction false negative with error=",falseNegatives,
-            print "out of",int(testSequence[t+1].sum()),"ones"
+            print("Pattern",s,"time",t,\
+                  "prediction false negative with error=",falseNegatives, end=' ')
+            print("out of",int(testSequence[t+1].sum()),"ones")
 
           if falsePositives > 0 and VERBOSITY > 1:
-            print "Pattern",s,"time",t,\
-                  "prediction false positive with error=",falsePositives,
-            print "out of",int(testSequence[t+1].sum()),"ones"
+            print("Pattern",s,"time",t,\
+                  "prediction false positive with error=",falsePositives, end=' ')
+            print("out of",int(testSequence[t+1].sum()),"ones")
 
           if falseNegatives > 3 or falsePositives > 3:
 
@@ -1160,19 +1160,19 @@
 
             # Analyze the failure if we care about it
             if VERBOSITY > 1 and not shouldFail:
-              print 'Test sequences'
+              print('Test sequences')
               if len(testSequences) > 1:
                   printAllTrainingSequences(testSequences, t+1)
               else:
-                  print '\t\t',; printOneTrainingVector(testSequence[t])
-                  print '\t\t',; printOneTrainingVector(testSequence[t+1])
-              print 'Acceptable'
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t])
+                  print('\t\t', end=' '); printOneTrainingVector(testSequence[t+1])
+              print('Acceptable')
               for p in acceptablePatterns:
-                  print '\t\t',; printOneTrainingVector(p)
-              print 'Output'
+                  print('\t\t', end=' '); printOneTrainingVector(p)
+              print('Output')
               diagnostic = ''
               output = sum(tm.currentOutput,axis=1)
-              print '\t\t',; printOneTrainingVector(output)
+              print('\t\t', end=' '); printOneTrainingVector(output)
 
         else:
             numPerfect += 1
@@ -1195,7 +1195,7 @@
 
   for numSequences in [1]:
 
-    print "Test "+name+" (sequence memory - 1 repetition - 1 sequence)"
+    print("Test "+name+" (sequence memory - 1 repetition - 1 sequence)")
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1222,13 +1222,13 @@
                                  activationThreshold = 8,
                                  doPooling = False)
       if numFailures == 0:
-        print "Test "+name+" ok"
+        print("Test "+name+" ok")
       else:
-        print "Test "+name+" failed"
+        print("Test "+name+" failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1240,7 +1240,7 @@
 
   for numSequences in [1]:
 
-    print "Test "+name+" (sequence memory - 4 repetition - 1 sequence - slow learning)"
+    print("Test "+name+" (sequence memory - 4 repetition - 1 sequence - slow learning)")
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1268,13 +1268,13 @@
                                  doPooling = False)
 
       if numFailures == 0:
-        print "Test "+name+" ok"
+        print("Test "+name+" ok")
       else:
-        print "Test "+name+" failed"
+        print("Test "+name+" failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures,
-        print "numStrictErrors=", numStrictErrors,
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures, end=' ')
+        print("numStrictErrors=", numStrictErrors, end=' ')
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1288,10 +1288,10 @@
 
   for numSequences in [1]: # TestC has multiple sequences
 
-    print "Test",name,"(sequence memory - second repetition of the same sequence" +\
-          " should not add synapses)"
-    print "Num patterns in sequence =", numUniquePatterns,
-    print "cellsPerColumn=",cellsPerColumn
+    print("Test",name,"(sequence memory - second repetition of the same sequence" +\
+          " should not add synapses)")
+    print("Num patterns in sequence =", numUniquePatterns, end=' ')
+    print("cellsPerColumn=",cellsPerColumn)
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1339,18 +1339,18 @@
       segmentInfo2 = tm2.getSegmentInfo()
       if (segmentInfo1[0] != segmentInfo2[0]) or \
          (segmentInfo1[1] != segmentInfo2[1]) :
-          print "Training twice incorrectly resulted in more segments or synapses"
-          print "Number of segments: ", segmentInfo1[0], segmentInfo2[0]
+          print("Training twice incorrectly resulted in more segments or synapses")
+          print("Number of segments: ", segmentInfo1[0], segmentInfo2[0])
           numFailures += 1
 
       if numFailures == 0:
-        print "Test",name,"ok"
+        print("Test",name,"ok")
       else:
-        print "Test",name,"failed"
+        print("Test",name,"failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1364,7 +1364,7 @@
 
   for numSequences in [2,5]:
 
-    print "Test B3 (sequence memory - 2 repetitions -", numSequences, "sequences)"
+    print("Test B3 (sequence memory - 2 repetitions -", numSequences, "sequences)")
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1391,13 +1391,13 @@
                                  activationThreshold = 8,
                                  doPooling = False)
       if numFailures == 0:
-        print "Test B3 ok"
+        print("Test B3 ok")
       else:
-        print "Test B3 failed"
+        print("Test B3 failed")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1407,7 +1407,7 @@
 
   cellsPerColumn = 4
 
-  print "Higher order test 0 with cellsPerColumn=",cellsPerColumn
+  print("Higher order test 0 with cellsPerColumn=",cellsPerColumn)
 
   trainingSet = buildSimpleTrainingSet(numOnes)
 
@@ -1431,13 +1431,13 @@
   if numFailures == 0 and \
      numStrictErrors == 0 and \
      numPerfect == len(trainingSet[0])*(len(trainingSet[0][0]) - 1):
-    print "Test PASS"
+    print("Test PASS")
     return 0
   else:
-    print "Test FAILED"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test FAILED")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1452,9 +1452,9 @@
 
   for numSequences in nSequences:
 
-    print "Higher order test with sequenceLength=",sequenceLength,
-    print "cellsPerColumn=",cellsPerColumn,"nTests=",nTests,
-    print "numSequences=",numSequences, "pctShared=", pctShared
+    print("Higher order test with sequenceLength=",sequenceLength, end=' ')
+    print("cellsPerColumn=",cellsPerColumn,"nTests=",nTests, end=' ')
+    print("numSequences=",numSequences, "pctShared=", pctShared)
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1486,17 +1486,17 @@
 
       if numFailures == 0 and not shouldFail \
              or numFailures > 0 and shouldFail:
-          print "Test PASS",
+          print("Test PASS", end=' ')
           if shouldFail:
-              print '(should fail, and failed)'
+              print('(should fail, and failed)')
           else:
-              print
+              print()
       else:
-        print "Test FAILED"
+        print("Test FAILED")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1506,7 +1506,7 @@
 
   cellsPerColumn = 4
 
-  print "Higher order test 11 with cellsPerColumn=",cellsPerColumn
+  print("Higher order test 11 with cellsPerColumn=",cellsPerColumn)
 
   trainingSet = buildAlternatingTrainingSet(numOnes= 3)
 
@@ -1529,13 +1529,13 @@
   if numFailures == 0 and \
      numStrictErrors == 0 and \
      numPerfect == len(trainingSet[0])*(len(trainingSet[0][0]) - 1):
-    print "Test PASS"
+    print("Test PASS")
     return 0
   else:
-    print "Test FAILED"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test FAILED")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1549,7 +1549,7 @@
       get correct high order prediction after multiple reps.
   """
 
-  print "Test H2a - second repetition of the same sequence should not add synapses"
+  print("Test H2a - second repetition of the same sequence should not add synapses")
 
   nFailed = 0
   subsequenceStartPos = 10
@@ -1557,10 +1557,10 @@
 
   for numSequences in nSequences:
 
-    print "Higher order test with sequenceLength=",sequenceLength,
-    print "cellsPerColumn=",cellsPerColumn,"nTests=",nTests,"numCols=", numCols
-    print "numSequences=",numSequences, "pctShared=", pctShared,
-    print "sharing mode=", seqGenMode
+    print("Higher order test with sequenceLength=",sequenceLength, end=' ')
+    print("cellsPerColumn=",cellsPerColumn,"nTests=",nTests,"numCols=", numCols)
+    print("numSequences=",numSequences, "pctShared=", pctShared, end=' ')
+    print("sharing mode=", seqGenMode)
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1571,7 +1571,7 @@
                                      numCols = numCols,
                                      minOnes = 21, maxOnes = 25)
 
-      print "============== 10 ======================"
+      print("============== 10 ======================")
 
       numFailures3, numStrictErrors3, numPerfect3, tm3 = \
                    _testSequence(trainingSet,
@@ -1590,7 +1590,7 @@
                                  doPooling = False,
                                  shouldFail = shouldFail)
 
-      print "============== 2 ======================"
+      print("============== 2 ======================")
 
       numFailures, numStrictErrors, numPerfect, tm2 = \
                    _testSequence(trainingSet,
@@ -1609,7 +1609,7 @@
                                  doPooling = False,
                                  shouldFail = shouldFail)
 
-      print "============== 1 ======================"
+      print("============== 1 ======================")
 
       numFailures1, numStrictErrors1, numPerfect1, tm1 = \
                    _testSequence(trainingSet,
@@ -1633,32 +1633,32 @@
       segmentInfo2 = tm2.getSegmentInfo()
       if (abs(segmentInfo1[0] - segmentInfo2[0]) > 3) or \
          (abs(segmentInfo1[1] - segmentInfo2[1]) > 3*15) :
-          print "Training twice incorrectly resulted in too many segments or synapses"
-          print segmentInfo1
-          print segmentInfo2
-          print tm3.getSegmentInfo()
+          print("Training twice incorrectly resulted in too many segments or synapses")
+          print(segmentInfo1)
+          print(segmentInfo2)
+          print(tm3.getSegmentInfo())
           tm3.trimSegments()
-          print tm3.getSegmentInfo()
-
-          print "Failures for 1, 2, and N reps"
-          print numFailures1, numStrictErrors1, numPerfect1
-          print numFailures, numStrictErrors, numPerfect
-          print numFailures3, numStrictErrors3, numPerfect3
+          print(tm3.getSegmentInfo())
+
+          print("Failures for 1, 2, and N reps")
+          print(numFailures1, numStrictErrors1, numPerfect1)
+          print(numFailures, numStrictErrors, numPerfect)
+          print(numFailures3, numStrictErrors3, numPerfect3)
           numFailures += 1
 
       if numFailures == 0 and not shouldFail \
              or numFailures > 0 and shouldFail:
-          print "Test PASS",
+          print("Test PASS", end=' ')
           if shouldFail:
-              print '(should fail, and failed)'
+              print('(should fail, and failed)')
           else:
-              print
+              print()
       else:
-        print "Test FAILED"
+        print("Test FAILED")
         nFailed = nFailed + 1
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
 
   return nFailed
 
@@ -1676,12 +1676,12 @@
 
   for numSequences in nSequences:
 
-    print "Pooling test with sequenceLength=",sequenceLength,
-    print 'numCols=', numCols,
-    print "cellsPerColumn=",cellsPerColumn,"nTests=",nTests,
-    print "numSequences=",numSequences, "pctShared=", pctShared,
-    print "nTrainingReps=", nTrainingReps, "minOnes=", minOnes,
-    print "maxOnes=", maxOnes
+    print("Pooling test with sequenceLength=",sequenceLength, end=' ')
+    print('numCols=', numCols, end=' ')
+    print("cellsPerColumn=",cellsPerColumn,"nTests=",nTests, end=' ')
+    print("numSequences=",numSequences, "pctShared=", pctShared, end=' ')
+    print("nTrainingReps=", nTrainingReps, "minOnes=", minOnes, end=' ')
+    print("maxOnes=", maxOnes)
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1713,12 +1713,12 @@
       if numFailures == 0 and \
          numStrictErrors == 0 and \
          numPerfect == numSequences*(sequenceLength - 1):
-        print "Test PASS"
+        print("Test PASS")
       else:
-        print "Test FAILED"
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("Test FAILED")
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
         nFailed = nFailed + 1
 
   return nFailed
@@ -1731,7 +1731,7 @@
   newSynapseCount = 5
   activationThreshold = newSynapseCount
 
-  print "HiLo test 0a with cellsPerColumn=",cellsPerColumn
+  print("HiLo test 0a with cellsPerColumn=",cellsPerColumn)
 
   trainingSet, testSet = buildHL0aTrainingSet()
   numCols = trainingSet[0][0].size
@@ -1756,22 +1756,22 @@
 
   tm.trimSegments()
   retAfter = tm.getSegmentInfo()
-  print retAfter[0], retAfter[1]
+  print(retAfter[0], retAfter[1])
   if retAfter[0] > 20:
-    print "Too many segments"
+    print("Too many segments")
     numFailures += 1
   if retAfter[1] > 100:
-    print "Too many synapses"
+    print("Too many synapses")
     numFailures += 1
 
   if numFailures == 0:
-    print "Test HL0a ok"
+    print("Test HL0a ok")
     return 0
   else:
-    print "Test HL0a failed"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test HL0a failed")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1782,11 +1782,11 @@
   newSynapseCount = 5
   activationThreshold = newSynapseCount
 
-  print "HiLo test 0b with cellsPerColumn=",cellsPerColumn
+  print("HiLo test 0b with cellsPerColumn=",cellsPerColumn)
 
   trainingSet, testSet = buildHL0bTrainingSet()
   numCols = trainingSet[0][0].size
-  print "numCols=", numCols
+  print("numCols=", numCols)
 
   numFailures, numStrictErrors, numPerfect, tm = \
                _testSequence([trainingSet],
@@ -1810,13 +1810,13 @@
   tm.printCells()
 
   if numFailures == 0:
-    print "Test HL0 ok"
+    print("Test HL0 ok")
     return 0
   else:
-    print "Test HL0 failed"
-    print "numFailures=", numFailures
-    print "numStrictErrors=", numStrictErrors
-    print "numPerfect=", numPerfect
+    print("Test HL0 failed")
+    print("numFailures=", numFailures)
+    print("numStrictErrors=", numStrictErrors)
+    print("numPerfect=", numPerfect)
     return 1
 
 
@@ -1838,12 +1838,12 @@
 
   for numSequences in nSequences:
 
-    print "Hilo test with sequenceLength=", sequenceLength,
-    print "cellsPerColumn=", cellsPerColumn, "nTests=", nTests,
-    print "numSequences=", numSequences, "pctShared=", pctShared,
-    print "nTrainingReps=", nTrainingReps, "minOnes=", minOnes,
-    print "maxOnes=", maxOnes,
-    print 'noiseModel=', noiseModel, 'noiseLevel=', noiseLevel
+    print("Hilo test with sequenceLength=", sequenceLength, end=' ')
+    print("cellsPerColumn=", cellsPerColumn, "nTests=", nTests, end=' ')
+    print("numSequences=", numSequences, "pctShared=", pctShared, end=' ')
+    print("nTrainingReps=", nTrainingReps, "minOnes=", minOnes, end=' ')
+    print("maxOnes=", maxOnes, end=' ')
+    print('noiseModel=', noiseModel, 'noiseLevel=', noiseLevel)
 
     for k in range(nTests): # Test that configuration several times
 
@@ -1877,12 +1877,12 @@
       if numFailures == 0 and \
          numStrictErrors == 0 and \
          numPerfect == numSequences*(sequenceLength - 1):
-        print "Test PASS"
+        print("Test PASS")
       else:
-        print "Test FAILED"
-        print "numFailures=", numFailures
-        print "numStrictErrors=", numStrictErrors
-        print "numPerfect=", numPerfect
+        print("Test FAILED")
+        print("numFailures=", numFailures)
+        print("numStrictErrors=", numStrictErrors)
+        print("numPerfect=", numPerfect)
         nFailed = nFailed + 1
 
   return nFailed
@@ -1897,7 +1897,7 @@
   sequenceLength = 10
   numCols = 200
 
-  print 'Started', cellsPerColumn, numSequences
+  print('Started', cellsPerColumn, numSequences)
 
   seqGenMode = 'shared subsequence, one pattern'
   subsequenceStartPos = 5
@@ -1951,9 +1951,9 @@
                              doPooling = False,
                              shouldFail = False)
 
-  print 'Completed',
-  print cellsPerColumn, numSequences, numFailures1, numStrictErrors1, numPerfect1, atHub, \
-         numFailures2, numStrictErrors2, numPerfect2
+  print('Completed', end=' ')
+  print(cellsPerColumn, numSequences, numFailures1, numStrictErrors1, numPerfect1, atHub, \
+         numFailures2, numStrictErrors2, numPerfect2)
 
   return cellsPerColumn, numSequences, numFailures1, numStrictErrors1, numPerfect1, atHub, \
          numFailures2, numStrictErrors2, numPerfect2
@@ -1971,16 +1971,16 @@
   from multiprocessing import Pool
   import itertools
 
-  print "Hub capacity test"
+  print("Hub capacity test")
   # scalar value on predictions by looking at max perm over column
 
   p = Pool(2)
 
-  results = p.map(worker, itertools.product([1,2,3,4,5,6,7,8], xrange(1,2000,200)))
+  results = p.map(worker, itertools.product([1,2,3,4,5,6,7,8], range(1,2000,200)))
 
   f = open('results-numPerfect.11.22.10.txt', 'w')
   for i,r in enumerate(results):
-      print >>f, '{%d,%d,%d,%d,%d,%d,%d,%d,%d},' % r
+      print('{%d,%d,%d,%d,%d,%d,%d,%d,%d},' % r, file=f)
   f.close()
 
 
@@ -1992,7 +1992,7 @@
 
   # always run this one: if that one fails, we can't do anything
   basicTest()
-  print
+  print()
 
   #---------------------------------------------------------------------------------
   if testLength == "long":
@@ -2008,7 +2008,7 @@
                                    cellsPerColumn = 4, name="B6")
   tests['B7'] = TestB7(numUniquePatterns, nTests)
 
-  print
+  print()
 
   #---------------------------------------------------------------------------------
 
@@ -2017,14 +2017,14 @@
 
   if True:
 
-      print "Test H0"
+      print("Test H0")
       tests['H0'] = TestH0(numOnes = 5)
 
-      print "Test H2"
+      print("Test H2")
       #tests['H2'] = TestH(numUniquePatterns, nTests, cellsPerColumn = 4,
       #                    nTrainingReps = numUniquePatterns, compareToPy = False)
 
-      print "Test H3"
+      print("Test H3")
       tests['H3'] = TestH(numUniquePatterns, nTests,
                           numCols = 200,
                           cellsPerColumn = 20,
@@ -2032,7 +2032,7 @@
                           compareToPy = False,
                           highOrder = True)
 
-      print "Test H4" # Produces 3 false positives, but otherwise fine.
+      print("Test H4") # Produces 3 false positives, but otherwise fine.
       # TODO: investigate initial false positives?
       tests['H4'] = TestH(numUniquePatterns, nTests,
                         cellsPerColumn = 20,
@@ -2040,14 +2040,14 @@
                         seqGenMode='shared subsequence at beginning')
 
   if True:
-      print "Test H0 with multistep prediction"
+      print("Test H0 with multistep prediction")
 
       tests['H0_MS'] = TestH0(numOnes = 5, nMultiStepPrediction=2)
 
 
   if True:
 
-      print "Test H1" # - Should Fail
+      print("Test H1") # - Should Fail
       tests['H1'] = TestH(numUniquePatterns, nTests,
                                       cellsPerColumn = 1, nTrainingReps = 1,
                                       shouldFail = True)
@@ -2060,13 +2060,13 @@
 
 
   if False:
-      print "Test H5" # make sure seqs are good even with shuffling, fast learning
+      print("Test H5") # make sure seqs are good even with shuffling, fast learning
       tests['H5'] = TestH(numUniquePatterns, nTests,
                             cellsPerColumn = 10,
                             pctShared = 0.0,
                             seqGenMode='shuffle, no shared subsequence')
 
-      print "Test H6" # should work
+      print("Test H6") # should work
       tests['H6'] = TestH(numUniquePatterns, nTests,
                             cellsPerColumn = 10,
                             pctShared = 0.4,
@@ -2089,7 +2089,7 @@
       #                                  pctShared = 0.4,
       #                                  seqGenMode='shuffle, shared subsequence')
 
-      print "Test H9" # plot hub capacity
+      print("Test H9") # plot hub capacity
       tests['H9'] = TestH(numUniquePatterns, nTests,
                                         cellsPerColumn = 10,
                                         pctShared = 0.4,
@@ -2101,11 +2101,11 @@
       #                                    pctShared = 0.4,
       #                                    seqGenMode='shuffle, shared subsequence')
 
-      print
+      print()
 
   #---------------------------------------------------------------------------------
   if False:
-      print "Test P1"
+      print("Test P1")
       tests['P1'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
@@ -2114,14 +2114,14 @@
 
   if False:
 
-      print "Test P2"
+      print("Test P2")
       tests['P2'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
                                         seqGenMode = 'no shared subsequence',
                                         nTrainingReps = 5)
 
-      print "Test P3"
+      print("Test P3")
       tests['P3'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
@@ -2129,7 +2129,7 @@
                                         nSequences = [2] if testLength == 'short' else [2,5],
                                         nTrainingReps = 5)
 
-      print "Test P4"
+      print("Test P4")
       tests['P4'] = TestP(numUniquePatterns, nTests,
                                         cellsPerColumn = 4,
                                         pctShared = 0.0,
@@ -2137,19 +2137,19 @@
                                         nSequences = [2] if testLength == 'short' else [2,5],
                                         nTrainingReps = 5)
 
-      print
+      print()
 
   #---------------------------------------------------------------------------------
   if True:
-      print "Test HL0a"
+      print("Test HL0a")
       tests['HL0a'] = TestHL0a(numOnes = 5)
 
   if False:
 
-      print "Test HL0b"
+      print("Test HL0b")
       tests['HL0b'] = TestHL0b(numOnes = 5)
 
-      print "Test HL1"
+      print("Test HL1")
       tests['HL1'] = TestHL(sequenceLength = 20,
                                            nTests = nTests,
                                            numCols = 100,
@@ -2161,7 +2161,7 @@
                                            noiseLevel = 0.1,
                                            doResets = False)
 
-      print "Test HL2"
+      print("Test HL2")
       tests['HL2'] = TestHL(numUniquePatterns = 20,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2173,7 +2173,7 @@
                                            noiseLevel = 0.1,
                                            doResets = False)
 
-      print "Test HL3"
+      print("Test HL3")
       tests['HL3'] = TestHL(numUniquePatterns = 30,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2186,7 +2186,7 @@
                                            noiseLevel = 0.0,
                                            doResets = True)
 
-      print "Test HL4"
+      print("Test HL4")
       tests['HL4'] = TestHL(numUniquePatterns = 30,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2199,7 +2199,7 @@
                                            noiseLevel = 0.0,
                                            doResets = False)
 
-      print "Test HL5"
+      print("Test HL5")
       tests['HL5'] = TestHL(numUniquePatterns = 30,
                                            nTests = nTests,
                                            numCols = 200,
@@ -2212,7 +2212,7 @@
                                            noiseLevel = 0.1,
                                            doResets = False)
 
-      print "Test HL6"
+      print("Test HL6")
       tests['HL6'] = nTests - TestHL(numUniquePatterns = 20,
                                      nTests = nTests,
                                      numCols = 200,
@@ -2225,21 +2225,21 @@
                                      doResets = True,
                                      hiloOn = False)
 
-      print
+      print()
 
   #---------------------------------------------------------------------------------
   nFailures = 0
-  for k,v in tests.iteritems():
+  for k,v in tests.items():
     nFailures = nFailures + v
 
   if nFailures > 0: # 1 to account for H1
-    print "There are failed tests"
-    print "Test\tn failures"
-    for k,v in tests.iteritems():
-      print k, "\t", v
+    print("There are failed tests")
+    print("Test\tn failures")
+    for k,v in tests.items():
+      print(k, "\t", v)
     assert 0
   else:
-    print "All tests pass"
+    print("All tests pass")
 
   #---------------------------------------------------------------------------------
   # Keep
@@ -2258,9 +2258,9 @@
 if __name__=="__main__":
 
   if not TEST_CPP_TM:
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
-    print "!!  WARNING: C++ TM testing is DISABLED until it can be updated."
-    print "!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
+    print("!!  WARNING: C++ TM testing is DISABLED until it can be updated.")
+    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
 
   # Three different test lengths are passed in through the command line.
   # Developer tests use --short. Autobuild does not pass in anything.
@@ -2281,7 +2281,7 @@
       if 'verbosity' in arg:
           VERBOSITY = int(sys.argv[i+1])
       if 'help' in arg:
-          print "TMTest.py --short|long --seed number|'rand' --verbosity number"
+          print("TMTest.py --short|long --seed number|'rand' --verbosity number")
           sys.exit()
       if "short" in arg:
         testLength = "short"
@@ -2295,19 +2295,19 @@
     numUniquePatterns = 50
     nTests = 1
   elif testLength == "autobuild":
-    print "Running autobuild tests"
+    print("Running autobuild tests")
     numUniquePatterns = 50
     nTests = 1
   elif testLength == "long":
     numUniquePatterns = 100
     nTests = 3
 
-  print "TM tests", testLength, "numUniquePatterns=", numUniquePatterns, "nTests=", nTests,
-  print "seed=", SEED
-  print
+  print("TM tests", testLength, "numUniquePatterns=", numUniquePatterns, "nTests=", nTests, end=' ')
+  print("seed=", SEED)
+  print()
 
   if testLength == "long":
-    print 'Testing Python TM'
+    print('Testing Python TM')
     TMClass = BacktrackingTM
     runTests(testLength)
 
@@ -2319,6 +2319,6 @@
     checkSynapseConsistency = False
 
   if TEST_CPP_TM:
-    print 'Testing C++ TM'
+    print('Testing C++ TM')
     TMClass = BacktrackingTMCPP
     runTests(testLength)
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tutorial_temporal_memory_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\tutorial_temporal_memory_test.py	(refactored)
@@ -29,9 +29,7 @@
 from nupic.support.unittesthelpers.abstract_temporal_memory_test import AbstractTemporalMemoryTest
 
 
-class TutorialTemporalMemoryTest(AbstractTemporalMemoryTest):
-  __metaclass__ = ABCMeta
-
+class TutorialTemporalMemoryTest(AbstractTemporalMemoryTest, metaclass=ABCMeta):
   VERBOSITY = 1
 
   def getPatternMachine(self):
@@ -139,7 +137,7 @@
 
     sequence = self.sequenceMachine.generateFromNumbers([0, 1])
 
-    for _ in xrange(7):
+    for _ in range(7):
       self.feedTM(sequence)
 
     self.feedTM(sequence, num=50)
@@ -153,7 +151,7 @@
 
     sequence = self.sequenceMachine.generateFromNumbers([0, 1])
 
-    for _ in xrange(7):
+    for _ in range(7):
       self.feedTM(sequence)
 
     self.feedTM(sequence, num=100)
@@ -167,7 +165,7 @@
     sequence *= 10
     sequence += [self.patternMachine.get(2), None]
 
-    for _ in xrange(4):
+    for _ in range(4):
       self.feedTM(sequence)
 
     self.feedTM(sequence, num=10)
@@ -179,10 +177,10 @@
 
     sequence = [self.patternMachine.get(0)]
 
-    for _ in xrange(4):
-      self.feedTM(sequence)
-
-    for _ in xrange(2):
+    for _ in range(4):
+      self.feedTM(sequence)
+
+    for _ in range(2):
       self.feedTM(sequence, num=10)
 
 
@@ -193,20 +191,20 @@
   def setUp(self):
     super(TutorialTemporalMemoryTest, self).setUp()
 
-    print ("\n"
+    print(("\n"
            "======================================================\n"
            "Test: {0} \n"
            "{1}\n"
            "======================================================\n"
-    ).format(self.id(), self.shortDescription())
+    ).format(self.id(), self.shortDescription()))
 
 
   def init(self, *args, **kwargs):
     super(TutorialTemporalMemoryTest, self).init(*args, **kwargs)
 
-    print "Initialized new TM with parameters:"
-    print pprint.pformat(self._computeTMParams(kwargs.get("overrides")))
-    print
+    print("Initialized new TM with parameters:")
+    print(pprint.pformat(self._computeTMParams(kwargs.get("overrides"))))
+    print()
 
 
   def feedTM(self, sequence, learn=True, num=1):
@@ -215,9 +213,9 @@
     super(TutorialTemporalMemoryTest, self).feedTM(
       sequence, learn=learn, num=num)
 
-    print self.tm.mmPrettyPrintTraces(self.tm.mmGetDefaultTraces(verbosity=2),
-                                    breakOnResets=self.tm.mmGetTraceResets())
-    print
+    print(self.tm.mmPrettyPrintTraces(self.tm.mmGetDefaultTraces(verbosity=2),
+                                    breakOnResets=self.tm.mmGetTraceResets()))
+    print()
 
     if learn:
       self._printConnections()
@@ -229,7 +227,7 @@
 
   def _printConnections(self):
     # This is in a helper so that it can be overridden.
-    print self.tm.mmPrettyPrintConnections()
+    print(self.tm.mmPrettyPrintConnections())
 
 
   def _showInput(self, sequence, learn=True, num=1):
@@ -238,9 +236,9 @@
       verbosity=self.VERBOSITY)
     learnText = "(learning {0})".format("enabled" if learn else "disabled")
     numText = " [{0} times]".format(num) if num > 1 else ""
-    print "Feeding sequence {0}{1}:\n{2}".format(
-      learnText, numText, sequenceText)
-    print
+    print("Feeding sequence {0}{1}:\n{2}".format(
+      learnText, numText, sequenceText))
+    print()
 
 
 
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\knn_classifier_test\classifier_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\knn_classifier_test\classifier_test.py	(refactored)
@@ -24,7 +24,7 @@
 import time
 import unittest2 as unittest
 
-import cPickle
+import pickle
 import numpy
 
 from nupic.bindings.regions.PyRegion import RealNumpyDType
@@ -69,7 +69,7 @@
     # numpy.random would be completely broken.
     # Patterns in testDict are identical to those in patternDict but for the
     # first 2% of items.
-    for i in xrange(numPatterns):
+    for i in range(numPatterns):
       patternDict[i] = dict()
       patternDict[i]['pattern'] = patterns[i]
       patternDict[i]['category'] = numpy.random.randint(0, numClasses-1)
@@ -105,7 +105,7 @@
 
     for i in patterns:
       iString = str(i.tolist())
-      if not patternDict.has_key(iString):
+      if iString not in patternDict:
         randCategory = numpy.random.randint(0, numClasses-1)
         patternDict[iString] = dict()
         patternDict[iString]['pattern'] = i
@@ -263,11 +263,11 @@
 
   LOGGER.info("Training the classifier")
   tick = time.time()
-  for i in patternDict.keys():
+  for i in list(patternDict.keys()):
     knn.learn(patternDict[i]['pattern'], patternDict[i]['category'])
   tock = time.time()
   LOGGER.info("Time Elapsed %s", tock-tick)
-  knnString = cPickle.dumps(knn)
+  knnString = pickle.dumps(knn)
   LOGGER.info("Size of the classifier is %s", len(knnString))
 
   # Run the classifier to infer categories on either the training data, or the
@@ -276,7 +276,7 @@
   tick = time.time()
   if testDict:
     LOGGER.info("Testing the classifier on the test set")
-    for i in testDict.keys():
+    for i in list(testDict.keys()):
       winner, _inferenceResult, _dist, _categoryDist \
         = knn.infer(testDict[i]['pattern'])
       if winner != testDict[i]['category']:
@@ -284,7 +284,7 @@
   else:
     LOGGER.info("Testing the classifier on the training set")
     LOGGER.info("Number of patterns: %s", len(patternDict))
-    for i in patternDict.keys():
+    for i in list(patternDict.keys()):
       LOGGER.info("Testing %s - %s %s", i, patternDict[i]['category'], \
         len(patternDict[i]['pattern']))
       winner, _inferenceResult, _dist, _categoryDist \
--- d:\nupic\src\python\python27\tests\integration\nupic\algorithms\monitor_mixin\temporal_memory_monitor_mixin_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\algorithms\monitor_mixin\temporal_memory_monitor_mixin_test.py	(refactored)
@@ -153,7 +153,7 @@
   # ==============================
 
   def _generateSequence(self):
-    numbers = range(0, 10)
+    numbers = list(range(0, 10))
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
     sequence.append(None)
     sequence *= 3
--- d:\nupic\src\python\python27\tests\integration\nupic\data\aggregation_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\data\aggregation_test.py	(refactored)
@@ -47,11 +47,11 @@
   while True:
     inRecord = input.getNextRecord()
     
-    print "Feeding in: ", inRecord
+    print("Feeding in: ", inRecord)
 
     (outRecord, aggBookmark) = aggregator.next(record = inRecord, 
                                             curInputBookmark = None)
-    print "Record out: ", outRecord
+    print("Record out: ", outRecord)
     
     if outRecord is not None:
       output.appendRecord(outRecord, None)
@@ -127,7 +127,7 @@
 
   def appendRecord(self, record, inputRef):
     if self._file == None:
-      print 'No File'
+      print('No File')
     self._file.appendRecord(record)
 
   def close(self):
@@ -536,7 +536,7 @@
       instance is constructed for every sub-test.
     """
     # Insert newline before each sub-test's output
-    print
+    print()
     return
 
 
@@ -586,11 +586,11 @@
       for i in range(1,len(outputRecords)):
         diffs.append(outputRecords[i][timeFieldIdx] - \
                      outputRecords[i-1][timeFieldIdx])
-      positiveTimeFlow = map((lambda x: x < datetime.timedelta(seconds=0)), 
-                            diffs)
+      positiveTimeFlow = list(map((lambda x: x < datetime.timedelta(seconds=0)), 
+                            diffs))
       #Make sure that old records are in the aggregated output and at the same
       #time make sure that they are in consecutive order after being inserted
-      self.assertEquals(sum(positiveTimeFlow), 1)
+      self.assertEqual(sum(positiveTimeFlow), 1)
         
     return
 
@@ -634,8 +634,8 @@
       dataOutput.close()
 
       for r in FileRecordStream(outputFile):
-        print r
-      print '-' * 30
+        print(r)
+      print('-' * 30)
 
     return
 
@@ -643,7 +643,7 @@
   def test_GenerateDataset(self):
     dataset = 'extra/gym/gym.csv'
 
-    print "Using input dataset: ", dataset
+    print("Using input dataset: ", dataset)
 
     filename = resource_filename("nupic.datafiles", dataset)
 
@@ -669,42 +669,42 @@
     outputFile = handle.name
     handle.close()
 
-    print "Expected outputFile path: ", outputFile
-
-    print "Files in the destination folder before the test:"
-    print os.listdir(os.path.abspath(os.path.dirname(
+    print("Expected outputFile path: ", outputFile)
+
+    print("Files in the destination folder before the test:")
+    print(os.listdir(os.path.abspath(os.path.dirname(
       resource_filename("nupic.datafiles", dataset)))
-    )
+    ))
 
     if os.path.isfile(outputFile):
-      print "Removing existing outputFile: ", outputFile
+      print("Removing existing outputFile: ", outputFile)
       os.remove(outputFile)
 
     self.assertFalse(os.path.exists(outputFile),
                      msg="Shouldn't exist, but does: " + str(outputFile))
 
     result = generateDataset(aggregationOptions, dataset, outputFile)
-    print "generateDataset() returned: ", result
+    print("generateDataset() returned: ", result)
 
     f1 = os.path.abspath(os.path.normpath(result))
-    print "normalized generateDataset() result path: ", f1
+    print("normalized generateDataset() result path: ", f1)
     f2 = os.path.normpath(outputFile)
-    print "normalized outputFile path: ", f2
+    print("normalized outputFile path: ", f2)
     self.assertEqual(f1, f2)
 
-    print "Checking for presence of outputFile: ", outputFile
+    print("Checking for presence of outputFile: ", outputFile)
     self.assertTrue(
       os.path.isfile(outputFile),
       msg="Missing outputFile: %r; normalized generateDataset() result: %r" % (
         outputFile, f1))
 
-    print "Files in the destination folder after the test:"
-    print os.listdir(os.path.abspath(os.path.dirname(
+    print("Files in the destination folder after the test:")
+    print(os.listdir(os.path.abspath(os.path.dirname(
       resource_filename("nupic.datafiles", dataset)
-    )))
-
-    print result
-    print '-' * 30
+    ))))
+
+    print(result)
+    print('-' * 30)
 
     return
 
@@ -713,7 +713,7 @@
     # Cleanup previous files if exist
     import glob
     for f in glob.glob('gap.*'):
-      print 'Removing', f
+      print('Removing', f)
       os.remove(f)
 
     #class TestParser(BaseParser):
@@ -795,8 +795,8 @@
       msg="result = '%s'; outputFile = '%s'" % (result, outputFile))
     self.assertTrue(os.path.isfile(outputFile),
                     msg="outputFile missing or is not file: %r" % (outputFile))
-    print outputFile
-    print '-' * 30
+    print(outputFile)
+    print('-' * 30)
 
     s = ''
     for r in FileRecordStream(outputFile):
@@ -861,7 +861,7 @@
 
     result = []
     with FileRecordStream(outputFile) as f:
-      print f.getFields()
+      print(f.getFields())
       for r in f:
         result.append(r)
 
@@ -921,7 +921,7 @@
 
     result = []
     with FileRecordStream(outputFile) as f:
-      print f.getFields()
+      print(f.getFields())
       for r in f:
         result.append(r)
 
--- d:\nupic\src\python\python27\tests\integration\nupic\engine\network_checkpoint_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\engine\network_checkpoint_test.py	(refactored)
@@ -26,7 +26,7 @@
 from nupic.regions.sp_region import SPRegion
 from nupic.regions.tm_region import TMRegion
 
-from network_creation_common import createAndRunNetwork
+from .network_creation_common import createAndRunNetwork
 
 try:
   import capnp
@@ -60,7 +60,7 @@
 
     self.assertEqual(len(results1), len(results2))
 
-    for i in xrange(len(results1)):
+    for i in range(len(results1)):
       result1 = list(results1[i].nonzero()[0])
       result2 = list(results2[i].nonzero()[0])
       self.assertEqual(result1, result2,
@@ -80,7 +80,7 @@
 
     self.assertEqual(len(results1), len(results2))
 
-    for i in xrange(len(results1)):
+    for i in range(len(results1)):
       result1 = list(results1[i].nonzero()[0])
       result2 = list(results2[i].nonzero()[0])
       self.assertEqual(result1, result2,
@@ -90,7 +90,7 @@
   def compareArrayResults(self, results1, results2):
     self.assertEqual(len(results1), len(results2))
 
-    for i in xrange(len(results1)):
+    for i in range(len(results1)):
       result1 = list(results1[i].nonzero()[0])
       result2 = list(results2[i].nonzero()[0])
 
--- d:\nupic\src\python\python27\tests\integration\nupic\engine\network_creation_common.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\engine\network_creation_common.py	(refactored)
@@ -214,7 +214,7 @@
 
     results = []
 
-    for i in xrange(_NUM_RECORDS):
+    for i in range(_NUM_RECORDS):
       if checkpointMidway and i == (_NUM_RECORDS / 2):
         network = saveAndLoadNetwork(network)
 
--- d:\nupic\src\python\python27\tests\integration\nupic\engine\network_testnode_interchangeability.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\engine\network_testnode_interchangeability.py	(refactored)
@@ -107,7 +107,7 @@
     # compute() behavior.
     l1output = level1.getOutputData("bottomUpOut")
     self.assertEqual(len(l1output), 48) # 24 nodes; 2 values per node
-    for i in xrange(24):
+    for i in range(24):
       self.assertEqual(l1output[2*i], 0)      # size of input to each node is 0
       self.assertEqual(l1output[2*i+1], i)    # node number
 
@@ -129,7 +129,7 @@
     outputVals.append(3 + (12 + 13 + 18 + 19))
     outputVals.append(4 + (14 + 15 + 20 + 21))
     outputVals.append(5 + (16 + 17 + 22 + 23))
-    for i in xrange(6):
+    for i in range(6):
       if l2output[2*i] != 8:
         LOGGER.info(l2output[2*i])
         # from dbgp.client import brk; brk(port=9019)
@@ -150,11 +150,11 @@
 
     # Outputs are all the same except that the first output is
     # incremented by the iteration number
-    for i in xrange(24):
+    for i in range(24):
       self.assertEqual(l1output[2*i], 1)
       self.assertEqual(l1output[2*i+1], i)
 
-    for i in xrange(6):
+    for i in range(6):
       self.assertEqual(l2output[2*i], 9)
       self.assertEqual(l2output[2*i+1], outputVals[i] + 4)
 
--- d:\nupic\src\python\python27\tests\integration\nupic\engine\network_twonode_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\engine\network_twonode_test.py	(refactored)
@@ -67,28 +67,28 @@
     # Check everything
     # =====================================================
     dims = level1.getDimensions()
-    self.assertEquals(len(dims), 2)
-    self.assertEquals(dims[0], 6)
-    self.assertEquals(dims[1], 4)
+    self.assertEqual(len(dims), 2)
+    self.assertEqual(dims[0], 6)
+    self.assertEqual(dims[1], 4)
 
     dims = level2.getDimensions()
-    self.assertEquals(len(dims), 2)
-    self.assertEquals(dims[0], 3)
-    self.assertEquals(dims[1], 2)
+    self.assertEqual(len(dims), 2)
+    self.assertEqual(dims[0], 3)
+    self.assertEqual(dims[1], 2)
 
     # Check L1 output. "False" means don't copy, i.e.
     # get a pointer to the actual output
     # Actual output values are determined by the TestNode
     # compute() behavior.
     l1output = level1.getOutputData("bottomUpOut")
-    self.assertEquals(len(l1output), 48) # 24 nodes; 2 values per node
-    for i in xrange(24):
-      self.assertEquals(l1output[2*i], 0)      # size of input to each node is 0
-      self.assertEquals(l1output[2*i+1], i)    # node number
+    self.assertEqual(len(l1output), 48) # 24 nodes; 2 values per node
+    for i in range(24):
+      self.assertEqual(l1output[2*i], 0)      # size of input to each node is 0
+      self.assertEqual(l1output[2*i+1], i)    # node number
 
     # check L2 output.
     l2output = level2.getOutputData("bottomUpOut", )
-    self.assertEquals(len(l2output), 12) # 6 nodes; 2 values per node
+    self.assertEqual(len(l2output), 12) # 6 nodes; 2 values per node
     # Output val = node number + sum(inputs)
     # Can compute from knowing L1 layout
     #
@@ -104,9 +104,9 @@
     outputVals.append(3 + (12 + 13 + 18 + 19))
     outputVals.append(4 + (14 + 15 + 20 + 21))
     outputVals.append(5 + (16 + 17 + 22 + 23))
-    for i in xrange(6):
-      self.assertEquals(l2output[2*i], 8) # size of input for each node is 8
-      self.assertEquals(l2output[2*i+1], outputVals[i])
+    for i in range(6):
+      self.assertEqual(l2output[2*i], 8) # size of input for each node is 8
+      self.assertEqual(l2output[2*i+1], outputVals[i])
 
 
     # =====================================================
@@ -121,13 +121,13 @@
 
     # Outputs are all the same except that the first output is
     # incremented by the iteration number
-    for i in xrange(24):
-      self.assertEquals(l1output[2*i], 1)
-      self.assertEquals(l1output[2*i+1], i)
+    for i in range(24):
+      self.assertEqual(l1output[2*i], 1)
+      self.assertEqual(l1output[2*i+1], i)
 
-    for i in xrange(6):
-      self.assertEquals(l2output[2*i], 9)
-      self.assertEquals(l2output[2*i+1], outputVals[i] + 4)
+    for i in range(6):
+      self.assertEqual(l2output[2*i], 9)
+      self.assertEqual(l2output[2*i+1], outputVals[i] + 4)
 
 
   def testLinkingDownwardDimensions(self):
@@ -143,8 +143,8 @@
     net.initialize()
 
     # Level1 should now have dimensions [6, 4]
-    self.assertEquals(level1.getDimensions()[0], 6)
-    self.assertEquals(level1.getDimensions()[1], 4)
+    self.assertEqual(level1.getDimensions()[0], 6)
+    self.assertEqual(level1.getDimensions()[1], 4)
 
     #
     # We get nice error messages when network can't be initialized
--- d:\nupic\src\python\python27\tests\integration\nupic\engine\temporal_memory_compatibility_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\engine\temporal_memory_compatibility_test.py	(refactored)
@@ -25,7 +25,7 @@
 
 from nupic.regions.tm_region import TMRegion
 
-from network_creation_common import createAndRunNetwork
+from .network_creation_common import createAndRunNetwork
 
 
 
@@ -51,7 +51,7 @@
   def compareArrayResults(self, results1, results2):
     self.assertEqual(len(results1), len(results2))
 
-    for i in xrange(len(results1)):
+    for i in range(len(results1)):
       result1 = list(results1[i].nonzero()[0])
       result2 = list(results2[i].nonzero()[0])
 
--- d:\nupic\src\python\python27\tests\integration\nupic\engine\vector_file_sensor_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\engine\vector_file_sensor_test.py	(refactored)
@@ -85,9 +85,9 @@
     """Run all the tests in our suite, catching any exceptions that might be
     thrown.
     """
-    print 'VectorFileSensorTest parameters:'
-    print 'PYTHONPATH: %s' % os.environ.get('PYTHONPATH', 'NOT SET')
-    print 'filename: %s' % self.filename
+    print('VectorFileSensorTest parameters:')
+    print('PYTHONPATH: %s' % os.environ.get('PYTHONPATH', 'NOT SET'))
+    print('filename: %s' % self.filename)
 
     self._testRunWithoutFile()
     self._testNetLoad()
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\expgenerator_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\expgenerator_test.py	(refactored)
@@ -308,7 +308,7 @@
     self.assertDictEqual(tmpAggregationInfo, minAggregation)
 
     predictAheadTime = dict(minAggregation)
-    for key in predictAheadTime.iterkeys():
+    for key in predictAheadTime.keys():
       predictAheadTime[key] *=  predictionSteps
     self.assertEqual(base.config['predictAheadTime'],
                      predictAheadTime)
@@ -467,13 +467,13 @@
                    oneGram,
                    trivialMetric,
                    legacyMetric=None):
-    print "base.control"
+    print("base.control")
     pprint.pprint(base.control)
     #taskMetrics = base.control['tasks'][0]['taskControl']['metrics']
     taskMetrics = base.control['metrics']
 
     for metricSpec in taskMetrics:
-      print metricSpec.metric
+      print(metricSpec.metric)
       self.assertTrue(metricSpec.metric in ["multiStep", optimizeMetric,
                                             movingBaseline, oneGram,
                                             nupicScore, trivialMetric,
@@ -500,8 +500,8 @@
                      ":window=%d:field=%s" % \
                                 (optimizeMetric, experiment_generator.METRIC_WINDOW,
                                  predictedField))
-    print "perm.minimize=",perm.minimize
-    print "optimizeString=",optimizeString
+    print("perm.minimize=",perm.minimize)
+    print("optimizeString=",optimizeString)
     self.assertEqual(perm.minimize, optimizeString,
                      msg="got: %s" % perm.minimize)
 
@@ -621,7 +621,7 @@
     actEncoderFields = set()
     actEncoderNames = set()
     for _, encoder in (
-        base.config['modelParams']['sensorParams']['encoders'].iteritems()):
+        iter(base.config['modelParams']['sensorParams']['encoders'].items())):
       actEncoderFields.add(encoder['fieldname'])
       actEncoderNames.add(encoder['name'])
 
@@ -649,7 +649,7 @@
     actEncoderFields = set()
     actEncoderNames = set()
     for _, encoder in (
-        base.config['modelParams']['sensorParams']['encoders'].iteritems()):
+        iter(base.config['modelParams']['sensorParams']['encoders'].items())):
       actEncoderFields.add(encoder['fieldname'])
       actEncoderNames.add(encoder['name'])
 
@@ -676,7 +676,7 @@
     minValues = set()
     maxValues = set()
     for _, encoder in (
-        base.config['modelParams']['sensorParams']['encoders'].iteritems()):
+        iter(base.config['modelParams']['sensorParams']['encoders'].items())):
       actEncoderFields.add(encoder['fieldname'])
       actEncoderNames.add(encoder['name'])
       actEncoderTypes.add(encoder['type'])
@@ -711,7 +711,7 @@
     minValues = set()
     maxValues = set()
     for _, encoder in (
-        base.config['modelParams']['sensorParams']['encoders'].iteritems()):
+        iter(base.config['modelParams']['sensorParams']['encoders'].items())):
       actEncoderFields.add(encoder['fieldname'])
       actEncoderNames.add(encoder['name'])
       actEncoderTypes.add(encoder['type'])
@@ -1018,13 +1018,13 @@
     # --------------------------------------------------------------------
     (base, perms) = self.getModules(expDesc)
 
-    print "base.config['modelParams']:"
+    print("base.config['modelParams']:")
     pprint.pprint(base.config['modelParams'])
-    print "perms.permutations"
+    print("perms.permutations")
     pprint.pprint(perms.permutations)
-    print "perms.minimize"
+    print("perms.minimize")
     pprint.pprint(perms.minimize)
-    print "expDesc"
+    print("expDesc")
     pprint.pprint(expDesc)
 
     # Make sure we have the expected info in the base description file
@@ -1185,7 +1185,7 @@
 
     self.assertNotIn(
       'consumption',
-      base.config['modelParams']['sensorParams']['encoders'].keys())
+      list(base.config['modelParams']['sensorParams']['encoders'].keys()))
 
 
   def test_DeltaEncoders(self):
@@ -1792,7 +1792,7 @@
                      expDesc['inferenceArgs']['predictedField'])
 
     self.assertNotIn('consumption',
-             base.config['modelParams']['sensorParams']['encoders'].keys())
+             list(base.config['modelParams']['sensorParams']['encoders'].keys()))
 
 
     # The SP and TM should both be disabled
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\hotgym_regression_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\hotgym_regression_test.py	(refactored)
@@ -54,7 +54,7 @@
           resultsDir, "DefaultTask.TemporalMultiStep.predictionLog.csv")
       with open(resultsPath) as f:
         reader = csv.reader(f)
-        headers = reader.next()
+        headers = next(reader)
         self.assertEqual(headers[14],
                          "multiStepBestPredictions:multiStep:errorMetric='aae':"
                          "steps=1:window=1000:field=consumption")
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\htmpredictionmodel_serialization_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\htmpredictionmodel_serialization_test.py	(refactored)
@@ -55,7 +55,7 @@
   'version': 1,
   'aggregationInfo': {
     'days': 0,
-    'fields': [(u'c1', 'sum'), (u'c0', 'first')],
+    'fields': [('c1', 'sum'), ('c0', 'first')],
     'hours': 1,
     'microseconds': 0,
     'milliseconds': 0,
@@ -76,12 +76,12 @@
     'sensorParams': {
       'verbosity' : 0,
       'encoders': {
-        u'consumption':    {  'clipInput': True,
-          'fieldname': u'consumption',
+        'consumption':    {  'clipInput': True,
+          'fieldname': 'consumption',
           'maxval': 100.0,
           'minval': 0.0,
           'n': 50,
-          'name': u'c1',
+          'name': 'c1',
           'type': 'ScalarEncoder',
           'w': 21},
       },
@@ -130,9 +130,9 @@
       'steps': '1,5',
     },
 
-    'anomalyParams': {  u'anomalyCacheRecords': None,
-                        u'autoDetectThreshold': None,
-                        u'autoDetectWaitRecords': 2184},
+    'anomalyParams': {  'anomalyCacheRecords': None,
+                        'autoDetectThreshold': None,
+                        'autoDetectWaitRecords': 2184},
 
     'trainSPNetOnlyIfRequested': False,
   },
@@ -144,7 +144,7 @@
   'model': 'HTMPrediction',
   'version': 1,
   'aggregationInfo': {  'days': 0,
-                        'fields': [(u'c1', 'sum'), (u'c0', 'first')],
+                        'fields': [('c1', 'sum'), ('c0', 'first')],
                         'hours': 1,
                         'microseconds': 0,
                         'milliseconds': 0,
@@ -165,12 +165,12 @@
     'sensorParams': {
       'verbosity' : 0,
       'encoders': {
-        u'consumption':    {  'clipInput': True,
-                              'fieldname': u'consumption',
+        'consumption':    {  'clipInput': True,
+                              'fieldname': 'consumption',
                               'maxval': 100.0,
                               'minval': 0.0,
                               'n': 50,
-                              'name': u'c1',
+                              'name': 'c1',
                               'type': 'ScalarEncoder',
                               'w': 21},
       },
@@ -219,9 +219,9 @@
       'steps': '1,5',
     },
 
-    'anomalyParams': {  u'anomalyCacheRecords': None,
-                        u'autoDetectThreshold': None,
-                        u'autoDetectWaitRecords': 2184},
+    'anomalyParams': {  'anomalyCacheRecords': None,
+                        'autoDetectThreshold': None,
+                        'autoDetectWaitRecords': 2184},
 
     'trainSPNetOnlyIfRequested': False,
   },
@@ -237,7 +237,7 @@
     headers = ['timestamp', 'consumption']
 
     record = [datetime.datetime(2013, 12, 12), numpy.random.uniform(100)]
-    modelInput = dict(zip(headers, record))
+    modelInput = dict(list(zip(headers, record)))
     m1.run(modelInput)
 
     # Serialize
@@ -256,7 +256,7 @@
 
     # Run computes on m1 & m2 and compare results
     record = [datetime.datetime(2013, 12, 14), numpy.random.uniform(100)]
-    modelInput = dict(zip(headers, record))
+    modelInput = dict(list(zip(headers, record)))
     # Use deepcopy to guarantee no input side-effect between calls
     r1 = m1.run(copy.deepcopy(modelInput))
     r2 = m2.run(copy.deepcopy(modelInput))
@@ -308,7 +308,7 @@
     headers = ['timestamp', 'consumption']
 
     record = [datetime.datetime(2013, 12, 12), numpy.random.uniform(100)]
-    modelInput = dict(zip(headers, record))
+    modelInput = dict(list(zip(headers, record)))
     m1.run(modelInput)
 
     # Serialize
@@ -327,7 +327,7 @@
     # Running the desrialized m2 without redundant enableInference call should
     # work
     record = [datetime.datetime(2013, 12, 14), numpy.random.uniform(100)]
-    modelInput = dict(zip(headers, record))
+    modelInput = dict(list(zip(headers, record)))
     m2.run(modelInput)
 
     # Check that disabled inference is saved, too (since constructor defaults to
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_stress_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_stress_test.py	(refactored)
@@ -42,7 +42,7 @@
     'model': "HTMPrediction",
     'version': 1,
     'aggregationInfo': {  'days': 0,
-        'fields': [(u'c1', 'sum'), (u'c0', 'first')],
+        'fields': [('c1', 'sum'), ('c0', 'first')],
         'hours': 1,
         'microseconds': 0,
         'milliseconds': 0,
@@ -57,12 +57,12 @@
         'sensorParams': {
             'verbosity' : 0,
             'encoders': {
-                u'consumption':    {  'clipInput': True,
-                    'fieldname': u'consumption',
+                'consumption':    {  'clipInput': True,
+                    'fieldname': 'consumption',
                     'maxval': 100.0,
                     'minval': 0.0,
                     'n': 50,
-                    'name': u'c1',
+                    'name': 'c1',
                     'type': 'ScalarEncoder',
                     'w': 21},},
             'sensorAutoReset' : None,
@@ -109,9 +109,9 @@
             'steps': '1,5',
         },
 
-        'anomalyParams': {  u'anomalyCacheRecords': None,
-    u'autoDetectThreshold': None,
-    u'autoDetectWaitRecords': 2184},
+        'anomalyParams': {  'anomalyCacheRecords': None,
+    'autoDetectThreshold': None,
+    'autoDetectWaitRecords': 2184},
 
         'trainSPNetOnlyIfRequested': False,
     },
@@ -130,7 +130,7 @@
 
       for _ in range(2):
         record = [datetime.datetime(2013, 12, 12), numpy.random.uniform(100)]
-        modelInput = dict(zip(headers, record))
+        modelInput = dict(list(zip(headers, record)))
         model.run(modelInput)
 
       # Save and load a checkpoint after each batch. Clean up.
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_experiment_results_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_experiment_results_test.py	(refactored)
@@ -324,7 +324,7 @@
       for path in toDelete:
         if not os.path.exists(path):
           continue
-        print "Removing %s ..." % path
+        print("Removing %s ..." % path)
         if os.path.isfile(path):
           os.remove(path)
         else:
@@ -334,22 +334,22 @@
       # ------------------------------------------------------------------------
       # Run the test.
       args = test.get('args', [])
-      print "Running experiment %s ..." % (expDirectory)
+      print("Running experiment %s ..." % (expDirectory))
       command = ['python', runExperiment, expDirectory] + args
       retVal = call(command)
 
       # If retVal is non-zero and this was not a negative test or if retVal is
       # zero and this is a negative test something went wrong.
       if retVal:
-        print "Details of failed test: %s" % test
-        print("TestIdx %d, OPF experiment '%s' failed with return code %i." %
-              (testIdx, expDirectory, retVal))
+        print("Details of failed test: %s" % test)
+        print(("TestIdx %d, OPF experiment '%s' failed with return code %i." %
+              (testIdx, expDirectory, retVal)))
       self.assertFalse(retVal)
 
 
       # -----------------------------------------------------------------------
       # Check the results
-      for (key, expValues) in test['results'].items():
+      for (key, expValues) in list(test['results'].items()):
         (logFilename, colName) = key
 
         # Open the prediction log file
@@ -357,16 +357,16 @@
                                                 logFilename))
         colNames = [x[0] for x in logFile.getFields()]
         if not colName in colNames:
-          print "TestIdx %d: %s not one of the columns in " \
+          print("TestIdx %d: %s not one of the columns in " \
             "prediction log file. Available column names are: %s" % (testIdx,
-                    colName, colNames)
+                    colName, colNames))
         self.assertTrue(colName in colNames)
         colIndex = colNames.index(colName)
 
         # Read till we get to the last line
         while True:
           try:
-            row = logFile.next()
+            row = next(logFile)
           except StopIteration:
             break
         result = row[colIndex]
@@ -374,33 +374,33 @@
         # Save summary of results
         summaryOfResults.append((expDirectory, colName, result))
 
-        print "Actual result for %s, %s:" % (expDirectory, colName), result
-        print "Expected range:", expValues
+        print("Actual result for %s, %s:" % (expDirectory, colName), result)
+        print("Expected range:", expValues)
         failed = (expValues[0] is not None and result < expValues[0]) \
             or (expValues[1] is not None and result > expValues[1])
         if failed:
-          print ("TestIdx %d: Experiment %s failed. \nThe actual result"
+          print(("TestIdx %d: Experiment %s failed. \nThe actual result"
              " for %s (%s) was outside the allowed range of %s" % (testIdx,
-              expDirectory, colName, result, expValues))
+              expDirectory, colName, result, expValues)))
         else:
-          print "  Within expected range."
+          print("  Within expected range.")
         self.assertFalse(failed)
 
 
     # =======================================================================
     # Print summary of results:
-    print
-    print "Summary of results in all experiments run:"
-    print "========================================="
+    print()
+    print("Summary of results in all experiments run:")
+    print("=========================================")
     prevExpDir = None
     for (expDir, key, results) in summaryOfResults:
       if expDir != prevExpDir:
-        print
-        print expDir
+        print()
+        print(expDir)
         prevExpDir = expDir
-      print "  %s: %s" % (key, results)
-
-    print "\nElapsed time: %.1f seconds" % (time.time() - startTime)
+      print("  %s: %s" % (key, results))
+
+    print("\nElapsed time: %.1f seconds" % (time.time() - startTime))
 
 
 
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_experiments_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_experiments_test.py	(refactored)
@@ -50,12 +50,12 @@
     for d in dirnames[:]:
       if d in excludeDirs:
         dirnames.remove(d)
-        print "EXCLUDING %s..." % (os.path.join(dirpath, d))
+        print("EXCLUDING %s..." % (os.path.join(dirpath, d)))
         
       # If this directory is UNDER_DEVELOPMENT, exclude it
       elif 'UNDER_DEVELOPMENT' in os.listdir(os.path.join(dirpath, d)):
         dirnames.remove(d)
-        print "EXCLUDING %s..." % (os.path.join(dirpath, d))
+        print("EXCLUDING %s..." % (os.path.join(dirpath, d)))
 
     for f in filenames:
       if f==filename:
@@ -106,25 +106,25 @@
     successExperiments = []
     for expDirPath in expDirPathList:
       if os.path.exists(os.path.join(expDirPath, "UNDER_DEVELOPMENT")):
-        print "Skipping experiment: %s -- under development" % expDirPath
+        print("Skipping experiment: %s -- under development" % expDirPath)
         continue
-      print "Running experiment: %s" % expDirPath
+      print("Running experiment: %s" % expDirPath)
       try:
         if RUN_ALL_ITERATIONS:
           runReducedExperiment(expDirPath, False)
         else:
           runReducedExperiment(expDirPath)
       except KeyboardInterrupt:
-        print "Keyboard interrupt received. Exiting"
+        print("Keyboard interrupt received. Exiting")
         sys.exit(1)
       except:
         failedExperiments.append(expDirPath)
-        print
-        print "Unable to run experiment: %s" % expDirPath
-        print "See the trace below-"
+        print()
+        print("Unable to run experiment: %s" % expDirPath)
+        print("See the trace below-")
         traceback.print_exc()
       else:
-        print "Successfully ran experiment: %s" % expDirPath
+        print("Successfully ran experiment: %s" % expDirPath)
         successExperiments.append(expDirPath)
 
     self.assertEqual(len(failedExperiments), 0)
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_region_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_region_test.py	(refactored)
@@ -160,7 +160,7 @@
   # ==========================================================================
   # Add the SP if requested
   if addSP:
-    print "Adding SPRegion"
+    print("Adding SPRegion")
     g_spRegionConfig['inputWidth'] = encoder.getWidth()
     n.addRegion("level1SP", "py.SPRegion", json.dumps(g_spRegionConfig))
 
@@ -176,7 +176,7 @@
   if addTP and addSP:
     # Add the TM on top of SP if requested
     # The input width of the TM is set to the column count of the SP
-    print "Adding TMRegion on top of SP"
+    print("Adding TMRegion on top of SP")
     g_tpRegionConfig['inputWidth'] = g_spRegionConfig['columnCount']
     n.addRegion("level1TP", "py.TMRegion", json.dumps(g_tpRegionConfig))
     n.link("level1SP", "level1TP", "UniformLink", "")
@@ -188,7 +188,7 @@
   elif addTP:
     # Add a lone TMRegion if requested
     # The input width of the TM is set to the encoder width
-    print "Adding TMRegion"
+    print("Adding TMRegion")
     g_tpRegionConfig['inputWidth'] = encoder.getWidth()
     n.addRegion("level1TP", "py.TMRegion", json.dumps(g_tpRegionConfig))
 
@@ -215,13 +215,13 @@
     results.
     """
 
-    print "Creating network..."
+    print("Creating network...")
 
     netOPF = _createOPFNetwork()
     level1OPF = netOPF.regions['level1SP']
 
     # ==========================================================================
-    print "Training network for 500 iterations"
+    print("Training network for 500 iterations")
     level1OPF.setParameter('learningMode', 1)
     level1OPF.setParameter('inferenceMode', 0)
     netOPF.run(500)
@@ -232,7 +232,7 @@
     # Save network and reload as a second instance. We need to reset the data
     # source for the unsaved network so that both instances start at the same
     # place
-    print "Saving and reload network"
+    print("Saving and reload network")
     _, tmpNetworkFilename = _setupTempDirectory("trained.nta")
     netOPF.save(tmpNetworkFilename)
     netOPF2 = Network(tmpNetworkFilename)
@@ -244,8 +244,8 @@
     sensor.dataSource.setAutoRewind(True)
 
     # ==========================================================================
-    print "Running inference on the two networks for 100 iterations"
-    for _ in xrange(100):
+    print("Running inference on the two networks for 100 iterations")
+    for _ in range(100):
       netOPF2.run(1)
       netOPF.run(1)
       l1outputOPF2 = level1OPF2.getOutputData("bottomUpOut")
@@ -259,7 +259,7 @@
   def testMaxEnabledPhase(self):
     """ Test maxEnabledPhase"""
 
-    print "Creating network..."
+    print("Creating network...")
 
     netOPF = _createOPFNetwork(addSP = True, addTP = True)
     netOPF.initialize()
@@ -271,21 +271,21 @@
     tm.setParameter('learningMode', 0)
     tm.setParameter('inferenceMode', 0)
 
-    print "maxPhase,maxEnabledPhase = ", netOPF.maxPhase, \
-                                      netOPF.getMaxEnabledPhase()
+    print("maxPhase,maxEnabledPhase = ", netOPF.maxPhase, \
+                                      netOPF.getMaxEnabledPhase())
     self.assertEqual(netOPF.maxPhase, 2)
     self.assertEqual(netOPF.getMaxEnabledPhase(), 2)
 
-    print "Setting setMaxEnabledPhase to 1"
+    print("Setting setMaxEnabledPhase to 1")
     netOPF.setMaxEnabledPhase(1)
-    print "maxPhase,maxEnabledPhase = ", netOPF.maxPhase, \
-                                      netOPF.getMaxEnabledPhase()
+    print("maxPhase,maxEnabledPhase = ", netOPF.maxPhase, \
+                                      netOPF.getMaxEnabledPhase())
     self.assertEqual(netOPF.maxPhase, 2)
     self.assertEqual(netOPF.getMaxEnabledPhase(), 1)
 
     netOPF.run(1)
 
-    print "RUN SUCCEEDED"
+    print("RUN SUCCEEDED")
 
     # TODO: The following does not run and is probably flawed.
     """
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\opf_checkpoint_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\opf_checkpoint_test.py	(refactored)
@@ -67,8 +67,8 @@
     experimentLabel = "%s prediction comparison" % \
                         ("Temporal" if temporal else "Non-Temporal")
 
-    print "%s: Performing comparison of OPF prediction CSV files %r and %r" % (
-            experimentLabel, path1, path2)
+    print("%s: Performing comparison of OPF prediction CSV files %r and %r" % (
+            experimentLabel, path1, path2))
 
     # Open CSV readers
     #
@@ -112,22 +112,22 @@
     if temporal:
       # Skip the first data rows for temporal tests, since they don't contain
       # prediction values.
-      _skipOpf1Row = opf1CsvReader.next()
+      _skipOpf1Row = next(opf1CsvReader)
       opf1CurrentDataRowIndex += 1
-      _skipOpf2Row = opf2CsvReader.next()
+      _skipOpf2Row = next(opf2CsvReader)
       opf2CurrentDataRowIndex += 1
 
 
-    fieldsIndexesToCompare = tuple(xrange(2, len(opf1FieldNames), 2))
+    fieldsIndexesToCompare = tuple(range(2, len(opf1FieldNames), 2))
 
     self.assertGreater(len(fieldsIndexesToCompare), 0)
 
-    print ("%s: Comparing fields at indexes: %s; "
+    print(("%s: Comparing fields at indexes: %s; "
            "opf1Labels: %s; opf2Labels: %s") % (
             experimentLabel,
             fieldsIndexesToCompare,
             [opf1FieldNames[i] for i in fieldsIndexesToCompare],
-            [opf2FieldNames[i] for i in fieldsIndexesToCompare])
+            [opf2FieldNames[i] for i in fieldsIndexesToCompare]))
 
 
     for i in fieldsIndexesToCompare:
@@ -141,26 +141,26 @@
     while True:
 
       try:
-        opf1Row = opf1CsvReader.next()
+        opf1Row = next(opf1CsvReader)
       except StopIteration:
         opf1EOF = True
       else:
         opf1CurrentDataRowIndex += 1
 
       try:
-        opf2Row = opf2CsvReader.next()
+        opf2Row = next(opf2CsvReader)
       except StopIteration:
         opf2EOF = True
       else:
         opf2CurrentDataRowIndex += 1
 
       if opf1EOF != opf2EOF:
-        print ("%s: ERROR: Data row counts mismatch: "
+        print(("%s: ERROR: Data row counts mismatch: "
                "opf1EOF: %s, opf1CurrentDataRowIndex: %s; "
                "opf2EOF: %s, opf2CurrentDataRowIndex: %s") % (
                   experimentLabel,
                   opf1EOF, opf1CurrentDataRowIndex,
-                  opf2EOF, opf2CurrentDataRowIndex)
+                  opf2EOF, opf2CurrentDataRowIndex))
         return False
 
       if opf1EOF and opf2EOF:
@@ -178,7 +178,7 @@
 
           mismatchCount += 1
 
-          print ("%s: ERROR: mismatch in "
+          print(("%s: ERROR: mismatch in "
            "prediction values: dataRowIndex: %s, fieldIndex: %s (%r); "
            "opf1FieldValue: <%s>, opf2FieldValue: <%s>; "
            "opf1FieldValueAsFloat: %s, opf2FieldValueAsFloat: %s; "
@@ -192,7 +192,7 @@
             opf1FloatValue,
             opf2FloatValue,
             opf1Row,
-            opf2Row)
+            opf2Row))
 
           # Stop comparison if we exceeded the allowed number of mismatches
           if maxMismatches is not None and mismatchCount >= maxMismatches:
@@ -200,8 +200,8 @@
 
 
     if mismatchCount != 0:
-      print "%s: ERROR: there were %s mismatches between %r and %r" % (
-              experimentLabel, mismatchCount, path1, path2)
+      print("%s: ERROR: there were %s mismatches between %r and %r" % (
+              experimentLabel, mismatchCount, path1, path2))
       return False
 
 
@@ -209,13 +209,13 @@
     self.assertEqual(opf1CurrentDataRowIndex, opf2CurrentDataRowIndex)
 
 
-    print ("%s: Comparison of predictions "
+    print(("%s: Comparison of predictions "
            "completed: OK; number of prediction rows examined: %s; "
            "path1: %r; path2: %r") % \
               (experimentLabel,
                opf1CurrentDataRowIndex + 1,
                path1,
-               path2)
+               path2))
 
     return True
 
@@ -231,9 +231,9 @@
     csvReader = self._openCsvFile(filepath)
 
     # Advance it past the three NUPIC header lines
-    names = csvReader.next()
-    _types = csvReader.next()
-    _specials = csvReader.next()
+    names = next(csvReader)
+    _types = next(csvReader)
+    _specials = next(csvReader)
 
     return (csvReader, names)
 
@@ -330,12 +330,12 @@
 
     # Skip past the 'a' records in aPlusB
     for i in range(checkpointAt):
-      aPlusBPred.next()
+      next(aPlusBPred)
 
     # Now, read through the records that don't have predictions yet
     for i in range(predSteps):
-      aPlusBPred.next()
-      bPred.next()
+      next(aPlusBPred)
+      next(bPred)
 
     # Now, compare predictions in the two files
     rowIdx = checkpointAt + predSteps + 4 - 1
@@ -343,8 +343,8 @@
     while True:
       rowIdx += 1
       try:
-        rowAPB = aPlusBPred.next()
-        rowB = bPred.next()
+        rowAPB = next(aPlusBPred)
+        rowB = next(bPred)
 
         # Compare actuals
         self.assertEqual(rowAPB[actValueColIdx], rowB[actValueColIdx],
@@ -357,8 +357,8 @@
         predB = eval(rowB[predValueColIdx])
 
         # Sort with highest probabilities first
-        predAPB = [(a, b) for b, a in predAPB.items()]
-        predB = [(a, b) for b, a in predB.items()]
+        predAPB = [(a, b) for b, a in list(predAPB.items())]
+        predB = [(a, b) for b, a in list(predB.items())]
         predAPB.sort(reverse=True)
         predB.sort(reverse=True)
 
@@ -397,7 +397,7 @@
     shutil.rmtree(getCheckpointParentDir(bExpDir))
     shutil.rmtree(getCheckpointParentDir(aPlusBExpDir))
 
-    print "Predictions match!"
+    print("Predictions match!")
 
 
   @staticmethod
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\backwards_compatibility\base.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\backwards_compatibility\base.py	(refactored)
@@ -107,7 +107,7 @@
       'seconds': 0,
       'weeks': 0,
       'years': 0,
-      'fields': [(u'c1', 'first'), (u'c0', 'first')],
+      'fields': [('c1', 'first'), ('c0', 'first')],
       },
 
     'predictAheadTime': None,
@@ -131,22 +131,22 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'c0_timeOfDay':     {   'fieldname': u'c0',
-    'name': u'c0_timeOfDay',
+                'c0_timeOfDay':     {   'fieldname': 'c0',
+    'name': 'c0_timeOfDay',
     'timeOfDay': (21, 1),
     'type': 'DateEncoder'},
-  u'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
-    'fieldname': u'c0',
-    'name': u'c0_dayOfWeek',
+  'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
+    'fieldname': 'c0',
+    'name': 'c0_dayOfWeek',
     'type': 'DateEncoder'},
-  u'c0_weekend':     {   'fieldname': u'c0',
-    'name': u'c0_weekend',
+  'c0_weekend':     {   'fieldname': 'c0',
+    'name': 'c0_weekend',
     'type': 'DateEncoder',
     'weekend': 21},
-  u'c1':     {   'clipInput': True,
-    'fieldname': u'c1',
+  'c1':     {   'clipInput': True,
+    'fieldname': 'c1',
     'n': 100,
-    'name': u'c1',
+    'name': 'c1',
     'type': 'AdaptiveScalarEncoder',
     'w': 21},
             },
@@ -352,15 +352,15 @@
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
   'dataset' : {   'aggregation': config['aggregationInfo'],
-        u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-        u'streams': [   {   u'columns': [u'c0', u'c1'],
-                            u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-                            u'source': 'file://%s' % (dataPath),
-                            u'first_record': config['firstRecord'],
-                            u'last_record': config['lastRecord'],
-                            u'types': [u'datetime', u'float']}],
-        u'timeField': u'c0',
-        u'version': 1},
+        'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+        'streams': [   {   'columns': ['c0', 'c1'],
+                            'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+                            'source': 'file://%s' % (dataPath),
+                            'first_record': config['firstRecord'],
+                            'last_record': config['lastRecord'],
+                            'types': ['datetime', 'float']}],
+        'timeField': 'c0',
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -373,12 +373,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'c1', u'predictionSteps': [24]},
+  "inferenceArgs":{'predictedField': 'c1', 'predictionSteps': [24]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
+    MetricSpec(field='c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\backwards_compatibility\a\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\backwards_compatibility\a\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 0,
   'lastRecord': 10,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\base.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\base.py	(refactored)
@@ -107,7 +107,7 @@
       'seconds': 0,
       'weeks': 0,
       'years': 0,
-      'fields': [(u'c1', 'first'), (u'c0', 'first')],
+      'fields': [('c1', 'first'), ('c0', 'first')],
       },
 
     'predictAheadTime': None,
@@ -131,22 +131,22 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'c0_timeOfDay':     {   'fieldname': u'c0',
-    'name': u'c0_timeOfDay',
+                'c0_timeOfDay':     {   'fieldname': 'c0',
+    'name': 'c0_timeOfDay',
     'timeOfDay': (21, 1),
     'type': 'DateEncoder'},
-  u'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
-    'fieldname': u'c0',
-    'name': u'c0_dayOfWeek',
+  'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
+    'fieldname': 'c0',
+    'name': 'c0_dayOfWeek',
     'type': 'DateEncoder'},
-  u'c0_weekend':     {   'fieldname': u'c0',
-    'name': u'c0_weekend',
+  'c0_weekend':     {   'fieldname': 'c0',
+    'name': 'c0_weekend',
     'type': 'DateEncoder',
     'weekend': 21},
-  u'c1':     {   'clipInput': True,
-    'fieldname': u'c1',
+  'c1':     {   'clipInput': True,
+    'fieldname': 'c1',
     'n': 100,
-    'name': u'c1',
+    'name': 'c1',
     'type': 'AdaptiveScalarEncoder',
     'w': 21},
             },
@@ -352,15 +352,15 @@
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
   'dataset' : {   'aggregation': config['aggregationInfo'],
-        u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-        u'streams': [   {   u'columns': [u'c0', u'c1'],
-                            u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-                            u'source': 'file://%s' % (dataPath),
-                            u'first_record': config['firstRecord'],
-                            u'last_record': config['lastRecord'],
-                            u'types': [u'datetime', u'float']}],
-        u'timeField': u'c0',
-        u'version': 1},
+        'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+        'streams': [   {   'columns': ['c0', 'c1'],
+                            'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+                            'source': 'file://%s' % (dataPath),
+                            'first_record': config['firstRecord'],
+                            'last_record': config['lastRecord'],
+                            'types': ['datetime', 'float']}],
+        'timeField': 'c0',
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -373,12 +373,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'c1', u'predictionSteps': [24]},
+  "inferenceArgs":{'predictedField': 'c1', 'predictionSteps': [24]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
+    MetricSpec(field='c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\a\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\a\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 0,
   'lastRecord': 250,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\a_plus_b\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\a_plus_b\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 0,
   'lastRecord': 500,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\b\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\non_temporal_multi_step\b\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 250,
   'lastRecord': 500,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\base.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\base.py	(refactored)
@@ -107,7 +107,7 @@
       'seconds': 0,
       'weeks': 0,
       'years': 0,
-      'fields': [(u'c1', 'first'), (u'c0', 'first')],
+      'fields': [('c1', 'first'), ('c0', 'first')],
       },
 
     'predictAheadTime': None,
@@ -131,22 +131,22 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'c0_timeOfDay':     {   'fieldname': u'c0',
-    'name': u'c0_timeOfDay',
+                'c0_timeOfDay':     {   'fieldname': 'c0',
+    'name': 'c0_timeOfDay',
     'timeOfDay': (21, 1),
     'type': 'DateEncoder'},
-  u'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
-    'fieldname': u'c0',
-    'name': u'c0_dayOfWeek',
+  'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
+    'fieldname': 'c0',
+    'name': 'c0_dayOfWeek',
     'type': 'DateEncoder'},
-  u'c0_weekend':     {   'fieldname': u'c0',
-    'name': u'c0_weekend',
+  'c0_weekend':     {   'fieldname': 'c0',
+    'name': 'c0_weekend',
     'type': 'DateEncoder',
     'weekend': 21},
-  u'c1':     {   'clipInput': True,
-    'fieldname': u'c1',
+  'c1':     {   'clipInput': True,
+    'fieldname': 'c1',
     'n': 100,
-    'name': u'c1',
+    'name': 'c1',
     'type': 'AdaptiveScalarEncoder',
     'w': 21},
             },
@@ -352,15 +352,15 @@
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
   'dataset' : {   'aggregation': config['aggregationInfo'],
-        u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-        u'streams': [   {   u'columns': [u'c0', u'c1'],
-                            u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-                            u'source': 'file://%s' % (dataPath),
-                            u'first_record': config['firstRecord'],
-                            u'last_record': config['lastRecord'],
-                            u'types': [u'datetime', u'float']}],
-        u'timeField': u'c0',
-        u'version': 1},
+        'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+        'streams': [   {   'columns': ['c0', 'c1'],
+                            'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+                            'source': 'file://%s' % (dataPath),
+                            'first_record': config['firstRecord'],
+                            'last_record': config['lastRecord'],
+                            'types': ['datetime', 'float']}],
+        'timeField': 'c0',
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -373,12 +373,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'c1', u'predictionSteps': [1]},
+  "inferenceArgs":{'predictedField': 'c1', 'predictionSteps': [1]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1], 'errorMetric': 'altMAPE'}),
+    MetricSpec(field='c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [1], 'errorMetric': 'altMAPE'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\a\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\a\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 0,
   'lastRecord': 250,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\a_plus_b\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\a_plus_b\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 0,
   'lastRecord': 500,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\b\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_anomaly\b\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 250,
   'lastRecord': 500,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\base.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\base.py	(refactored)
@@ -107,7 +107,7 @@
       'seconds': 0,
       'weeks': 0,
       'years': 0,
-      'fields': [(u'c1', 'first'), (u'c0', 'first')],
+      'fields': [('c1', 'first'), ('c0', 'first')],
       },
 
     'predictAheadTime': None,
@@ -131,22 +131,22 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                u'c0_timeOfDay':     {   'fieldname': u'c0',
-    'name': u'c0_timeOfDay',
+                'c0_timeOfDay':     {   'fieldname': 'c0',
+    'name': 'c0_timeOfDay',
     'timeOfDay': (21, 1),
     'type': 'DateEncoder'},
-  u'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
-    'fieldname': u'c0',
-    'name': u'c0_dayOfWeek',
+  'c0_dayOfWeek':     {   'dayOfWeek': (21, 1),
+    'fieldname': 'c0',
+    'name': 'c0_dayOfWeek',
     'type': 'DateEncoder'},
-  u'c0_weekend':     {   'fieldname': u'c0',
-    'name': u'c0_weekend',
+  'c0_weekend':     {   'fieldname': 'c0',
+    'name': 'c0_weekend',
     'type': 'DateEncoder',
     'weekend': 21},
-  u'c1':     {   'clipInput': True,
-    'fieldname': u'c1',
+  'c1':     {   'clipInput': True,
+    'fieldname': 'c1',
     'n': 100,
-    'name': u'c1',
+    'name': 'c1',
     'type': 'AdaptiveScalarEncoder',
     'w': 21},
             },
@@ -352,15 +352,15 @@
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
   'dataset' : {   'aggregation': config['aggregationInfo'],
-        u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-        u'streams': [   {   u'columns': [u'c0', u'c1'],
-                            u'info': u'82b42f21-7f86-47b3-bab4-3738703bf612',
-                            u'source': 'file://%s' % (dataPath),
-                            u'first_record': config['firstRecord'],
-                            u'last_record': config['lastRecord'],
-                            u'types': [u'datetime', u'float']}],
-        u'timeField': u'c0',
-        u'version': 1},
+        'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+        'streams': [   {   'columns': ['c0', 'c1'],
+                            'info': '82b42f21-7f86-47b3-bab4-3738703bf612',
+                            'source': 'file://%s' % (dataPath),
+                            'first_record': config['firstRecord'],
+                            'last_record': config['lastRecord'],
+                            'types': ['datetime', 'float']}],
+        'timeField': 'c0',
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -373,12 +373,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'c1', u'predictionSteps': [24]},
+  "inferenceArgs":{'predictedField': 'c1', 'predictionSteps': [24]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
+    MetricSpec(field='c1', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 1000, 'steps': [24], 'errorMetric': 'altMAPE'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\a\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\a\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 0,
   'lastRecord': 250,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\a_plus_b\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\a_plus_b\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 0,
   'lastRecord': 500,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\b\description.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_checkpoint_test\experiments\temporal_multi_step\b\description.py	(refactored)
@@ -33,7 +33,7 @@
 
 # the sub-experiment configuration
 config ={
-  'modelParams' : {'sensorParams': {'encoders': {u'c0_timeOfDay': None, u'c0_dayOfWeek': None, u'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, u'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
+  'modelParams' : {'sensorParams': {'encoders': {'c0_timeOfDay': None, 'c0_dayOfWeek': None, 'c1': {'name': 'c1', 'clipInput': True, 'n': 275, 'fieldname': 'c1', 'w': 21, 'type': 'AdaptiveScalarEncoder'}, 'c0_weekend': None}}, 'inferenceType': 'NontemporalMultiStep', 'spParams': {'synPermInactiveDec': 0.052500000000000005}, 'tmParams': {'minThreshold': 11, 'activationThreshold': 14, 'pamLength': 3}, 'clParams': {'alpha': 0.050050000000000004}},
 
   'firstRecord': 250,
   'lastRecord': 500,
--- d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_description_template_test\opf_description_template_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\opf\opf_description_template_test\opf_description_template_test.py	(refactored)
@@ -279,8 +279,8 @@
 def _debugOut(msg):
   if g_debug:
     callerTraceback = whoisCallersCaller()
-    print "OPF TestDescriptionTemplate (f=%s;line=%s): %s" % \
-            (callerTraceback.function, callerTraceback.lineno, msg,)
+    print("OPF TestDescriptionTemplate (f=%s;line=%s): %s" % \
+            (callerTraceback.function, callerTraceback.lineno, msg,))
     sys.stdout.flush()
 
 
--- d:\nupic\src\python\python27\tests\integration\nupic\regions\multiclass_knn_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\regions\multiclass_knn_test.py	(refactored)
@@ -121,7 +121,7 @@
                     [0.5, 0.5, 0.0],
                     [0.0, 0.5, 0.5])
     dataSource.rewind()
-    for i in xrange(8):
+    for i in range(8):
       net.run(1)
       inferredCats = classifier.getOutputData("categoriesOut")
       self.assertSequenceEqual(expectedCats[i], inferredCats.tolist(),
--- d:\nupic\src\python\python27\tests\integration\nupic\regions\single_step_sdr_classifier_test.py	(original)
+++ d:\nupic\src\python\python27\tests\integration\nupic\regions\single_step_sdr_classifier_test.py	(refactored)
@@ -121,7 +121,7 @@
 
     expectedCats = ([0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0],)
     dataSource.rewind()
-    for i in xrange(8):
+    for i in range(8):
       net.run(1)
       inferredCats = classifier.getOutputData("categoriesOut")
       self.assertSequenceEqual(expectedCats[i], inferredCats.tolist(),
@@ -203,7 +203,7 @@
 
     expectedCats = ([0.0], [1.0], [0.0], [1.0], [0.0], [1.0], [0.0], [1.0],)
     dataSource.rewind()
-    for i in xrange(8):
+    for i in range(8):
       net.run(1)
       inferredCats = classifier.getOutputData("categoriesOut")
       self.assertSequenceEqual(expectedCats[i], inferredCats.tolist(),
@@ -231,8 +231,8 @@
           "verbosity": 0,
           "encoders": {
             "token": {
-              "fieldname": u"token",
-              "name": u"token",
+              "fieldname": "token",
+              "name": "token",
               "type": "CategoryEncoder",
               "categoryList": categories,
               "w": colsPerChar,
@@ -293,14 +293,14 @@
 
     # train
     prediction = None
-    for rpt in xrange(20):
+    for rpt in range(20):
       for token in text:
         if prediction is not None:
           if rpt > 15:
             self.assertEqual(prediction, token)
         modelInput = {"token": token}
         result = model.run(modelInput)
-        prediction = sorted(result.inferences["multiStepPredictions"][1].items(),
+        prediction = sorted(list(result.inferences["multiStepPredictions"][1].items()),
                        key=itemgetter(1), reverse=True)[0][0]
       model.resetSequenceStates()
       prediction = None
@@ -382,7 +382,7 @@
 
     expectedValues = ([0.5], [1.5], [0.5], [1.5], [0.5], [1.5], [0.5], [1.5],)
     dataSource.rewind()
-    for i in xrange(8):
+    for i in range(8):
       net.run(1)
       predictedValue = classifier.getOutputData("categoriesOut")
       self.assertAlmostEqual(expectedValues[i], predictedValue[0],
@@ -476,7 +476,7 @@
 
     expectedValues = ([0.5], [1.5], [0.5], [1.5], [0.5], [1.5], [0.5], [1.5],)
     dataSource.rewind()
-    for i in xrange(8):
+    for i in range(8):
       net.run(1)
       predictedValue = classifier.getOutputData("categoriesOut")
       self.assertAlmostEqual(expectedValues[i], predictedValue[0],
--- d:\nupic\src\python\python27\tests\regression\run_opf_benchmarks_test.py	(original)
+++ d:\nupic\src\python\python27\tests\regression\run_opf_benchmarks_test.py	(refactored)
@@ -36,7 +36,7 @@
 
 from optparse import OptionParser
 from multiprocessing import Process, Queue
-from Queue import Empty
+from queue import Empty
 from collections import deque
 
 from nupic.database import client_jobs_dao as cjdao
@@ -147,12 +147,12 @@
     """ Function that cancels all the jobs in the
     process queue.
     """
-    print "Terminating all Jobs due to reaching timeout"
+    print("Terminating all Jobs due to reaching timeout")
     for proc in self.__procs:
       if not proc.is_alive():
 
         proc.terminate()
-    print "All jobs have been terminated"
+    print("All jobs have been terminated")
 
 
   def runJobs(self, maxJobs):
@@ -178,7 +178,7 @@
         elif jobsindx == len(self.testQ):
 
           time.sleep(30)
-          print "Waiting for all scheduled tests to finish."
+          print("Waiting for all scheduled tests to finish.")
         #Update the number of active running processes.
         jobsrunning = self.__updateProcessCounter()
         for proc in self.__procs:
@@ -300,7 +300,7 @@
     done=False
     while(not done):
       done=True
-      for jobID in self.swarmJobIDProductionJobIDMap.keys():
+      for jobID in list(self.swarmJobIDProductionJobIDMap.keys()):
         if (jobsDB.jobGetFields(self.swarmJobIDProductionJobIDMap[jobID],
             ["status",])[0] != 'completed'):
           done=False
@@ -320,7 +320,7 @@
     outputDir=self.descriptions[dataSet][0]
     streamDef = self.descriptions[dataSet][1]["streamDef"]
     #streamDef["streams"][0]["first_record"]=self.splits[dataSet]
-    streamDef["streams"][0]["last_record"]=sys.maxint
+    streamDef["streams"][0]["last_record"]=sys.maxsize
 
     cmdLine = "$PRODUCTIONMODEL"
 
@@ -347,8 +347,8 @@
   def runProductionWorkers(self):
 
     jobsDB = cjdao.ClientJobsDAO.get()
-    print "Starting Production Worker Jobs"
-    print "__expJobMap " + str(self.__expJobMap) + str(id(self.__expJobMap))
+    print("Starting Production Worker Jobs")
+    print("__expJobMap " + str(self.__expJobMap) + str(id(self.__expJobMap)))
     while not self.__expJobMap.empty():
       (dataSet, jobID) = self.__expJobMap.get()
       modelCounterPairs = jobsDB.modelsGetUpdateCounters(jobID)
@@ -391,25 +391,25 @@
 
   @classmethod
   def setMetaOptimize(cls, paramString):
-    print paramString
+    print(paramString)
     if paramString is None:
       cls.__metaOptimize = False
     else:
       cls.__metaOptimize = True
       paramsDict=json.loads(paramString)
-      if(paramsDict.has_key("inertia")):
+      if("inertia" in paramsDict):
         os.environ['NTA_CONF_PROP_nupic_hypersearch_inertia'] = \
                   str(paramsDict['inertia'])
 
-      if(paramsDict.has_key('socRate')):
+      if('socRate' in paramsDict):
         os.environ['NTA_CONF_PROP_nupic_hypersearch_socRate'] = \
                   str(paramsDict['socRate'])
 
-      if(paramsDict.has_key('cogRate')):
+      if('cogRate' in paramsDict):
         os.environ['NTA_CONF_PROP_nupic_hypersearch_cogRate'] = \
                   str(paramsDict['cogRate'])
 
-      if(paramsDict.has_key('minParticlesPerSwarm')):
+      if('minParticlesPerSwarm' in paramsDict):
         os.environ['NTA_CONF_PROP_nupic_hypersearch_minParticlesPerSwarm'] = \
                    str(paramsDict['minParticlesPerSwarm'])
 
@@ -452,7 +452,7 @@
 
 
   def removeTmpDirs(self):
-    print "Removing temporary directory <%s>" % self.outdir
+    print("Removing temporary directory <%s>" % self.outdir)
     if(self.onCluster()):
       os.system("onall rm -r %s" % self.outdir)
     else :
@@ -482,12 +482,12 @@
                         self.swarmJobIDProductionJobIDMap[restup["jobID"]],
                         ["results",])[0])['bestValue']
 
-      print ("Test: %10s      Expected: %10.4f     Swarm Error: %10.4f     "
+      print(("Test: %10s      Expected: %10.4f     Swarm Error: %10.4f     "
              "ProductionError: %10.4f   TotalModelWallTime: %8d    "
              "RecordsProcessed: %10d    Status: %10s") % \
             (key, self.benchmarkDB[key][0], restup['metric'],
              productionError, restup['totalModelWallTime'],
-             restup["totalNumRecordsProcessed"], restup['status'])
+             restup["totalNumRecordsProcessed"], restup['status']))
 
 
       if self.__metaOptimize:
@@ -499,7 +499,7 @@
           Configuration.get("nupic.hypersearch.cogRate")+", "+\
           Configuration.get("nupic.hypersearch.socRate")+", "+\
           str(productionError)+", "+str(self.__trainFraction)+"\n"
-        print lineMeta
+        print(lineMeta)
         with open("allResults.csv", "a") as results:
           results.write(lineResults+", "+lineMeta)
 
@@ -508,29 +508,29 @@
     outpath = os.path.join(self.outdir, "BenchmarkResults.csv")
     csv = open(outpath, 'w')
     optionalKeys = ['maxBranching', 'maxParticles']
-    print >> csv , (
+    print((
         "JobID, Output Directory, Benchmark, Search, Swarm Error Metric,"
         " Prod. Error Metric, encoders, TotalModelElapsedTime(s), "
-        "TotalCpuTime(s), JobWallTime, RecordsProcessed, Completion Status"),
+        "TotalCpuTime(s), JobWallTime, RecordsProcessed, Completion Status"), end=' ', file=csv)
     addstr = ""
     for key in optionalKeys:
       addstr+= ",%s" % key
-    print >> csv, addstr
+    print(addstr, file=csv)
     for result in self.__resultList:
-      print >> csv, "%d,%s,%s,%s,%f,%s,%s,%d,%f,%s,%d,%s" % (result['jobID'],
+      print("%d,%s,%s,%s,%f,%s,%s,%d,%f,%s,%d,%s" % (result['jobID'],
             result['outDir'], result['expName'], result['searchType'], \
             result["metric"], result["prodMetric"], \
             result['encoders'], \
             result["totalModelWallTime"], \
             result['totalModelCpuTime'], str(result['jobTime']), \
-            result["totalNumRecordsProcessed"], result['status']),
+            result["totalNumRecordsProcessed"], result['status']), end=' ', file=csv)
       addstr = ""
       for key in optionalKeys:
         if key in result:
           addstr+= ",%s" % str(result[key])
         else:
           addstr+= ",None"
-      print >> csv, addstr
+      print(addstr, file=csv)
 
     csv.close()
 
@@ -566,8 +566,8 @@
     for modelInfo in modelInfos:
       if modelInfo.modelId == bestModel:
         metrics = json.loads(modelInfo.results)[0]
-        bestmetric = json.loads(modelInfo.results)[1].keys()[0]
-        for key in metrics.keys():
+        bestmetric = list(json.loads(modelInfo.results)[1].keys())[0]
+        for key in list(metrics.keys()):
           if "nupicScore" in key and "moving" in key:
             ret["nupicScore"] = ret[key] = metrics[key]
           ret[key] = metrics[key]
@@ -1254,9 +1254,9 @@
     self.resultDB[expname + ',' + searchMethod] = resultdict
     self.__resultList.append(resultdict)
     if (resultdict['metric'] / benchmark[0]) > (1+benchmark[1]):
-      print "HyperSearch %s on %s benchmark did not match " \
+      print("HyperSearch %s on %s benchmark did not match " \
         "the expected value. (Expected: %f    Observed:  %f)" % \
-        (searchMethod, expname, benchmark[0], resultdict['metric'])
+        (searchMethod, expname, benchmark[0], resultdict['metric']))
       self.__failures+=1
     return
 
@@ -1374,7 +1374,7 @@
       self.saveResults()
     else:
       self.removeTmpDirs()
-    print "Done with all tests"
+    print("Done with all tests")
 
 
 
@@ -1465,11 +1465,11 @@
   options, remainingArgs = parser.parse_args()
 
   # Set up module
-  print "\nCURRENT DIRECTORY:", os.getcwd()
+  print("\nCURRENT DIRECTORY:", os.getcwd())
   if not os.path.isdir(options.outdir):
     options.outdir = tempfile.mkdtemp()
-    print "Provided directory to store Benchmark files is invalid.",
-    print "Now storing in <%s> and then deleting" % options.outdir
+    print("Provided directory to store Benchmark files is invalid.", end=' ')
+    print("Now storing in <%s> and then deleting" % options.outdir)
     OPFBenchmarkRunner.isSaveResults = False
   OPFBenchmarkRunner.outdir = os.path.abspath(os.path.join(options.outdir,
                                                            "BenchmarkFiles"))
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\swarming_test.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\swarming_test.py	(refactored)
@@ -28,7 +28,7 @@
 import pprint
 import shutil
 import copy
-import StringIO
+import io
 import logging
 import itertools
 import numpy
@@ -120,8 +120,8 @@
     """ Print out what test we are running
     """
 
-    print "###############################################################"
-    print "Running test: %s.%s..." % (self.__class__, self._testMethodName)
+    print("###############################################################")
+    print("Running test: %s.%s..." % (self.__class__, self._testMethodName))
 
 
   def _setDataPath(self, env):
@@ -327,10 +327,10 @@
     """
 
 
-    print
-    print "=================================================================="
-    print "Running Hypersearch job using 1 worker in current process"
-    print "=================================================================="
+    print()
+    print("==================================================================")
+    print("Running Hypersearch job using 1 worker in current process")
+    print("==================================================================")
 
     # Plug in modified environment variables
     if env is not None:
@@ -380,7 +380,7 @@
     metricResults = []
     for result in results:
       if result.results is not None:
-        metricResults.append(json.loads(result.results)[1].values()[0])
+        metricResults.append(list(json.loads(result.results)[1].values())[0])
       else:
         metricResults.append(None)
       if not ignoreErrModels:
@@ -417,16 +417,16 @@
     retval:          (jobID, jobInfo, resultsInfoForAllModels, metricResults)
     """
 
-    print
-    print "=================================================================="
-    print "Running Hypersearch job on cluster"
-    print "=================================================================="
+    print()
+    print("==================================================================")
+    print("Running Hypersearch job on cluster")
+    print("==================================================================")
 
     # --------------------------------------------------------------------
     # Submit the job
     if env is not None and len(env) > 0:
       envItems = []
-      for (key, value) in env.iteritems():
+      for (key, value) in env.items():
         if (sys.platform.startswith('win')):
           envItems.append("set \"%s=%s\"" % (key, value))
         else:
@@ -454,9 +454,9 @@
                           % (envStr, jobID, loggingLevel)
     workers = self._launchWorkers(cmdLine=workerCmdLine, numWorkers=maxNumWorkers)
 
-    print "Successfully submitted new test job, jobID=%d" % (jobID)
-    print "Each of %d workers executing the command line: " % (maxNumWorkers), \
-            cmdLine
+    print("Successfully submitted new test job, jobID=%d" % (jobID))
+    print("Each of %d workers executing the command line: " % (maxNumWorkers), \
+            cmdLine)
 
     if not waitForCompletion:
       return (jobID, None, None)
@@ -479,9 +479,9 @@
     lastActiveSwarms = None
     lastEngStatus = None
     modelIDs = []
-    print "\n%-15s    %-15s %-15s %-15s %-15s" % ("jobStatus", "modelsStarted",
-                                "modelsCompleted", "modelErrs", "modelOrphans")
-    print "-------------------------------------------------------------------"
+    print("\n%-15s    %-15s %-15s %-15s %-15s" % ("jobStatus", "modelsStarted",
+                                "modelsCompleted", "modelErrs", "modelOrphans"))
+    print("-------------------------------------------------------------------")
     while (lastJobStatus != ClientJobsDAO.STATUS_COMPLETED) \
           and (time.time() - lastUpdate < timeout):
 
@@ -495,8 +495,8 @@
       if jobInfo.status != lastJobStatus:
         if jobInfo.status == ClientJobsDAO.STATUS_RUNNING \
             and lastJobStatus != ClientJobsDAO.STATUS_RUNNING:
-          print "# Swarm job now running. jobID=%s" \
-                % (jobInfo.jobId)
+          print("# Swarm job now running. jobID=%s" \
+                % (jobInfo.jobId))
 
         lastJobStatus = jobInfo.status
         printUpdate = True
@@ -506,18 +506,18 @@
           activeSwarms = json.loads(jobInfo.engWorkerState)['activeSwarms']
           if activeSwarms != lastActiveSwarms:
             #print "-------------------------------------------------------"
-            print ">> Active swarms:\n   ", '\n    '.join(activeSwarms)
+            print(">> Active swarms:\n   ", '\n    '.join(activeSwarms))
             lastActiveSwarms = activeSwarms
-            print
+            print()
 
         if jobInfo.results != lastJobResults:
           #print "-------------------------------------------------------"
-          print ">> New best:", jobInfo.results, "###"
+          print(">> New best:", jobInfo.results, "###")
           lastJobResults = jobInfo.results
 
         if jobInfo.engStatus != lastEngStatus:
-          print '>> Status: "%s"' % jobInfo.engStatus
-          print
+          print('>> Status: "%s"' % jobInfo.engStatus)
+          print()
           lastEngStatus = jobInfo.engStatus
 
 
@@ -558,19 +558,19 @@
       if printUpdate:
         lastUpdate = time.time()
         if g_myEnv.options.verbosity >= 1:
-          print ">>",
-        print "%-15s %-15d %-15d %-15d %-15d" % (lastJobStatus, lastStarted,
+          print(">>", end=' ')
+        print("%-15s %-15d %-15d %-15d %-15d" % (lastJobStatus, lastStarted,
                 lastCompleted,
                 lastCompletedWithError,
-                lastCompletedAsOrphan)
+                lastCompletedAsOrphan))
 
 
     # ========================================================================
     # Final total
-    print "\n<< %-15s %-15d %-15d %-15d %-15d" % (lastJobStatus, lastStarted,
+    print("\n<< %-15s %-15d %-15d %-15d %-15d" % (lastJobStatus, lastStarted,
                 lastCompleted,
                 lastCompletedWithError,
-                lastCompletedAsOrphan)
+                lastCompletedAsOrphan))
 
     # Success?
     jobInfo = self._getJobInfo(cjDAO, workers, jobID)
@@ -590,7 +590,7 @@
     metricResults = []
     for result in results:
       if result.results is not None:
-        metricResults.append(json.loads(result.results)[1].values()[0])
+        metricResults.append(list(json.loads(result.results)[1].values())[0])
       else:
         metricResults.append(None)
       if not ignoreErrModels:
@@ -678,14 +678,14 @@
       return (jobID, jobInfo, resultInfos, metricResults, None)
 
     # Print job status
-    print "\n------------------------------------------------------------------"
-    print "Hadoop completion reason: %s" % (jobInfo.completionReason)
-    print "Worker completion reason: %s" % (jobInfo.workerCompletionReason)
-    print "Worker completion msg: %s" % (jobInfo.workerCompletionMsg)
+    print("\n------------------------------------------------------------------")
+    print("Hadoop completion reason: %s" % (jobInfo.completionReason))
+    print("Worker completion reason: %s" % (jobInfo.workerCompletionReason))
+    print("Worker completion msg: %s" % (jobInfo.workerCompletionMsg))
 
     if jobInfo.engWorkerState is not None:
-      print "\nEngine worker state:"
-      print "---------------------------------------------------------------"
+      print("\nEngine worker state:")
+      print("---------------------------------------------------------------")
       pprint.pprint(json.loads(jobInfo.engWorkerState))
 
 
@@ -706,12 +706,12 @@
       # Get model info
       cjDAO = ClientJobsDAO.get()
       modelParams = cjDAO.modelsGetParams([minModelID])[0].params
-      print "Model params for best model: \n%s" \
-                              % (pprint.pformat(json.loads(modelParams)))
-      print "Best model result: %f" % (minErrScore)
+      print("Model params for best model: \n%s" \
+                              % (pprint.pformat(json.loads(modelParams))))
+      print("Best model result: %f" % (minErrScore))
 
     else:
-      print "No models finished"
+      print("No models finished")
 
 
     return (jobID, jobInfo, resultInfos, metricResults, minErrScore)
@@ -1321,8 +1321,8 @@
     params = json.loads(bestModel.params)
 
     actualFieldContributions = jobResults['fieldContributions']
-    print "Actual field contributions:", \
-                              pprint.pformat(actualFieldContributions)
+    print("Actual field contributions:", \
+                              pprint.pformat(actualFieldContributions))
     expectedFieldContributions = {
                       'address': 100 * (90.0-30)/90.0,
                       'gym': 100 * (90.0-40)/90.0,
@@ -1330,7 +1330,7 @@
                       'timestamp_timeOfDay': 100 * (90.0-90.0)/90.0,
                       }
 
-    for key, value in expectedFieldContributions.items():
+    for key, value in list(expectedFieldContributions.items()):
       self.assertEqual(actualFieldContributions[key], value,
                        "actual field contribution from field '%s' does not "
                        "match the expected value of %f" % (key, value))
@@ -1475,8 +1475,8 @@
 
       # Check the field contributions
       actualFieldContributions = jobResults['fieldContributions']
-      print "Actual field contributions:", \
-                                pprint.pformat(actualFieldContributions)
+      print("Actual field contributions:", \
+                                pprint.pformat(actualFieldContributions))
 
       expectedFieldContributions = {
                         'consumption': 0.0,
@@ -1486,7 +1486,7 @@
                         'gym': 100 * (60.0-30.0)/60.0}
 
 
-      for key, value in expectedFieldContributions.items():
+      for key, value in list(expectedFieldContributions.items()):
         self.assertEqual(actualFieldContributions[key], value,
                          "actual field contribution from field '%s' does not "
                          "match the expected value of %f" % (key, value))
@@ -1533,8 +1533,8 @@
 
       # Check field contributions returned
       actualFieldContributions = jobResults['fieldContributions']
-      print "Actual field contributions:", \
-                                pprint.pformat(actualFieldContributions)
+      print("Actual field contributions:", \
+                                pprint.pformat(actualFieldContributions))
 
       expectedFieldContributions = {
                         'consumption': 0.0,
@@ -1543,7 +1543,7 @@
                         'timestamp_dayOfWeek': 100 * (60.0-10.0)/60.0,
                         'gym': 100 * (60.0-30.0)/60.0}
 
-      for key, value in expectedFieldContributions.items():
+      for key, value in list(expectedFieldContributions.items()):
         self.assertEqual(actualFieldContributions[key], value,
                          "actual field contribution from field '%s' does not "
                          "match the expected value of %f" % (key, value))
@@ -1632,8 +1632,8 @@
 
       # Check field contributions returned
       actualFieldContributions = jobResults['fieldContributions']
-      print "Actual field contributions:", \
-                                pprint.pformat(actualFieldContributions)
+      print("Actual field contributions:", \
+                                pprint.pformat(actualFieldContributions))
 
       expectedFieldContributions = {
                         'consumption': 0.0,
@@ -1999,7 +1999,7 @@
     jobResults = json.loads(jobResultsStr)
 
     actualFieldContributions = jobResults['fieldContributions']
-    print "Actual field contributions:", actualFieldContributions
+    print("Actual field contributions:", actualFieldContributions)
 
     expectedFieldContributions = {'consumption': 0.0,
                                   'address': 0.0,
@@ -2008,7 +2008,7 @@
                                   'gym': 10.0}
 
 
-    for key, value in expectedFieldContributions.items():
+    for key, value in list(expectedFieldContributions.items()):
       self.assertEqual(actualFieldContributions[key], value,
                        "actual field contribution from field '%s' does not "
                        "match the expected value of %f" % (key, value))
@@ -2331,9 +2331,9 @@
     cjDB = ClientJobsDAO.get()
 
     modelIDs, records, completionReasons, matured = \
-                    zip(*self.getModelFields( jobID, ['numRecords',
+                    list(zip(*self.getModelFields( jobID, ['numRecords',
                                                            'completionReason',
-                                                            'engMatured']))
+                                                            'engMatured'])))
 
     results = cjDB.jobGetFields(jobID, ['results'])[0]
     results = json.loads(results)
@@ -2393,10 +2393,10 @@
     modelOrders = [json.loads(e[1][0])['structuredParams']['__model_num'] for e in modelParams]
     modelFields = []
 
-    for f in xrange(len(fields)):
+    for f in range(len(fields)):
       modelFields.append([e[1][f+1] for e in modelParams])
 
-    modelInfo = zip(modelOrders, modelIDs, *tuple(modelFields))
+    modelInfo = list(zip(modelOrders, modelIDs, *tuple(modelFields)))
     modelInfo.sort(key=lambda info:info[0])
 
     return [e[1:] for e in sorted(modelInfo, key=lambda info:info[0])]
@@ -2444,7 +2444,7 @@
         'nupic.hypersearch.swarmMaturityWindow'))
 
     prefix = 'modelParams|sensorParams|encoders|'
-    for swarm, (generation, scores) in terminatedSwarms.iteritems():
+    for swarm, (generation, scores) in terminatedSwarms.items():
       if prefix + 'gym' in swarm.split('.'):
         self.assertEqual(generation, swarmMaturityWindow-1)
       else:
@@ -2476,7 +2476,7 @@
         'nupic.hypersearch.swarmMaturityWindow'))
 
     prefix = 'modelParams|sensorParams|encoders|'
-    for swarm, (generation, scores) in terminatedSwarms.iteritems():
+    for swarm, (generation, scores) in terminatedSwarms.items():
       encoders = swarm.split('.')
       if prefix + 'gym' in encoders:
         self.assertEqual(generation, swarmMaturityWindow-1 + 3)
@@ -2508,7 +2508,7 @@
 
   cjDAO = ClientJobsDAO.get()
   jobResults = cjDAO.jobGetFields(jobID, ['results'])[0]
-  print "Hypersearch job results: %r" % (jobResults,)
+  print("Hypersearch job results: %r" % (jobResults,))
   jobResults = json.loads(jobResults)
   return jobResults['bestModel']
 
@@ -2556,7 +2556,7 @@
 def _debugOut(text):
   global g_debug
   if g_debug:
-    print text
+    print(text)
     sys.stdout.flush()
 
   return
@@ -2650,7 +2650,7 @@
 
 
 def setUpModule():
-  print "\nCURRENT DIRECTORY:", os.getcwd()
+  print("\nCURRENT DIRECTORY:", os.getcwd())
 
   initLogging(verbose=True)
 
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\delta\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\delta\description.py	(refactored)
@@ -128,14 +128,14 @@
             'encoders': {
               'value': {
                   'clipInput': True,
-                  'fieldname': u'value',
+                  'fieldname': 'value',
                   'n': 100,
-                  'name': u'value',
+                  'name': 'value',
                   'type': 'ScalarSpaceEncoder',
                   'w': 21},
               '_classifierInput':     {
-                'name': u'_classifierInput',
-                'fieldname': u'value',
+                'name': '_classifierInput',
+                'fieldname': 'value',
                 'classifierOnly': True,
                 'type': 'ScalarSpaceEncoder',
                 'n': 100,
@@ -341,11 +341,11 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'sawtooth test',
-        u'streams': [   {   u'columns': [u'value'],
-                            u'info': u'sawtooth',
-                            u'source': u'file://extra/sawtooth/sawtooth.csv'}],
-        u'version': 1},
+  'dataset' : {   'info': 'sawtooth test',
+        'streams': [   {   'columns': ['value'],
+                            'info': 'sawtooth',
+                            'source': 'file://extra/sawtooth/sawtooth.csv'}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -358,13 +358,13 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'value', u'predictionSteps': [1, 5]},
+  "inferenceArgs":{'predictedField': 'value', 'predictionSteps': [1, 5]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'value', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 10, 'steps': 1, 'errorMetric': 'aae'}),
-    MetricSpec(field=u'value', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 10, 'steps': 5, 'errorMetric': 'aae'}),
+    MetricSpec(field='value', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 10, 'steps': 1, 'errorMetric': 'aae'}),
+    MetricSpec(field='value', metric='multiStep', inferenceElement='multiStepBestPredictions', params={'window': 10, 'steps': 5, 'errorMetric': 'aae'}),
   ],
 
   # Logged Metrics: A sequence of regular expressions that specify which of
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\dummyV2\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\dummyV2\description.py	(refactored)
@@ -95,10 +95,10 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-        'fields': [   (u'timestamp', 'first'),
-                      (u'gym', 'first'),
-                      (u'consumption', 'mean'),
-                      (u'address', 'first')],
+        'fields': [   ('timestamp', 'first'),
+                      ('gym', 'first'),
+                      ('consumption', 'mean'),
+                      ('address', 'first')],
         'hours': 0,
         'microseconds': 0,
         'milliseconds': 0,
@@ -129,30 +129,30 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                'address': {   'fieldname': u'address',
+                'address': {   'fieldname': 'address',
                                'n': 300,
-                               'name': u'address',
+                               'name': 'address',
                                'type': 'SDRCategoryEncoder',
                                'w': 21},
                 'consumption': {   'clipInput': True,
-                                   'fieldname': u'consumption',
+                                   'fieldname': 'consumption',
                                    'maxval': 200,
                                    'minval': 0,
                                    'n': 1500,
-                                   'name': u'consumption',
+                                   'name': 'consumption',
                                    'type': 'ScalarEncoder',
                                    'w': 21},
-                'gym': {   'fieldname': u'gym',
+                'gym': {   'fieldname': 'gym',
                            'n': 600,
-                           'name': u'gym',
+                           'name': 'gym',
                            'type': 'SDRCategoryEncoder',
                            'w': 21},
                 'timestamp_dayOfWeek': {   'dayOfWeek': (7, 3),
-                                           'fieldname': u'timestamp',
-                                           'name': u'timestamp_dayOfWeek',
+                                           'fieldname': 'timestamp',
+                                           'name': 'timestamp_dayOfWeek',
                                            'type': 'DateEncoder'},
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (7, 8),
                                            'type': 'DateEncoder'}},
 
@@ -358,11 +358,11 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' :   {u'info': u'test_NoProviders',
-      u'streams': [   {   u'columns': [u'*'],
-                          u'info': "test data",
-                          u'source': "file://swarming/test_data.csv"}],
-      u'version': 1},
+  'dataset' :   {'info': 'test_NoProviders',
+      'streams': [   {   'columns': ['*'],
+                          'info': "test data",
+                          'source': "file://swarming/test_data.csv"}],
+      'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -376,7 +376,7 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption',inferenceElement=InferenceElement.prediction,
+    MetricSpec(field='consumption',inferenceElement=InferenceElement.prediction,
                metric='rmse'),
   ],
 
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\dummy_multi_v2\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\dummy_multi_v2\description.py	(refactored)
@@ -95,10 +95,10 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-        'fields': [   (u'timestamp', 'first'),
-                      (u'gym', 'first'),
-                      (u'consumption', 'mean'),
-                      (u'address', 'first')],
+        'fields': [   ('timestamp', 'first'),
+                      ('gym', 'first'),
+                      ('consumption', 'mean'),
+                      ('address', 'first')],
         'hours': 0,
         'microseconds': 0,
         'milliseconds': 0,
@@ -128,30 +128,30 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   'address': {   'fieldname': u'address',
+            'encoders': {   'address': {   'fieldname': 'address',
                                'n': 300,
-                               'name': u'address',
+                               'name': 'address',
                                'type': 'SDRCategoryEncoder',
                                'w': 21},
                 'consumption': {   'clipInput': True,
-                                   'fieldname': u'consumption',
+                                   'fieldname': 'consumption',
                                    'maxval': 200,
                                    'minval': 0,
                                    'n': 1500,
-                                   'name': u'consumption',
+                                   'name': 'consumption',
                                    'type': 'ScalarEncoder',
                                    'w': 21},
-                'gym': {   'fieldname': u'gym',
+                'gym': {   'fieldname': 'gym',
                            'n': 300,
-                           'name': u'gym',
+                           'name': 'gym',
                            'type': 'SDRCategoryEncoder',
                            'w': 21},
                 'timestamp_dayOfWeek': {   'dayOfWeek': (7, 3),
-                                           'fieldname': u'timestamp',
-                                           'name': u'timestamp_dayOfWeek',
+                                           'fieldname': 'timestamp',
+                                           'name': 'timestamp_dayOfWeek',
                                            'type': 'DateEncoder'},
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (7, 8),
                                            'type': 'DateEncoder'}},
 
@@ -357,11 +357,11 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' :   {u'info': u'test_NoProviders',
-      u'streams': [   {   u'columns': [u'*'],
-                          u'info': u'test data',
-                          u'source': u'file://swarming/test_data.csv'}],
-      u'version': 1},
+  'dataset' :   {'info': 'test_NoProviders',
+      'streams': [   {   'columns': ['*'],
+                          'info': 'test data',
+                          'source': 'file://swarming/test_data.csv'}],
+      'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -375,7 +375,7 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption',
+    MetricSpec(field='consumption',
                inferenceElement=InferenceElement.prediction,
                metric='rmse'),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\field_contrib_temporal\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\field_contrib_temporal\description.py	(refactored)
@@ -95,10 +95,10 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-        'fields': [   (u'timestamp', 'first'),
-                      (u'gym', 'first'),
-                      (u'consumption', 'mean'),
-                      (u'address', 'first')],
+        'fields': [   ('timestamp', 'first'),
+                      ('gym', 'first'),
+                      ('consumption', 'mean'),
+                      ('address', 'first')],
         'hours': 0,
         'microseconds': 0,
         'milliseconds': 0,
@@ -128,30 +128,30 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   'address': {   'fieldname': u'address',
+            'encoders': {   'address': {   'fieldname': 'address',
                                'n': 300,
-                               'name': u'address',
+                               'name': 'address',
                                'type': 'SDRCategoryEncoder',
                                'w': 21},
                 'consumption': {   'clipInput': True,
-                                   'fieldname': u'consumption',
+                                   'fieldname': 'consumption',
                                    'maxval': 200,
                                    'minval': 0,
                                    'n': 1500,
-                                   'name': u'consumption',
+                                   'name': 'consumption',
                                    'type': 'ScalarEncoder',
                                    'w': 21},
-                'gym': {   'fieldname': u'gym',
+                'gym': {   'fieldname': 'gym',
                            'n': 300,
-                           'name': u'gym',
+                           'name': 'gym',
                            'type': 'SDRCategoryEncoder',
                            'w': 21},
                 'timestamp_dayOfWeek': {   'dayOfWeek': (7, 3),
-                                           'fieldname': u'timestamp',
-                                           'name': u'timestamp_dayOfWeek',
+                                           'fieldname': 'timestamp',
+                                           'name': 'timestamp_dayOfWeek',
                                            'type': 'DateEncoder'},
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (7, 8),
                                            'type': 'DateEncoder'}},
 
@@ -357,11 +357,11 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'test_NoProviders',
-      u'streams': [   {   u'columns': [u'*'],
-                          u'info': u'test data',
-                          u'source': u'file://swarming/test_data.csv'}],
-      u'version': 1},
+  'dataset' : {   'info': 'test_NoProviders',
+      'streams': [   {   'columns': ['*'],
+                          'info': 'test data',
+                          'source': 'file://swarming/test_data.csv'}],
+      'version': 1},
 
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
@@ -376,7 +376,7 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption',
+    MetricSpec(field='consumption',
                inferenceElement=InferenceElement.prediction,
                metric='rmse'),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\field_threshold_temporal\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\field_threshold_temporal\description.py	(refactored)
@@ -125,46 +125,46 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   u'attendance': {   'clipInput': True,
-                                   'fieldname': u'attendance',
+            'encoders': {   'attendance': {   'clipInput': True,
+                                   'fieldname': 'attendance',
                                    'maxval': 36067,
                                    'minval': 0,
                                    'n': 150,
-                                   'name': u'attendance',
+                                   'name': 'attendance',
                                    'type': 'AdaptiveScalarEncoder',
                                    'w': 21},
-                u'daynight': {   'fieldname': u'daynight',
+                'daynight': {   'fieldname': 'daynight',
                                  'n': 300,
-                                 'name': u'daynight',
+                                 'name': 'daynight',
                                  'type': 'SDRCategoryEncoder',
                                  'w': 21},
-                u'home_winloss': {   'clipInput': True,
-                                     'fieldname': u'home_winloss',
+                'home_winloss': {   'clipInput': True,
+                                     'fieldname': 'home_winloss',
                                      'maxval': 0.69999999999999996,
                                      'minval': 0.0,
                                      'n': 150,
-                                     'name': u'home_winloss',
+                                     'name': 'home_winloss',
                                      'type': 'AdaptiveScalarEncoder',
                                      'w': 21},
-                u'precip': {   'fieldname': u'precip',
+                'precip': {   'fieldname': 'precip',
                                'n': 300,
-                               'name': u'precip',
+                               'name': 'precip',
                                'type': 'SDRCategoryEncoder',
                                'w': 21},
-                u'timestamp_dayOfWeek': {   'dayOfWeek': (7, 1),
-                                            'fieldname': u'timestamp',
-                                            'name': u'timestamp_dayOfWeek',
+                'timestamp_dayOfWeek': {   'dayOfWeek': (7, 1),
+                                            'fieldname': 'timestamp',
+                                            'name': 'timestamp_dayOfWeek',
                                             'type': 'DateEncoder'},
-                u'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                            'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                            'name': 'timestamp_timeOfDay',
                                             'timeOfDay': (7, 1),
                                             'type': 'DateEncoder'},
-                u'visitor_winloss': {   'clipInput': True,
-                                        'fieldname': u'visitor_winloss',
+                'visitor_winloss': {   'clipInput': True,
+                                        'fieldname': 'visitor_winloss',
                                         'maxval': 0.78600000000000003,
                                         'minval': 0.0,
                                         'n': 150,
-                                        'name': u'visitor_winloss',
+                                        'name': 'visitor_winloss',
                                         'type': 'AdaptiveScalarEncoder',
                                         'w': 21}},
 
@@ -370,16 +370,16 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'baseball benchmark test',
-      u'streams': [   {   u'columns': [   u'daynight',
-                                          u'precip',
-                                          u'home_winloss',
-                                          u'visitor_winloss',
-                                          u'attendance',
-                                          u'timestamp'],
-                          u'info': u'OAK01.csv',
-                          u'source': u'file://extra/baseball_stadium/OAK01reformatted.csv'}],
-      u'version': 1},
+  'dataset' : {   'info': 'baseball benchmark test',
+      'streams': [   {   'columns': [   'daynight',
+                                          'precip',
+                                          'home_winloss',
+                                          'visitor_winloss',
+                                          'attendance',
+                                          'timestamp'],
+                          'info': 'OAK01.csv',
+                          'source': 'file://extra/baseball_stadium/OAK01reformatted.csv'}],
+      'version': 1},
 
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
@@ -394,16 +394,16 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'attendance',
+    MetricSpec(field='attendance',
                inferenceElement=InferenceElement.prediction,
                metric='aae', params={'window': 1000}),
-            MetricSpec(field=u'attendance',
+            MetricSpec(field='attendance',
                        inferenceElement=InferenceElement.prediction,
                        metric='trivial_aae', params={'window': 1000}),
-            MetricSpec(field=u'attendance',
+            MetricSpec(field='attendance',
                        inferenceElement=InferenceElement.encodings,
                        metric='nupicScore_scalar', params={'frequencyWindow': 1000, 'movingAverageWindow': 1000}),
-            MetricSpec(field=u'attendance',
+            MetricSpec(field='attendance',
                        inferenceElement=InferenceElement.encodings,
                        metric='nupicScore_scalar',
                        params={'frequencyWindow': 1000})
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\input_predicted_field\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\input_predicted_field\description.py	(refactored)
@@ -65,8 +65,8 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-    'fields': [   (u'timestamp', 'first'),
-                  (u'consumption', 'sum'),
+    'fields': [   ('timestamp', 'first'),
+                  ('consumption', 'sum'),
               ],
     'hours': 0,
     'microseconds': 0,
@@ -98,36 +98,36 @@
             'encoders': {
               'consumption':     {
                 'clipInput': True,
-                'fieldname': u'consumption',
+                'fieldname': 'consumption',
                 'n': 100,
-                'name': u'consumption',
+                'name': 'consumption',
                 'type': 'AdaptiveScalarEncoder',
                 'w': 21},
               'address': {
-                'fieldname': u'address',
+                'fieldname': 'address',
                 'n': 300,
-                'name': u'address',
+                'name': 'address',
                 'type': 'SDRCategoryEncoder',
                 'w': 21},
               'gym':     {
-                'fieldname': u'gym',
+                'fieldname': 'gym',
                 'n': 100,
-                'name': u'gym',
+                'name': 'gym',
                 'type': 'SDRCategoryEncoder',
                 'w': 21},
               'timestamp_dayOfWeek': {
                 'dayOfWeek': (7, 3),
-                'fieldname': u'timestamp',
-                'name': u'timestamp_dayOfWeek',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_dayOfWeek',
                 'type': 'DateEncoder'},
               'timestamp_timeOfDay': {
-                'fieldname': u'timestamp',
-                'name': u'timestamp_timeOfDay',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_timeOfDay',
                 'timeOfDay': (7, 8),
                 'type': 'DateEncoder'},
               '_classifierInput':     {
-                'name': u'_classifierInput',
-                'fieldname': u'consumption',
+                'name': '_classifierInput',
+                'fieldname': 'consumption',
                 'classifierOnly': True,
                 'type': 'AdaptiveScalarEncoder',
                 'clipInput': True,
@@ -146,7 +146,7 @@
             # Example for 1.5 days: sensorAutoReset = dict(days=1,hours=12),
             #
             # (value generated from SENSOR_AUTO_RESET)
-            'sensorAutoReset' : {   u'days': 0, u'hours': 0},
+            'sensorAutoReset' : {   'days': 0, 'hours': 0},
         },
 
         'spEnable': True,
@@ -300,9 +300,9 @@
             'steps': '1',
         },
 
-        'anomalyParams': {   u'anomalyCacheRecords': None,
-    u'autoDetectThreshold': None,
-    u'autoDetectWaitRecords': None},
+        'anomalyParams': {   'anomalyCacheRecords': None,
+    'autoDetectThreshold': None,
+    'autoDetectWaitRecords': None},
 
         'trainSPNetOnlyIfRequested': False,
     },
@@ -338,11 +338,11 @@
   # Input stream specification per py/nupic/frameworks/opf/jsonschema/stream_def.json.
   #
   'dataset' : {
-        u'info': u'test_hotgym',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'test data',
-                            u'source': u'file://swarming/test_data.csv'}],
-        u'version': 1},
+        'info': 'test_hotgym',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'test data',
+                            'source': 'file://swarming/test_data.csv'}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -355,12 +355,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'consumption', u'predictionSteps': [1]},
+  "inferenceArgs":{'predictedField': 'consumption', 'predictionSteps': [1]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption', metric='multiStep',
+    MetricSpec(field='consumption', metric='multiStep',
                inferenceElement='multiStepBestPredictions',
                params={'window': 1000, 'steps': [1], 'errorMetric': 'altMAPE'}),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\legacy_cla_multistep\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\legacy_cla_multistep\description.py	(refactored)
@@ -65,8 +65,8 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-    'fields': [   (u'timestamp', 'first'),
-                  (u'consumption', 'sum'),
+    'fields': [   ('timestamp', 'first'),
+                  ('consumption', 'sum'),
               ],
     'hours': 0,
     'microseconds': 0,
@@ -98,19 +98,19 @@
             'encoders': {
               'consumption':     {
                 'clipInput': True,
-                'fieldname': u'consumption',
+                'fieldname': 'consumption',
                 'n': 100,
-                'name': u'consumption',
+                'name': 'consumption',
                 'type': 'AdaptiveScalarEncoder',
                 'w': 21},
               'timestamp_timeOfDay':     {
-                'fieldname': u'timestamp',
-                'name': u'timestamp_timeOfDay',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_timeOfDay',
                 'type': 'DateEncoder',
                 'timeOfDay': (21, 1)},
               'timestamp_dayOfWeek':     {
-                'fieldname': u'timestamp',
-                'name': u'timestamp_dayOfWeek',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_dayOfWeek',
                 'type': 'DateEncoder',
                 'dayOfWeek': (21, 1)},
             },
@@ -126,7 +126,7 @@
             # Example for 1.5 days: sensorAutoReset = dict(days=1,hours=12),
             #
             # (value generated from SENSOR_AUTO_RESET)
-            'sensorAutoReset' : {   u'days': 0, u'hours': 0},
+            'sensorAutoReset' : {   'days': 0, 'hours': 0},
         },
 
         'spEnable': True,
@@ -280,9 +280,9 @@
             'steps': '1',
         },
 
-        'anomalyParams': {   u'anomalyCacheRecords': None,
-    u'autoDetectThreshold': None,
-    u'autoDetectWaitRecords': None},
+        'anomalyParams': {   'anomalyCacheRecords': None,
+    'autoDetectThreshold': None,
+    'autoDetectWaitRecords': None},
 
         'trainSPNetOnlyIfRequested': False,
     },
@@ -318,11 +318,11 @@
   # Input stream specification per py/nupic/frameworks/opf/jsonschema/stream_def.json.
   #
   'dataset' : {
-        u'info': u'test_hotgym',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'test data',
-                            u'source': u'file://swarming/test_data.csv'}],
-        u'version': 1},
+        'info': 'test_hotgym',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'test data',
+                            'source': 'file://swarming/test_data.csv'}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -335,12 +335,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'consumption', u'predictionSteps': [1]},
+  "inferenceArgs":{'predictedField': 'consumption', 'predictionSteps': [1]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption', metric='multiStep',
+    MetricSpec(field='consumption', metric='multiStep',
                inferenceElement='multiStepBestPredictions',
                params={'window': 1000, 'steps': [1], 'errorMetric': 'altMAPE'}),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\legacy_cla_multistep\permutations.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\legacy_cla_multistep\permutations.py	(refactored)
@@ -41,9 +41,9 @@
 
 permutations = {
   'aggregationInfo': {   'days': 0,
-    'fields': [   (u'timestamp', 'first'),
-                  (u'gym', 'first'),
-                  (u'consumption', 'sum')],
+    'fields': [   ('timestamp', 'first'),
+                  ('gym', 'first'),
+                  ('consumption', 'sum')],
     'hours': 1,
     'microseconds': 0,
     'milliseconds': 0,
@@ -58,22 +58,22 @@
 
     'sensorParams': {
       'encoders': {
-        u'timestamp_timeOfDay': PermuteEncoder(
+        'timestamp_timeOfDay': PermuteEncoder(
                               fieldName='timestamp',
                               encoderClass='DateEncoder.timeOfDay',
                               w=21,
                               radius=PermuteFloat(0.5, 12)),
-        u'timestamp_dayOfWeek': PermuteEncoder(
+        'timestamp_dayOfWeek': PermuteEncoder(
                               fieldName='timestamp',
                               encoderClass='DateEncoder.dayOfWeek',
                               w=21,
                               radius=PermuteFloat(1, 6)),
-        u'timestamp_weekend': PermuteEncoder(
+        'timestamp_weekend': PermuteEncoder(
                               fieldName='timestamp',
                               encoderClass='DateEncoder.weekend',
                               w=21,
                               radius=PermuteChoices([1])),
-        u'consumption': PermuteEncoder(
+        'consumption': PermuteEncoder(
                               fieldName='consumption',
                               encoderClass='AdaptiveScalarEncoder',
                               w=21,
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\max_branching_temporal\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\max_branching_temporal\description.py	(refactored)
@@ -125,46 +125,46 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   u'attendance': {   'clipInput': True,
-                                   'fieldname': u'attendance',
+            'encoders': {   'attendance': {   'clipInput': True,
+                                   'fieldname': 'attendance',
                                    'maxval': 36067,
                                    'minval': 0,
                                    'n': 150,
-                                   'name': u'attendance',
+                                   'name': 'attendance',
                                    'type': 'AdaptiveScalarEncoder',
                                    'w': 21},
-                u'daynight': {   'fieldname': u'daynight',
+                'daynight': {   'fieldname': 'daynight',
                                  'n': 300,
-                                 'name': u'daynight',
+                                 'name': 'daynight',
                                  'type': 'SDRCategoryEncoder',
                                  'w': 21},
-                u'home_winloss': {   'clipInput': True,
-                                     'fieldname': u'home_winloss',
+                'home_winloss': {   'clipInput': True,
+                                     'fieldname': 'home_winloss',
                                      'maxval': 0.69999999999999996,
                                      'minval': 0.0,
                                      'n': 150,
-                                     'name': u'home_winloss',
+                                     'name': 'home_winloss',
                                      'type': 'AdaptiveScalarEncoder',
                                      'w': 21},
-                u'precip': {   'fieldname': u'precip',
+                'precip': {   'fieldname': 'precip',
                                'n': 300,
-                               'name': u'precip',
+                               'name': 'precip',
                                'type': 'SDRCategoryEncoder',
                                'w': 21},
-                u'timestamp_dayOfWeek': {   'dayOfWeek': (7, 1),
-                                            'fieldname': u'timestamp',
-                                            'name': u'timestamp_dayOfWeek',
+                'timestamp_dayOfWeek': {   'dayOfWeek': (7, 1),
+                                            'fieldname': 'timestamp',
+                                            'name': 'timestamp_dayOfWeek',
                                             'type': 'DateEncoder'},
-                u'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                            'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                            'name': 'timestamp_timeOfDay',
                                             'timeOfDay': (7, 1),
                                             'type': 'DateEncoder'},
-                u'visitor_winloss': {   'clipInput': True,
-                                        'fieldname': u'visitor_winloss',
+                'visitor_winloss': {   'clipInput': True,
+                                        'fieldname': 'visitor_winloss',
                                         'maxval': 0.78600000000000003,
                                         'minval': 0.0,
                                         'n': 150,
-                                        'name': u'visitor_winloss',
+                                        'name': 'visitor_winloss',
                                         'type': 'AdaptiveScalarEncoder',
                                         'w': 21}},
 
@@ -370,16 +370,16 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'baseball benchmark test',
-      u'streams': [   {   u'columns': [   u'daynight',
-                                          u'precip',
-                                          u'home_winloss',
-                                          u'visitor_winloss',
-                                          u'attendance',
-                                          u'timestamp'],
-                          u'info': u'OAK01.csv',
-                          u'source': u'file://extra/baseball_stadium/OAK01reformatted.csv'}],
-      u'version': 1},
+  'dataset' : {   'info': 'baseball benchmark test',
+      'streams': [   {   'columns': [   'daynight',
+                                          'precip',
+                                          'home_winloss',
+                                          'visitor_winloss',
+                                          'attendance',
+                                          'timestamp'],
+                          'info': 'OAK01.csv',
+                          'source': 'file://extra/baseball_stadium/OAK01reformatted.csv'}],
+      'version': 1},
 
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
@@ -394,16 +394,16 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'attendance',
+    MetricSpec(field='attendance',
                inferenceElement=InferenceElement.prediction,
                metric='aae', params={'window': 1000}),
-            MetricSpec(field=u'attendance',
+            MetricSpec(field='attendance',
                        inferenceElement=InferenceElement.prediction,
                        metric='trivial_aae', params={'window': 1000}),
-            MetricSpec(field=u'attendance',
+            MetricSpec(field='attendance',
                        inferenceElement=InferenceElement.encodings,
                        metric='nupicScore_scalar', params={'frequencyWindow': 1000, 'movingAverageWindow': 1000}),
-            MetricSpec(field=u'attendance',
+            MetricSpec(field='attendance',
                        inferenceElement=InferenceElement.encodings,
                        metric='nupicScore_scalar',
                        params={'frequencyWindow': 1000})
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\oneField\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\oneField\description.py	(refactored)
@@ -95,10 +95,10 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-        'fields': [   (u'timestamp', 'first'),
-                      (u'gym', 'first'),
-                      (u'consumption', 'mean'),
-                      (u'address', 'first')],
+        'fields': [   ('timestamp', 'first'),
+                      ('gym', 'first'),
+                      ('consumption', 'mean'),
+                      ('address', 'first')],
         'hours': 0,
         'microseconds': 0,
         'milliseconds': 0,
@@ -130,11 +130,11 @@
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
                 'consumption': {   'clipInput': True,
-                                   'fieldname': u'consumption',
+                                   'fieldname': 'consumption',
                                    'maxval': 200,
                                    'minval': 0,
                                    'n': 1500,
-                                   'name': u'consumption',
+                                   'name': 'consumption',
                                    'type': 'ScalarEncoder',
                                    'w': 21
                 },
@@ -342,11 +342,11 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'test_NoProviders',
-      u'streams': [   {   u'columns': [u'*'],
-                          u'info': u'test data',
-                          u'source': u'file://swarming/test_data.csv'}],
-      u'version': 1},
+  'dataset' : {   'info': 'test_NoProviders',
+      'streams': [   {   'columns': ['*'],
+                          'info': 'test data',
+                          'source': 'file://swarming/test_data.csv'}],
+      'version': 1},
 
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
@@ -361,7 +361,7 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption',
+    MetricSpec(field='consumption',
                inferenceElement=InferenceElement.prediction,
                metric='rmse'),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\simpleV2\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\simpleV2\description.py	(refactored)
@@ -95,10 +95,10 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-        'fields': [   (u'timestamp', 'first'),
-                      (u'gym', 'first'),
-                      (u'consumption', 'mean'),
-                      (u'address', 'first')],
+        'fields': [   ('timestamp', 'first'),
+                      ('gym', 'first'),
+                      ('consumption', 'mean'),
+                      ('address', 'first')],
         'hours': 0,
         'microseconds': 0,
         'milliseconds': 0,
@@ -129,30 +129,30 @@
             #
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
-                'address': {   'fieldname': u'address',
+                'address': {   'fieldname': 'address',
                                'n': 300,
-                               'name': u'address',
+                               'name': 'address',
                                'type': 'SDRCategoryEncoder',
                                'w': 21},
                 'consumption': {   'clipInput': True,
-                                   'fieldname': u'consumption',
+                                   'fieldname': 'consumption',
                                    'maxval': 200,
                                    'minval': 0,
                                    'n': 1500,
-                                   'name': u'consumption',
+                                   'name': 'consumption',
                                    'type': 'ScalarEncoder',
                                    'w': 21},
-                'gym': {   'fieldname': u'gym',
+                'gym': {   'fieldname': 'gym',
                            'n': 300,
-                           'name': u'gym',
+                           'name': 'gym',
                            'type': 'SDRCategoryEncoder',
                            'w': 21},
                 'timestamp_dayOfWeek': {   'dayOfWeek': (7, 3),
-                                           'fieldname': u'timestamp',
-                                           'name': u'timestamp_dayOfWeek',
+                                           'fieldname': 'timestamp',
+                                           'name': 'timestamp_dayOfWeek',
                                            'type': 'DateEncoder'},
-                'timestamp_timeOfDay': {   'fieldname': u'timestamp',
-                                           'name': u'timestamp_timeOfDay',
+                'timestamp_timeOfDay': {   'fieldname': 'timestamp',
+                                           'name': 'timestamp_timeOfDay',
                                            'timeOfDay': (7, 8),
                                            'type': 'DateEncoder'}},
 
@@ -358,11 +358,11 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'test_NoProviders',
-      u'streams': [   {   u'columns': [u'*'],
-                          u'info': u'test data',
-                          u'source': u'file://swarming/test_data.csv'}],
-      u'version': 1},
+  'dataset' : {   'info': 'test_NoProviders',
+      'streams': [   {   'columns': ['*'],
+                          'info': 'test data',
+                          'source': 'file://swarming/test_data.csv'}],
+      'version': 1},
 
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
@@ -377,7 +377,7 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption',
+    MetricSpec(field='consumption',
                inferenceElement=InferenceElement.prediction,
                metric='rmse'),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\simple_cla_multistep\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\simple_cla_multistep\description.py	(refactored)
@@ -65,8 +65,8 @@
     # Intermediate variables used to compute fields in modelParams and also
     # referenced from the control section.
     'aggregationInfo': {   'days': 0,
-    'fields': [   (u'timestamp', 'first'),
-                  (u'consumption', 'sum'),
+    'fields': [   ('timestamp', 'first'),
+                  ('consumption', 'sum'),
               ],
     'hours': 0,
     'microseconds': 0,
@@ -98,24 +98,24 @@
             'encoders': {
               'consumption':     {
                 'clipInput': True,
-                'fieldname': u'consumption',
+                'fieldname': 'consumption',
                 'n': 100,
-                'name': u'consumption',
+                'name': 'consumption',
                 'type': 'AdaptiveScalarEncoder',
                 'w': 21},
               'timestamp_timeOfDay':     {
-                'fieldname': u'timestamp',
-                'name': u'timestamp_timeOfDay',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_timeOfDay',
                 'type': 'DateEncoder',
                 'timeOfDay': (21, 1)},
               'timestamp_dayOfWeek':     {
-                'fieldname': u'timestamp',
-                'name': u'timestamp_dayOfWeek',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_dayOfWeek',
                 'type': 'DateEncoder',
                 'dayOfWeek': (21, 1)},
               '_classifierInput':     {
-                'name': u'_classifierInput',
-                'fieldname': u'consumption',
+                'name': '_classifierInput',
+                'fieldname': 'consumption',
                 'classifierOnly': True,
                 'type': 'AdaptiveScalarEncoder',
                 'clipInput': True,
@@ -134,7 +134,7 @@
             # Example for 1.5 days: sensorAutoReset = dict(days=1,hours=12),
             #
             # (value generated from SENSOR_AUTO_RESET)
-            'sensorAutoReset' : {   u'days': 0, u'hours': 0},
+            'sensorAutoReset' : {   'days': 0, 'hours': 0},
         },
 
         'spEnable': True,
@@ -288,9 +288,9 @@
             'steps': '1',
         },
 
-        'anomalyParams': {   u'anomalyCacheRecords': None,
-    u'autoDetectThreshold': None,
-    u'autoDetectWaitRecords': None},
+        'anomalyParams': {   'anomalyCacheRecords': None,
+    'autoDetectThreshold': None,
+    'autoDetectWaitRecords': None},
 
         'trainSPNetOnlyIfRequested': False,
     },
@@ -326,11 +326,11 @@
   # Input stream specification per py/nupic/frameworks/opf/jsonschema/stream_def.json.
   #
   'dataset' : {
-        u'info': u'test_hotgym',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'test data',
-                            u'source': u'file://swarming/test_data.csv'}],
-        u'version': 1},
+        'info': 'test_hotgym',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'test data',
+                            'source': 'file://swarming/test_data.csv'}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -343,12 +343,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'consumption', u'predictionSteps': [1]},
+  "inferenceArgs":{'predictedField': 'consumption', 'predictionSteps': [1]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption', metric='multiStep',
+    MetricSpec(field='consumption', metric='multiStep',
                inferenceElement='multiStepBestPredictions',
                params={'window': 1000, 'steps': [1], 'errorMetric': 'altMAPE'}),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\simple_cla_multistep\permutations.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\simple_cla_multistep\permutations.py	(refactored)
@@ -41,9 +41,9 @@
 
 permutations = {
   'aggregationInfo': {   'days': 0,
-    'fields': [   (u'timestamp', 'first'),
-                  (u'gym', 'first'),
-                  (u'consumption', 'sum')],
+    'fields': [   ('timestamp', 'first'),
+                  ('gym', 'first'),
+                  ('consumption', 'sum')],
     'hours': 1,
     'microseconds': 0,
     'milliseconds': 0,
@@ -58,28 +58,28 @@
 
     'sensorParams': {
       'encoders': {
-        u'timestamp_timeOfDay': PermuteEncoder(
+        'timestamp_timeOfDay': PermuteEncoder(
                               fieldName='timestamp',
                               encoderClass='DateEncoder.timeOfDay',
                               w=21,
                               radius=PermuteFloat(0.5, 12)),
-        u'timestamp_dayOfWeek': PermuteEncoder(
+        'timestamp_dayOfWeek': PermuteEncoder(
                               fieldName='timestamp',
                               encoderClass='DateEncoder.dayOfWeek',
                               w=21,
                               radius=PermuteFloat(1, 6)),
-        u'timestamp_weekend': PermuteEncoder(
+        'timestamp_weekend': PermuteEncoder(
                               fieldName='timestamp',
                               encoderClass='DateEncoder.weekend',
                               w=21,
                               radius=PermuteChoices([1])),
-        u'consumption': PermuteEncoder(
+        'consumption': PermuteEncoder(
                               fieldName='consumption',
                               encoderClass='AdaptiveScalarEncoder',
                               w=21,
                               n=PermuteInt(28, 521),
                               clipInput=True),
-        u'_classifierInput': dict(
+        '_classifierInput': dict(
                               fieldname='consumption',
                               classifierOnly=True,
                               type='AdaptiveScalarEncoder',
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_spatial_classification\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_spatial_classification\description.py	(refactored)
@@ -125,52 +125,52 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   u'A': {   'fieldname': u'daynight',
+            'encoders': {   'A': {   'fieldname': 'daynight',
                           'n': 300,
-                          'name': u'daynight',
+                          'name': 'daynight',
                           'type': 'SDRCategoryEncoder',
                           'w': 21},
-                u'B': {   'fieldname': u'daynight',
+                'B': {   'fieldname': 'daynight',
                           'n': 300,
-                          'name': u'daynight',
+                          'name': 'daynight',
                           'type': 'SDRCategoryEncoder',
                           'w': 21},
-                u'C': {   'fieldname': u'precip',
+                'C': {   'fieldname': 'precip',
                           'n': 300,
-                          'name': u'precip',
+                          'name': 'precip',
                           'type': 'SDRCategoryEncoder',
                           'w': 21},
-                u'D': {   'clipInput': True,
-                          'fieldname': u'visitor_winloss',
+                'D': {   'clipInput': True,
+                          'fieldname': 'visitor_winloss',
                           'maxval': 0.78600000000000003,
                           'minval': 0.0,
                           'n': 150,
-                          'name': u'visitor_winloss',
+                          'name': 'visitor_winloss',
                           'type': 'AdaptiveScalarEncoder',
                           'w': 21},
-                u'E': {   'clipInput': True,
-                          'fieldname': u'home_winloss',
+                'E': {   'clipInput': True,
+                          'fieldname': 'home_winloss',
                           'maxval': 0.69999999999999996,
                           'minval': 0.0,
                           'n': 150,
-                          'name': u'home_winloss',
+                          'name': 'home_winloss',
                           'type': 'AdaptiveScalarEncoder',
                           'w': 21},
-                u'F': {   'dayOfWeek': (7, 1),
-                          'fieldname': u'timestamp',
-                          'name': u'timestamp_dayOfWeek',
+                'F': {   'dayOfWeek': (7, 1),
+                          'fieldname': 'timestamp',
+                          'name': 'timestamp_dayOfWeek',
                           'type': 'DateEncoder'},
-                u'G': {   'fieldname': u'timestamp',
-                          'name': u'timestamp_timeOfDay',
+                'G': {   'fieldname': 'timestamp',
+                          'name': 'timestamp_timeOfDay',
                           'timeOfDay': (7, 1),
                           'type': 'DateEncoder'},
-                u'_classifierInput': {   'clipInput': True,
-                             'fieldname': u'attendance',
+                '_classifierInput': {   'clipInput': True,
+                             'fieldname': 'attendance',
                              'classifierOnly': True,
                              'maxval': 36067,
                              'minval': 0,
                              'n': 150,
-                             'name': u'attendance',
+                             'name': 'attendance',
                              'type': 'AdaptiveScalarEncoder',
                              'w': 21}},
 
@@ -376,16 +376,16 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'baseball benchmark test',
-      u'streams': [   {   u'columns': [   u'daynight',
-                                          u'precip',
-                                          u'home_winloss',
-                                          u'visitor_winloss',
-                                          u'attendance',
-                                          u'timestamp'],
-                          u'info': u'OAK01.csv',
-                          u'source': u'file://extra/baseball_stadium/OAK01reformatted.csv'}],
-      u'version': 1},
+  'dataset' : {   'info': 'baseball benchmark test',
+      'streams': [   {   'columns': [   'daynight',
+                                          'precip',
+                                          'home_winloss',
+                                          'visitor_winloss',
+                                          'attendance',
+                                          'timestamp'],
+                          'info': 'OAK01.csv',
+                          'source': 'file://extra/baseball_stadium/OAK01reformatted.csv'}],
+      'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -399,7 +399,7 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'attendance', metric='multiStep',
+    MetricSpec(field='attendance', metric='multiStep',
                inferenceElement='multiStepBestPredictions',
                params={'window': 1000, 'steps': [0], 'errorMetric': 'aae'}),
   ],
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_spatial_classification\permutations.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_spatial_classification\permutations.py	(refactored)
@@ -96,7 +96,7 @@
   #  speculative models
   encoderCount = 0
   encoders = perm['modelParams']['sensorParams']['encoders']
-  for field,encoder in encoders.items():
+  for field,encoder in list(encoders.items()):
    	if encoder is not None:
    		encoderCount += 1
 
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_temporal\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_temporal\description.py	(refactored)
@@ -125,51 +125,51 @@
             #     ],
             #
             # (value generated from DS_ENCODER_SCHEMA)
-            'encoders': {   u'A': {   'fieldname': u'daynight',
+            'encoders': {   'A': {   'fieldname': 'daynight',
                           'n': 300,
-                          'name': u'daynight',
+                          'name': 'daynight',
                           'type': 'SDRCategoryEncoder',
                           'w': 21},
-                u'B': {   'fieldname': u'daynight',
+                'B': {   'fieldname': 'daynight',
                           'n': 300,
-                          'name': u'daynight',
+                          'name': 'daynight',
                           'type': 'SDRCategoryEncoder',
                           'w': 21},
-                u'C': {   'fieldname': u'precip',
+                'C': {   'fieldname': 'precip',
                           'n': 300,
-                          'name': u'precip',
+                          'name': 'precip',
                           'type': 'SDRCategoryEncoder',
                           'w': 21},
-                u'D': {   'clipInput': True,
-                          'fieldname': u'visitor_winloss',
+                'D': {   'clipInput': True,
+                          'fieldname': 'visitor_winloss',
                           'maxval': 0.78600000000000003,
                           'minval': 0.0,
                           'n': 150,
-                          'name': u'visitor_winloss',
+                          'name': 'visitor_winloss',
                           'type': 'AdaptiveScalarEncoder',
                           'w': 21},
-                u'E': {   'clipInput': True,
-                          'fieldname': u'home_winloss',
+                'E': {   'clipInput': True,
+                          'fieldname': 'home_winloss',
                           'maxval': 0.69999999999999996,
                           'minval': 0.0,
                           'n': 150,
-                          'name': u'home_winloss',
+                          'name': 'home_winloss',
                           'type': 'AdaptiveScalarEncoder',
                           'w': 21},
-                u'F': {   'dayOfWeek': (7, 1),
-                          'fieldname': u'timestamp',
-                          'name': u'timestamp_dayOfWeek',
+                'F': {   'dayOfWeek': (7, 1),
+                          'fieldname': 'timestamp',
+                          'name': 'timestamp_dayOfWeek',
                           'type': 'DateEncoder'},
-                u'G': {   'fieldname': u'timestamp',
-                          'name': u'timestamp_timeOfDay',
+                'G': {   'fieldname': 'timestamp',
+                          'name': 'timestamp_timeOfDay',
                           'timeOfDay': (7, 1),
                           'type': 'DateEncoder'},
-                u'pred': {   'clipInput': True,
-                             'fieldname': u'attendance',
+                'pred': {   'clipInput': True,
+                             'fieldname': 'attendance',
                              'maxval': 36067,
                              'minval': 0,
                              'n': 150,
-                             'name': u'attendance',
+                             'name': 'attendance',
                              'type': 'AdaptiveScalarEncoder',
                              'w': 21}},
 
@@ -375,16 +375,16 @@
 
   # Input stream specification per py/nupicengine/cluster/database/StreamDef.json.
   #
-  'dataset' : {   u'info': u'baseball benchmark test',
-      u'streams': [   {   u'columns': [   u'daynight',
-                                          u'precip',
-                                          u'home_winloss',
-                                          u'visitor_winloss',
-                                          u'attendance',
-                                          u'timestamp'],
-                          u'info': u'OAK01.csv',
-                          u'source': u'file://extra/baseball_stadium/OAK01reformatted.csv'}],
-      u'version': 1},
+  'dataset' : {   'info': 'baseball benchmark test',
+      'streams': [   {   'columns': [   'daynight',
+                                          'precip',
+                                          'home_winloss',
+                                          'visitor_winloss',
+                                          'attendance',
+                                          'timestamp'],
+                          'info': 'OAK01.csv',
+                          'source': 'file://extra/baseball_stadium/OAK01reformatted.csv'}],
+      'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -398,13 +398,13 @@
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'attendance', inferenceElement=InferenceElement.prediction,
+    MetricSpec(field='attendance', inferenceElement=InferenceElement.prediction,
                metric='aae', params={'window': 1000}),
-    MetricSpec(field=u'attendance', inferenceElement=InferenceElement.prediction,
+    MetricSpec(field='attendance', inferenceElement=InferenceElement.prediction,
                metric='trivial_aae', params={'window': 1000}),
-    MetricSpec(field=u'attendance', inferenceElement=InferenceElement.prediction,
+    MetricSpec(field='attendance', inferenceElement=InferenceElement.prediction,
                metric='nupicScore_scalar', params={'frequencyWindow': 1000, 'movingAverageWindow': 1000}),
-    MetricSpec(field=u'attendance', inferenceElement=InferenceElement.prediction,
+    MetricSpec(field='attendance', inferenceElement=InferenceElement.prediction,
                metric='nupicScore_scalar', params={'frequencyWindow': 1000})
   ],
 
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_temporal\permutations.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\smart_speculation_temporal\permutations.py	(refactored)
@@ -109,7 +109,7 @@
   #If the model only has the A field have it run slowly to simulate speculation.
 
   encoderCount = 0
-  for key in perm.keys():
+  for key in list(perm.keys()):
  	if 'encoder' in key and not perm[key] is None:
  		encoderCount+=1
   delay=encoderCount*encoderCount*.1
--- d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\spatial_classification\description.py	(original)
+++ d:\nupic\src\python\python27\tests\swarming\nupic\swarming\experiments\spatial_classification\description.py	(refactored)
@@ -129,15 +129,15 @@
             # (value generated from DS_ENCODER_SCHEMA)
             'encoders': {
               'address': {
-                'fieldname': u'address',
+                'fieldname': 'address',
                  'n': 300,
-                 'name': u'address',
+                 'name': 'address',
                  'type': 'SDRCategoryEncoder',
                  'w': 21
               },
               '_classifierInput': {
-                 'name': u'_classifierInput',
-                 'fieldname': u'consumption',
+                 'name': '_classifierInput',
+                 'fieldname': 'consumption',
                  'classifierOnly': True,
                  'clipInput': True,
                  'maxval': 200,
@@ -147,21 +147,21 @@
                  'w': 21
               },
               'gym': {
-                'fieldname': u'gym',
+                'fieldname': 'gym',
                 'n': 300,
-                'name': u'gym',
+                'name': 'gym',
                 'type': 'SDRCategoryEncoder',
                 'w': 21
               },
               'timestamp_dayOfWeek': {
                 'dayOfWeek': (7, 3),
-                'fieldname': u'timestamp',
-                'name': u'timestamp_dayOfWeek',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_dayOfWeek',
                 'type': 'DateEncoder'
               },
               'timestamp_timeOfDay': {
-                'fieldname': u'timestamp',
-                'name': u'timestamp_timeOfDay',
+                'fieldname': 'timestamp',
+                'name': 'timestamp_timeOfDay',
                 'timeOfDay': (7, 8),
                 'type': 'DateEncoder'
               }
@@ -334,9 +334,9 @@
             'steps': '0',
         },
 
-        'anomalyParams': {   u'anomalyCacheRecords': None,
-    u'autoDetectThreshold': None,
-    u'autoDetectWaitRecords': None},
+        'anomalyParams': {   'anomalyCacheRecords': None,
+    'autoDetectThreshold': None,
+    'autoDetectWaitRecords': None},
 
         'trainSPNetOnlyIfRequested': False,
     },
@@ -371,11 +371,11 @@
 
   # Input stream specification per py/nupic/frameworks/opf/jsonschema/stream_def.json.
   #
-  'dataset' : {   u'info': u'testSpatialClassification',
-        u'streams': [   {   u'columns': [u'*'],
-                            u'info': u'test data',
-                            u'source': u'file://swarming/test_data.csv'}],
-        u'version': 1},
+  'dataset' : {   'info': 'testSpatialClassification',
+        'streams': [   {   'columns': ['*'],
+                            'info': 'test data',
+                            'source': 'file://swarming/test_data.csv'}],
+        'version': 1},
 
   # Iteration count: maximum number of iterations.  Each iteration corresponds
   # to one record from the (possibly aggregated) dataset.  The task is
@@ -388,12 +388,12 @@
 
 
   # A dictionary containing all the supplementary parameters for inference
-  "inferenceArgs":{u'predictedField': u'consumption', u'predictionSteps': [0]},
+  "inferenceArgs":{'predictedField': 'consumption', 'predictionSteps': [0]},
 
   # Metrics: A list of MetricSpecs that instantiate the metrics that are
   # computed for this experiment
   'metrics':[
-    MetricSpec(field=u'consumption', metric='multiStep',
+    MetricSpec(field='consumption', metric='multiStep',
                inferenceElement='multiStepBestPredictions',
                params={'window': 1000, 'steps': [0], 'errorMetric': 'avg_err'})
   ],
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\anomaly_likelihood_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\anomaly_likelihood_test.py	(refactored)
@@ -44,7 +44,7 @@
 
   :returns: A numpy array of samples.
   """
-  if params.has_key("name"):
+  if "name" in params:
     if params["name"] == "normal":
       samples = numpy.random.normal(loc=params["mean"],
                                     scale=math.sqrt(params["variance"]),
@@ -60,9 +60,9 @@
     raise ValueError("Bad distribution params: " + str(params))
 
   if verbosity > 0:
-    print "\nSampling from distribution:", params
-    print "After estimation, mean=", numpy.mean(samples), \
-          "var=", numpy.var(samples), "stdev=", math.sqrt(numpy.var(samples))
+    print("\nSampling from distribution:", params)
+    print("After estimation, mean=", numpy.mean(samples), \
+          "var=", numpy.var(samples), "stdev=", math.sqrt(numpy.var(samples)))
   return samples
 
 
@@ -463,10 +463,10 @@
   def testEstimateAnomalyLikelihoodsCategoryValues(self):
     start = datetime.datetime(2017, 1, 1, 0, 0, 0)
     delta = datetime.timedelta(minutes=5)
-    dts = [start + (i * delta) for i in xrange(10)]
+    dts = [start + (i * delta) for i in range(10)]
     values = ["a", "b", "c", "d", "e"] * 2
-    rawScores = [0.1 * i for i in xrange(10)]
-    data = zip(dts, values, rawScores)
+    rawScores = [0.1 * i for i in range(10)]
+    data = list(zip(dts, values, rawScores))
 
     likelihoods, avgRecordList, estimatorParams = (
       an.estimateAnomalyLikelihoods(data)
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\backtracking_tm_constant_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\backtracking_tm_constant_test.py	(refactored)
@@ -39,7 +39,7 @@
 
 def _printOneTrainingVector(x):
   "Print a single vector succinctly."
-  print ''.join('1' if k != 0 else '.' for k in x)
+  print(''.join('1' if k != 0 else '.' for k in x))
 
 
 def _getSimplePatterns(numOnes, numPatterns):
@@ -49,7 +49,7 @@
 
   numCols = numOnes * numPatterns
   p = []
-  for i in xrange(numPatterns):
+  for i in range(numPatterns):
     x = np.zeros(numCols, dtype='float32')
     x[i*numOnes:(i + 1)*numOnes] = 1
     p.append(x)
@@ -124,10 +124,10 @@
           tm.learn(seq)
         tm.reset()
 
-    print "Learning completed"
+    print("Learning completed")
 
     # Infer
-    print "Running inference"
+    print("Running inference")
 
     tm.collectStats = True
     for seq in trainingSet[0:5]:
@@ -136,20 +136,20 @@
       for _ in range(10):
         tm.infer(seq)
         if VERBOSITY > 1 :
-          print
+          print()
           _printOneTrainingVector(seq)
           tm.printStates(False, False)
-          print
-          print
+          print()
+          print()
       if VERBOSITY > 1:
-        print tm.getStats()
+        print(tm.getStats())
 
       # Ensure our predictions are accurate for each sequence
       self.assertGreater(tm.getStats()['predictionScoreAvg2'], 0.8)
-      print ("tm.getStats()['predictionScoreAvg2'] = ",
-             tm.getStats()['predictionScoreAvg2'])
+      print(("tm.getStats()['predictionScoreAvg2'] = ",
+             tm.getStats()['predictionScoreAvg2']))
 
-    print "TMConstant basicTest ok"
+    print("TMConstant basicTest ok")
 
 
   def testCppTmBasic(self):
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\backtracking_tm_cpp2_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\backtracking_tm_cpp2_test.py	(refactored)
@@ -21,7 +21,7 @@
 
 """Tests for the C++ implementation of the temporal memory."""
 
-import cPickle as pickle
+import pickle as pickle
 import numpy
 import unittest2 as unittest
 from nupic.bindings.math import Random
@@ -76,7 +76,7 @@
     self.assertTrue(fdrutils.tmDiff2(tm, tm2, VERBOSITY, checkStates=False))
 
     # Learn
-    for i in xrange(5):
+    for i in range(5):
       x = numpy.zeros(tm.numberOfCols, dtype='uint32')
       _RGEN.initializeUInt32Array(x, 2)
       tm.learn(x)
@@ -90,10 +90,10 @@
 
     ## Infer
     patterns = numpy.zeros((4, tm.numberOfCols), dtype='uint32')
-    for i in xrange(4):
+    for i in range(4):
       _RGEN.initializeUInt32Array(patterns[i], 2)
 
-    for i in xrange(10):
+    for i in range(10):
       x = numpy.zeros(tm.numberOfCols, dtype='uint32')
       _RGEN.initializeUInt32Array(x, 2)
       tm.infer(x)
@@ -135,8 +135,8 @@
     sequence = fdrutils.generateCoincMatrix(nCoinc=numPatterns,
                                             length=tm.numberOfCols,
                                             activity=activity)
-    for r in xrange(numRepetitions):
-      for i in xrange(sequence.nRows()):
+    for r in range(numRepetitions):
+      for i in range(sequence.nRows()):
 
         #if i > 11:
         #  setVerbosity(6, tm, tmPy)
@@ -146,8 +146,8 @@
           tmPy.reset()
 
         if verbosity >= 2:
-          print "\n\n    ===================================\nPattern:",
-          print i, "Round:", r, "input:", sequence.getRow(i)
+          print("\n\n    ===================================\nPattern:", end=' ')
+          print(i, "Round:", r, "input:", sequence.getRow(i))
 
         y1 = tm.learn(sequence.getRow(i))
         y2 = tmPy.learn(sequence.getRow(i))
@@ -162,19 +162,19 @@
           tmPy.trimSegments()
 
         if verbosity > 2:
-          print "\n   ------  CPP states  ------ ",
+          print("\n   ------  CPP states  ------ ", end=' ')
           tm.printStates()
-          print "\n   ------  PY states  ------ ",
+          print("\n   ------  PY states  ------ ", end=' ')
           tmPy.printStates()
           if verbosity > 6:
-            print "C++ cells: "
+            print("C++ cells: ")
             tm.printCells()
-            print "PY cells: "
+            print("PY cells: ")
             tmPy.printCells()
 
         if verbosity >= 3:
-          print "Num segments in PY and C++", tmPy.getNumSegments(), \
-              tm.getNumSegments()
+          print("Num segments in PY and C++", tmPy.getNumSegments(), \
+              tm.getNumSegments())
 
         # Check if the two TM's are identical or not. This check is slow so
         # we do it every other iteration. Make it every iteration for debugging
@@ -184,7 +184,7 @@
         # Check that outputs are identical
         self.assertLess(abs((y1 - y2).sum()), 3)
 
-    print "Learning completed"
+    print("Learning completed")
 
     self.assertTrue(fdrutils.tmDiff2(tm, tmPy, verbosity))
 
@@ -194,24 +194,24 @@
     # Remove unconnected synapses and check TM's again
 
     # Test rebuild out synapses
-    print "Rebuilding outSynapses"
+    print("Rebuilding outSynapses")
     tm.cells4.rebuildOutSynapses()
     self.assertTrue(fdrutils.tmDiff2(tm, tmPy, VERBOSITY))
 
-    print "Trimming segments"
+    print("Trimming segments")
     tm.trimSegments()
     tmPy.trimSegments()
     self.assertTrue(fdrutils.tmDiff2(tm, tmPy, VERBOSITY))
 
     # Save and reload after learning
-    print "Pickling and unpickling"
+    print("Pickling and unpickling")
     tm.makeCells4Ephemeral = False
     pickle.dump(tm, open("test_tm_cpp.pkl", "wb"))
     tm2 = pickle.load(open("test_tm_cpp.pkl"))
     self.assertTrue(fdrutils.tmDiff2(tm, tm2, VERBOSITY, checkStates=False))
 
     # Infer
-    print "Testing inference"
+    print("Testing inference")
 
     # Setup for inference
     tm.reset()
@@ -219,7 +219,7 @@
     setVerbosity(INFERENCE_VERBOSITY, tm, tmPy)
 
     patterns = numpy.zeros((40, tm.numberOfCols), dtype='uint32')
-    for i in xrange(4):
+    for i in range(4):
       _RGEN.initializeUInt32Array(patterns[i], 2)
 
     for i, x in enumerate(patterns):
@@ -231,16 +231,16 @@
 
       self.assertTrue(fdrutils.tmDiff2(tm, tmPy, VERBOSITY, checkLearn=False))
       if abs((y - yPy).sum()) > 0:
-        print "C++ output", y
-        print "Py output", yPy
+        print("C++ output", y)
+        print("Py output", yPy)
         assert False
 
       if i > 0:
         tm._checkPrediction(patterns)
         tmPy._checkPrediction(patterns)
 
-    print "Inference completed"
-    print "===================================="
+    print("Inference completed")
+    print("====================================")
 
     return tm, tmPy
 
@@ -250,12 +250,12 @@
     PY versions are identical throughout."""
 
     if short == True:
-      print "Testing short version"
+      print("Testing short version")
     else:
-      print "Testing long version"
+      print("Testing long version")
 
     if short:
-      print "\nTesting with fixed resource CLA - test max segment and synapses"
+      print("\nTesting with fixed resource CLA - test max segment and synapses")
       tm = BacktrackingTMCPP(numberOfCols=30, cellsPerColumn=5,
                              initialPerm=.5, connectedPerm= 0.5,
                              permanenceMax=1,
@@ -271,7 +271,7 @@
       self.basicTest2(tm, numPatterns=15, numRepetitions=1)
 
     if not short:
-      print "\nTesting with fixed resource CLA - test max segment and synapses"
+      print("\nTesting with fixed resource CLA - test max segment and synapses")
       tm = BacktrackingTMCPP(numberOfCols=30, cellsPerColumn=5,
                              initialPerm = .5, connectedPerm= 0.5,
                              permanenceMax = 1,
@@ -286,7 +286,7 @@
       tm.cells4.setCellSegmentOrder(1)
       self.basicTest2(tm, numPatterns=30, numRepetitions=2)
 
-      print "\nTesting with permanenceInc = 0 and Dec = 0"
+      print("\nTesting with permanenceInc = 0 and Dec = 0")
       tm = BacktrackingTMCPP(numberOfCols=30, cellsPerColumn=5,
                              initialPerm = .5, connectedPerm= 0.5,
                              minThreshold = 3, newSynapseCount = 3,
@@ -299,7 +299,7 @@
       tm.printParameters()
       self.basicTest2(tm, numPatterns = 30, numRepetitions = 3)
 
-      print "Testing with permanenceInc = 0 and Dec = 0 and 1 cell per column"
+      print("Testing with permanenceInc = 0 and Dec = 0 and 1 cell per column")
       tm = BacktrackingTMCPP(numberOfCols=30, cellsPerColumn=1,
                              initialPerm = .5, connectedPerm= 0.5,
                              minThreshold = 3, newSynapseCount = 3,
@@ -311,7 +311,7 @@
                              checkSynapseConsistency = False)
       self.basicTest2(tm)
 
-      print "Testing with permanenceInc = 0.1 and Dec = .0"
+      print("Testing with permanenceInc = 0.1 and Dec = .0")
       tm = BacktrackingTMCPP(numberOfCols=30, cellsPerColumn=5,
                              initialPerm = .5, connectedPerm= 0.5,
                              minThreshold = 3, newSynapseCount = 3,
@@ -336,7 +336,7 @@
                              checkSynapseConsistency = True)
       self.basicTest2(tm, numPatterns=10, numRepetitions=2)
 
-      print "Testing age based global decay"
+      print("Testing age based global decay")
       tm = BacktrackingTMCPP(numberOfCols=30, cellsPerColumn=5,
                              initialPerm = .4, connectedPerm= 0.5,
                              minThreshold = 3, newSynapseCount = 3,
@@ -350,7 +350,7 @@
       tm.cells4.setCellSegmentOrder(1)
       self.basicTest2(tm)
 
-      print "\nTesting with fixed size CLA, max segments per cell"
+      print("\nTesting with fixed size CLA, max segments per cell")
       tm = BacktrackingTMCPP(numberOfCols=30, cellsPerColumn=5,
                              initialPerm = .5, connectedPerm= 0.5, permanenceMax = 1,
                              minThreshold = 8, newSynapseCount = 10,
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\backtracking_tm_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\backtracking_tm_test.py	(refactored)
@@ -21,7 +21,7 @@
 
 """Tests for the Python implementation of the temporal memory."""
 
-import cPickle as pickle
+import pickle as pickle
 import csv
 import itertools
 import numpy
@@ -67,7 +67,7 @@
     # Create a model and give it some inputs to learn.
     tm1 = BacktrackingTM(numberOfCols=100, cellsPerColumn=12,
                          verbosity=VERBOSITY)
-    sequences = [self.generateSequence() for _ in xrange(5)]
+    sequences = [self.generateSequence() for _ in range(5)]
     train = list(itertools.chain.from_iterable(sequences[:3]))
     for bottomUpInput in train:
       if bottomUpInput is None:
@@ -107,7 +107,7 @@
     # Create a model and give it some inputs to learn.
     tm1 = BacktrackingTM(numberOfCols=100, cellsPerColumn=12,
                          verbosity=VERBOSITY)
-    sequences = [self.generateSequence() for _ in xrange(5)]
+    sequences = [self.generateSequence() for _ in range(5)]
     train = list(itertools.chain.from_iterable(sequences[:3] +
                                                [sequences[3][:5]]))
     for bottomUpInput in train:
@@ -164,13 +164,13 @@
 
     i = 1
     for r in records[:250]:
-      print i
+      print(i)
       i += 1
       output1 = tm1.compute(r, True, True)
       output2 = tm2.compute(r, True, True)
       self.assertTrue(numpy.array_equal(output1, output2))
 
-    print 'Serializing and deserializing models.'
+    print('Serializing and deserializing models.')
 
     savePath1 = os.path.join(self._tmpDir, 'tm1.bin')
     tmProto1 = BacktrackingTM.getSchema().new_message()
@@ -194,7 +194,7 @@
     self.assertTMsEqual(tm2, tm4)
 
     for r in records[250:]:
-      print i
+      print(i)
       i += 1
       out1 = tm1.compute(r, True, True)
       out2 = tm2.compute(r, True, True)
@@ -214,7 +214,7 @@
     # Create a model and give it some inputs to learn.
     tm1 = BacktrackingTM(numberOfCols=100, cellsPerColumn=12,
                          verbosity=VERBOSITY)
-    sequences = [self.generateSequence() for _ in xrange(5)]
+    sequences = [self.generateSequence() for _ in range(5)]
     train = list(itertools.chain.from_iterable(sequences[:3]))
     for bottomUpInput in train:
       if bottomUpInput is None:
@@ -249,7 +249,7 @@
     # Create a model and give it some inputs to learn.
     tm1 = BacktrackingTM(numberOfCols=100, cellsPerColumn=12,
                          verbosity=VERBOSITY)
-    sequences = [self.generateSequence() for _ in xrange(5)]
+    sequences = [self.generateSequence() for _ in range(5)]
     train = list(itertools.chain.from_iterable(sequences[:3] +
                                                [sequences[3][:5]]))
     for bottomUpInput in train:
@@ -301,13 +301,13 @@
 
     i = 1
     for r in records[:250]:
-      print i
+      print(i)
       i += 1
       output1 = tm1.compute(r, True, True)
       output2 = tm2.compute(r, True, True)
       self.assertTrue(numpy.array_equal(output1, output2))
 
-    print 'Serializing and deserializing models.'
+    print('Serializing and deserializing models.')
 
     savePath1 = os.path.join(self._tmpDir, 'tm1.bin')
     tm1.saveToFile(savePath1)
@@ -323,7 +323,7 @@
     self.assertTMsEqual(tm2, tm4)
 
     for r in records[250:]:
-      print i
+      print(i)
       i += 1
       out1 = tm1.compute(r, True, True)
       out2 = tm2.compute(r, True, True)
@@ -354,7 +354,7 @@
     """Generates a sequence of n patterns."""
     return [None] + [BacktrackingTMTest.generatePattern(numCols, minOnes,
                                                         maxOnes)
-                     for _ in xrange(n)]
+                     for _ in range(n)]
 
 
   @staticmethod
@@ -370,7 +370,7 @@
     assert maxOnes < numCols
 
     nOnes = random.randint(minOnes, maxOnes)
-    ind = random.sample(xrange(numCols), nOnes)
+    ind = random.sample(range(numCols), nOnes)
     x = numpy.zeros(numCols, dtype='float32')
     x[ind] = 1
 
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\knn_classifier_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\knn_classifier_test.py	(refactored)
@@ -179,20 +179,20 @@
     b = np.array([2, 4, 8, 12, 14, 18, 20, 28, 30], dtype=np.int32)
 
     numPatterns = classifier.learn(a, 0, isSparse=dimensionality)
-    self.assertEquals(numPatterns, 1)
+    self.assertEqual(numPatterns, 1)
 
     numPatterns = classifier.learn(b, 1, isSparse=dimensionality)
-    self.assertEquals(numPatterns, 2)
+    self.assertEqual(numPatterns, 2)
 
     denseA = np.zeros(dimensionality)
     denseA[a] = 1.0
     cat, _, _, _ = classifier.infer(denseA)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
     denseB = np.zeros(dimensionality)
     denseB[b] = 1.0
     cat, _, _, _ = classifier.infer(denseB)
-    self.assertEquals(cat, 1)
+    self.assertEqual(cat, 1)
 
 
   def testMinSparsity(self):
@@ -213,16 +213,16 @@
     d = np.array([2, 3, 8, 11, 18], dtype=np.int32)
 
     numPatterns = classifier.learn(a, 0, isSparse=dimensionality)
-    self.assertEquals(numPatterns, 1)
+    self.assertEqual(numPatterns, 1)
 
     numPatterns = classifier.learn(b, 1, isSparse=dimensionality)
-    self.assertEquals(numPatterns, 2)
+    self.assertEqual(numPatterns, 2)
 
     numPatterns = classifier.learn(c, 1, isSparse=dimensionality)
-    self.assertEquals(numPatterns, 3)
+    self.assertEqual(numPatterns, 3)
 
     numPatterns = classifier.learn(d, 1, isSparse=dimensionality)
-    self.assertEquals(numPatterns, 3)
+    self.assertEqual(numPatterns, 3)
 
     # Test that inference ignores low sparsity vectors but not others
     e = np.array([2, 4, 5, 6, 8, 12, 14, 18, 20], dtype=np.int32)
@@ -271,16 +271,16 @@
     classifier.learn(b, 1, isSparse=dimensionality, partitionId=1)
 
     cat, _, _, _ = classifier.infer(denseA, partitionId=1)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
     cat, _, _, _ = classifier.infer(denseA, partitionId=0)
-    self.assertEquals(cat, 1)
+    self.assertEqual(cat, 1)
 
     cat, _, _, _ = classifier.infer(denseB, partitionId=0)
-    self.assertEquals(cat, 1)
+    self.assertEqual(cat, 1)
 
     cat, _, _, _ = classifier.infer(denseB, partitionId=1)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
     # Ensure it works even if you invoke learning again. To make it a bit more
     # complex this time we insert A again but now with Id=2
@@ -289,7 +289,7 @@
     # Even though first A should be ignored, the second instance of A should
     # not be ignored.
     cat, _, _, _ = classifier.infer(denseA, partitionId=0)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
 
   def testGetPartitionId(self):
@@ -322,23 +322,23 @@
     classifier.learn(c, 1, isSparse=dimensionality, partitionId=None)
     classifier.learn(d, 1, isSparse=dimensionality, partitionId=433)
 
-    self.assertEquals(classifier.getPartitionId(0), 433)
-    self.assertEquals(classifier.getPartitionId(1), 213)
-    self.assertEquals(classifier.getPartitionId(2), None)
-    self.assertEquals(classifier.getPartitionId(3), 433)
+    self.assertEqual(classifier.getPartitionId(0), 433)
+    self.assertEqual(classifier.getPartitionId(1), 213)
+    self.assertEqual(classifier.getPartitionId(2), None)
+    self.assertEqual(classifier.getPartitionId(3), 433)
 
     cat, _, _, _ = classifier.infer(denseA, partitionId=213)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
     # Test with patternId not in classifier
     cat, _, _, _ = classifier.infer(denseA, partitionId=666)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
     # Partition Ids should be maintained after inference
-    self.assertEquals(classifier.getPartitionId(0), 433)
-    self.assertEquals(classifier.getPartitionId(1), 213)
-    self.assertEquals(classifier.getPartitionId(2), None)
-    self.assertEquals(classifier.getPartitionId(3), 433)
+    self.assertEqual(classifier.getPartitionId(0), 433)
+    self.assertEqual(classifier.getPartitionId(1), 213)
+    self.assertEqual(classifier.getPartitionId(2), None)
+    self.assertEqual(classifier.getPartitionId(3), 433)
 
     # Should return exceptions if we go out of bounds
     with self.assertRaises(RuntimeError):
@@ -348,7 +348,7 @@
 
     # Learn again
     classifier.learn(e, 4, isSparse=dimensionality, partitionId=413)
-    self.assertEquals(classifier.getPartitionId(4), 413)
+    self.assertEqual(classifier.getPartitionId(4), 413)
 
     # Test getPatternIndicesWithPartitionId
     self.assertItemsEqual(classifier.getPatternIndicesWithPartitionId(433),
@@ -358,7 +358,7 @@
     self.assertItemsEqual(classifier.getPatternIndicesWithPartitionId(413),
                           [4])
 
-    self.assertEquals(classifier.getNumPartitionIds(), 3)
+    self.assertEqual(classifier.getNumPartitionIds(), 3)
 
     # Check that the full set of partition ids is what we expect
     self.assertItemsEqual(classifier.getPartitionIdList(),
@@ -366,7 +366,7 @@
     self.assertItemsEqual(classifier.getPartitionIdKeys(), [433, 413, 213])
 
     # Remove two rows - all indices shift down
-    self.assertEquals(classifier._removeRows([0,2]), 2)
+    self.assertEqual(classifier._removeRows([0,2]), 2)
     self.assertItemsEqual(classifier.getPatternIndicesWithPartitionId(433),
                           [1])
     self.assertItemsEqual(classifier.getPatternIndicesWithPartitionId(413),
@@ -374,7 +374,7 @@
 
     # Remove another row and check number of partitions have decreased
     classifier._removeRows([0])
-    self.assertEquals(classifier.getNumPartitionIds(), 2)
+    self.assertEqual(classifier.getNumPartitionIds(), 2)
 
     # Check that the full set of partition ids is what we expect
     self.assertItemsEqual(classifier.getPartitionIdList(), [433, 413])
@@ -408,13 +408,13 @@
     classifier.learn(d, 1, isSparse=dimensionality, partitionId=405)
 
     cat, _, _, _ = classifier.infer(denseA, partitionId=405)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
     cat, _, _, _ = classifier.infer(denseD, partitionId=405)
-    self.assertEquals(cat, 2)
+    self.assertEqual(cat, 2)
 
     cat, _, _, _ = classifier.infer(denseD)
-    self.assertEquals(cat, 1)
+    self.assertEqual(cat, 1)
 
 
   @unittest.skipUnless(__debug__, "Only applicable when asserts are enabled")
@@ -441,13 +441,13 @@
     # Learn with incorrect dimensionality, greater than largest ON bit, but
     # inconsistent when inferring
     numPatterns = classifier.learn(a, 0, isSparse=31)
-    self.assertEquals(numPatterns, 1)
+    self.assertEqual(numPatterns, 1)
 
     denseA = np.zeros(dimensionality)
     denseA[a] = 1.0
 
     cat, _, _, _ = classifier.infer(denseA)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
 
   @unittest.skipUnless(__debug__, "Only applicable when asserts are enabled")
@@ -476,12 +476,12 @@
     a = np.array([], dtype=np.int32)
 
     numPatterns = classifier.learn(a, 0, isSparse=dimensionality)
-    self.assertEquals(numPatterns, 1)
+    self.assertEqual(numPatterns, 1)
 
     denseA = np.zeros(dimensionality)
     denseA[a] = 1.0
     cat, _, _, _ = classifier.infer(denseA)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
 
   @unittest.skip("Finish when infer has options for sparse and dense "
@@ -500,10 +500,10 @@
     # TODO Test case where infer is passed a sparse representation after
     # infer() has been extended to handle sparse and dense
     cat, _, _, _ = classifier.infer(a)
-    self.assertEquals(cat, 0)
+    self.assertEqual(cat, 0)
 
     cat, _, _, _ = classifier.infer(b)
-    self.assertEquals(cat, 1)
+    self.assertEqual(cat, 1)
 
 
   @unittest.skipUnless(
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\sdr_classifier_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\sdr_classifier_test.py	(refactored)
@@ -22,7 +22,7 @@
 """Unit tests for SDRClassifier module."""
 
 
-import cPickle as pickle
+import pickle as pickle
 import random
 import tempfile
 import types
@@ -76,7 +76,7 @@
 
     # Enough times to perform Inference and learn associations
     retval = []
-    for recordNum in xrange(10):
+    for recordNum in range(10):
       retval = self._compute(classifier, recordNum, [1, 5], 0, 10)
 
     self.assertEqual(retval["actualValues"][0], 10)
@@ -90,7 +90,7 @@
 
     # Enough times to perform Inference and learn associations
     retval = []
-    for recordNum in xrange(10):
+    for recordNum in range(10):
       retval = self._compute(classifier, recordNum, [1, 5], 0, 10)
 
     self.assertEqual(retval["actualValues"][0], 10)
@@ -130,7 +130,7 @@
     retval = c.compute(recordNum=recordNum, patternNZ=[1, 5, 9],
               classification={"bucketIdx": 4, "actValue": 34.7},
               learn=True, infer=False)
-    self.assertEquals({}, retval)
+    self.assertEqual({}, retval)
     recordNum += 1
 
     # infer only
@@ -149,7 +149,7 @@
     retval3 = c.compute(recordNum=recordNum, patternNZ=[1, 2],
                         classification={"bucketIdx": 2, "actValue": 14.2},
                         learn=False, infer=False)
-    self.assertEquals({}, retval3)
+    self.assertEqual({}, retval3)
 
 
   def testCompute1(self):
@@ -246,7 +246,7 @@
                                               "actValue": None},
                               learn=True, infer=True)
     for value in predictResult["actualValues"]:
-      self.assertIsInstance(value, (types.NoneType, types.StringType))
+      self.assertIsInstance(value, (type(None), bytes))
 
 
   def testComputeCategory2(self):
@@ -353,10 +353,10 @@
                      [0, 10, 20, 30, 40, 50, 60, 70, 80, 90])
 
     self.assertGreater(retval[1][0], 0.99)
-    for i in xrange(1, 10):
+    for i in range(1, 10):
       self.assertLess(retval[1][i], 0.01)
     self.assertGreater(retval[2][1], 0.99)
-    for i in [0] + range(2, 10):
+    for i in [0] + list(range(2, 10)):
       self.assertLess(retval[2][i], 0.01)
 
 
@@ -496,7 +496,7 @@
     SDR2 = [2, 4, 6]
     recordNum = 0
     random.seed(42)
-    for _ in xrange(5000):
+    for _ in range(5000):
       randomNumber = random.random()
       if randomNumber < 0.3:
         bucketIdx = 0
@@ -560,7 +560,7 @@
     SDR2[5] = SDR1[11]
 
     random.seed(42)
-    for _ in xrange(5000):
+    for _ in range(5000):
       randomNumber = random.random()
       if randomNumber < 0.3:
         bucketIdx = 0
@@ -616,7 +616,7 @@
     SDR2 = [2, 4, 6]
     recordNum = 0
     random.seed(42)
-    for _ in xrange(5000):
+    for _ in range(5000):
       c.compute(recordNum=recordNum, patternNZ=SDR1,
                 classification={"bucketIdx": [0, 1], "actValue": [0, 1]},
                 learn=True, infer=False)
@@ -671,7 +671,7 @@
     SDR2 = [2, 4, 6]
 
     random.seed(42)
-    for _ in xrange(10000):
+    for _ in range(10000):
       randomNumber = random.random()
       if randomNumber < 0.3:
         bucketIdx = 0
@@ -711,7 +711,7 @@
     self.assertAlmostEqual(result2[0][1], 0.5, places=1)
     self.assertAlmostEqual(result2[0][3], 0.5, places=1)
 
-    for _ in xrange(20000):
+    for _ in range(20000):
       randomNumber = random.random()
       if randomNumber < 0.3:
         bucketIdx = 0
@@ -763,7 +763,7 @@
     SDR1 = [1, 3, 5]
     SDR2 = [2, 4, 6]
     recordNum = 0
-    for _ in xrange(100):
+    for _ in range(100):
       c.compute(recordNum=recordNum, patternNZ=SDR1,
                 classification={"bucketIdx": 0, "actValue": 0},
                 learn=True, infer=False)
@@ -816,8 +816,8 @@
     self.assertAlmostEqual(c1.alpha, c2.alpha)
     self.assertAlmostEqual(c1.actValueAlpha, c2.actValueAlpha)
     self.assertEqual(c1._patternNZHistory, c2._patternNZHistory)
-    self.assertEqual(c1._weightMatrix.keys(), c2._weightMatrix.keys())
-    for step in c1._weightMatrix.keys():
+    self.assertEqual(list(c1._weightMatrix.keys()), list(c2._weightMatrix.keys()))
+    for step in list(c1._weightMatrix.keys()):
       c1Weight = c1._weightMatrix[step]
       c2Weight = c2._weightMatrix[step]
       self.assertSequenceEqual(list(c1Weight.flatten()),
@@ -825,7 +825,7 @@
     self.assertEqual(c1._maxBucketIdx, c2._maxBucketIdx)
     self.assertEqual(c1._maxInputIdx, c2._maxInputIdx)
     self.assertEqual(len(c1._actualValues), len(c2._actualValues))
-    for i in xrange(len(c1._actualValues)):
+    for i in range(len(c1._actualValues)):
       self.assertAlmostEqual(c1._actualValues[i], c2._actualValues[i], 5)
     self.assertEqual(c1._version, c2._version)
     self.assertEqual(c1.verbosity, c2.verbosity)
@@ -843,13 +843,13 @@
                          classification={'bucketIdx': 4, 'actValue': 34.7},
                          learn=True, infer=True)
 
-    self.assertEqual(result1.keys(), result2.keys())
-
-    for key in result1.keys():
+    self.assertEqual(list(result1.keys()), list(result2.keys()))
+
+    for key in list(result1.keys()):
       self.assertEqual(len(result1[key]), len(result2[key]))
       self.assertEqual(len(result1[key]), expectedActualValuesLen)
 
-      for i in xrange(expectedActualValuesLen):
+      for i in range(expectedActualValuesLen):
         self.assertAlmostEqual(result1[key][i], result2[key][i], 5)
 
 
@@ -867,8 +867,8 @@
 
   def test_pFormatArray(self):
     from nupic.algorithms.sdr_classifier import _pFormatArray
-    pretty = _pFormatArray(range(10))
-    self.assertIsInstance(pretty, basestring)
+    pretty = _pFormatArray(list(range(10)))
+    self.assertIsInstance(pretty, str)
     self.assertEqual(pretty[0], "[")
     self.assertEqual(pretty[-1], "]")
     self.assertEqual(len(pretty.split(" ")), 12)
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\sp_learn_inference_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\sp_learn_inference_test.py	(refactored)
@@ -26,7 +26,7 @@
 TODO: implement an SP Diff routine.  That should be fun!
 """
 
-import cPickle as pickle
+import pickle as pickle
 import numpy as np
 import random
 import time
@@ -83,9 +83,9 @@
 
     # Build up training set with numTrainingRecords patterns
     inputs = []         # holds post-encoded input patterns
-    for i in xrange(numTrainingRecords):
+    for i in range(numTrainingRecords):
       inputVector = np.zeros(n, dtype=realDType)
-      inputVector [random.sample(xrange(n), w)] = 1
+      inputVector [random.sample(range(n), w)] = 1
       inputs.append(inputVector)
 
     # Train each SP with identical inputs
@@ -93,9 +93,9 @@
 
     random.seed(seed)
     np.random.seed(seed)
-    for i in xrange(numTrainingRecords):
+    for i in range(numTrainingRecords):
       if spVerbosity > 0:
-        print "Input #%d" % i
+        print("Input #%d" % i)
       # TODO: See https://github.com/numenta/nupic/issues/2072
       encodedInput = inputs[i]
       decodedOutput = np.zeros(columnDimensions)
@@ -103,15 +103,15 @@
 
     random.seed(seed)
     np.random.seed(seed)
-    for i in xrange(numTrainingRecords):
+    for i in range(numTrainingRecords):
       if spVerbosity > 0:
-        print "Input #%d" % i
+        print("Input #%d" % i)
       # TODO: See https://github.com/numenta/nupic/issues/2072
       encodedInput = inputs[i]
       decodedOutput = np.zeros(columnDimensions)
       spLearnInfer.compute(encodedInput, learn=True, activeArray=decodedOutput)
 
-    print "\nElapsed time: %.2f seconds\n" % (time.time() - startTime)
+    print("\nElapsed time: %.2f seconds\n" % (time.time() - startTime))
 
     # Test that both SP"s are identical by checking learning stats
     # A more in depth test would check all the coincidences, duty cycles, etc.
@@ -129,14 +129,14 @@
     spLearnOnlyLoaded = pickle.loads(spPickle)
     success = success and spDiff(spLearnOnly, spLearnOnlyLoaded)
     self.assertTrue(success)
-    for k in learnOnlyStats.keys():
+    for k in list(learnOnlyStats.keys()):
       if learnOnlyStats[k] != learnInferStats[k]:
         success = False
-        print "Stat", k, "is different:", learnOnlyStats[k], learnInferStats[k]
+        print("Stat", k, "is different:", learnOnlyStats[k], learnInferStats[k])
 
     self.assertTrue(success)
     if success:
-      print "Test succeeded"
+      print("Test succeeded")
 
 
   @unittest.skip("Currently fails due to switch from FDRCSpatial2 to SpatialPooler."
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\sp_overlap_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\sp_overlap_test.py	(refactored)
@@ -99,8 +99,8 @@
     """ Helper function that tests whether the SP predicts the most
     frequent record """
 
-    print "\nRunning SP overlap test..."
-    print encoder, 'encoder,', 'Random seed:', seed, 'and', numColors, 'colors'
+    print("\nRunning SP overlap test...")
+    print(encoder, 'encoder,', 'Random seed:', seed, 'and', numColors, 'colors')
     #Setting up SP and creating training patterns
 
     # Instantiate Spatial Pooler
@@ -128,20 +128,20 @@
     if encoder=='scalar':
       enc = scalar.ScalarEncoder(name='car', w=w, n=n, minval=minVal,
                                  maxval=maxVal, periodic=False, forced=True) # forced: it's strongly recommended to use w>=21, in the example we force skip the check for readibility
-      for y in xrange(numColors):
+      for y in range(numColors):
         temp = enc.encode(rnd.random()*maxVal)
         colors.append(numpy.array(temp, dtype=realDType))
     else:
-      for y in xrange(numColors):
+      for y in range(numColors):
         sdr = numpy.zeros(n, dtype=realDType)
         # Randomly setting w out of n bits to 1
-        sdr[rnd.sample(xrange(n), w)] = 1
+        sdr[rnd.sample(range(n), w)] = 1
         colors.append(sdr)
 
     # Training the sp
-    print 'Starting to train the sp on', numColors, 'patterns'
+    print('Starting to train the sp on', numColors, 'patterns')
     startTime = time.time()
-    for i in xrange(numColors):
+    for i in range(numColors):
       # TODO: See https://github.com/numenta/nupic/issues/2072
       spInput = colors[i]
       onCells = numpy.zeros(columnDimensions)
@@ -163,10 +163,10 @@
       patterns.update(activeCoincIndices)
 
       if (i + 1) % 100 == 0:
-        print 'Record number:', i + 1
-
-        print "Elapsed time: %.2f seconds" % (time.time() - startTime)
-        print len(reUsedCoincs), "re-used coinc(s),"
+        print('Record number:', i + 1)
+
+        print("Elapsed time: %.2f seconds" % (time.time() - startTime))
+        print(len(reUsedCoincs), "re-used coinc(s),")
 
     # Check if results match expectations
     summ = []
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_boost_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_boost_test.py	(refactored)
@@ -43,8 +43,8 @@
 
 def _areAllSDRsUnique(sdrDict):
   """Return True iff all the SDR's in the dict are unique."""
-  for k1, v1 in sdrDict.iteritems():
-    for k2, v2 in sdrDict.iteritems():
+  for k1, v1 in sdrDict.items():
+    for k2, v2 in sdrDict.items():
       # Return false if two different keys have identical SDR's
       if (k2 != k1) and ((v1 == v2).sum() == v1.size):
         return False
@@ -134,7 +134,7 @@
       'boostStrength':              10.0,
       'seed':                       SEED,
     }
-    print "SP seed set to:", self.params['seed']
+    print("SP seed set to:", self.params['seed'])
 
   def debugPrint(self):
     """
@@ -146,22 +146,22 @@
     
     boost = numpy.zeros(self.columnDimensions, dtype=GetNTAReal())
     self.sp.getBoostFactors(boost)
-    print "\n--------- ITERATION", (
-      self.sp.getIterationNum() ),"-----------------------"
-    print "SP implementation:", self.spImplementation
-    print "Learning iteration:",
-    print "Max/min active duty cycle:", (
-      activeDutyCycle.max(), activeDutyCycle.min() )
-    print "Average non-zero active duty cycle:", (
-      activeDutyCycle[activeDutyCycle>0].mean() )
-    print "Active duty cycle", activeDutyCycle
-    print
-    print "Boost factor for sp:", boost
-    print
-    print "Last winning iteration for each column"
-    print self.winningIteration
-    print "Number of columns that have won at some point:", (
-      self.columnDimensions - (self.winningIteration==0).sum() )
+    print("\n--------- ITERATION", (
+      self.sp.getIterationNum() ),"-----------------------")
+    print("SP implementation:", self.spImplementation)
+    print("Learning iteration:", end=' ')
+    print("Max/min active duty cycle:", (
+      activeDutyCycle.max(), activeDutyCycle.min() ))
+    print("Average non-zero active duty cycle:", (
+      activeDutyCycle[activeDutyCycle>0].mean() ))
+    print("Active duty cycle", activeDutyCycle)
+    print()
+    print("Boost factor for sp:", boost)
+    print()
+    print("Last winning iteration for each column")
+    print(self.winningIteration)
+    print("Number of columns that have won at some point:", (
+      self.columnDimensions - (self.winningIteration==0).sum() ))
 
     
   def verifySDRProperties(self):
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_compatability_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_compatability_test.py	(refactored)
@@ -19,7 +19,7 @@
 # http://numenta.org/licenses/
 # ----------------------------------------------------------------------
 
-import cPickle as pickle
+import pickle as pickle
 import numpy
 import time
 import traceback
@@ -126,8 +126,8 @@
     cppSp.getMinOverlapDutyCycles(cppMinOverlap)
     self.assertListAlmostEqual(list(pyMinOverlap), list(cppMinOverlap))
 
-    for i in xrange(pySp.getNumColumns()):
-      if self.verbosity > 2: print "Column:",i
+    for i in range(pySp.getNumColumns()):
+      if self.verbosity > 2: print("Column:",i)
       pyPot = numpy.zeros(numInputs).astype(uintType)
       cppPot = numpy.zeros(numInputs).astype(uintType)
       pySp.getPotential(i, pyPot)
@@ -178,13 +178,13 @@
       randomState.rand(numRecords,numInputs) > threshold).astype(uintType)
 
     # Run side by side for numRecords iterations
-    for i in xrange(numRecords):
+    for i in range(numRecords):
       if learnMode is None:
         learn = (randomState.rand() > 0.5)
       else:
         learn = learnMode
       if self.verbosity > 1:
-        print "Iteration:",i,"learn=",learn
+        print("Iteration:",i,"learn=",learn)
       PyActiveArray = numpy.zeros(numColumns).astype(uintType)
       CppActiveArray = numpy.zeros(numColumns).astype(uintType)
       inputVector = inputMatrix[i,:]
@@ -219,14 +219,14 @@
     inputMatrix = (
       randomState.rand(numRecords,numInputs) > threshold).astype(uintType)
 
-    for i in xrange(numRecords/2):
+    for i in range(numRecords/2):
       activeArray = numpy.zeros(numColumns).astype(uintType)
       inputVector = inputMatrix[i,:]
       learn = (randomState.rand() > 0.5)
       sp1.compute(inputVector, learn, activeArray)
 
     sp2 = pickle.loads(pickle.dumps(sp1))
-    for i in xrange(numRecords/2+1,numRecords):
+    for i in range(numRecords/2+1,numRecords):
       activeArray1 = numpy.zeros(numColumns).astype(uintType)
       activeArray2 = numpy.zeros(numColumns).astype(uintType)
       inputVector = inputMatrix[i,:]
@@ -414,7 +414,7 @@
         inputDimensions=[121], columnDimensions=[300])
 
     data = numpy.zeros([121], dtype=uintType)
-    for i in xrange(21):
+    for i in range(21):
       data[i] = 1
 
     nCols = 300
@@ -440,7 +440,7 @@
         inputDimensions=[121, 1], columnDimensions=[30, 30])
 
     data = numpy.zeros([121, 1], dtype=uintType)
-    for i in xrange(21):
+    for i in range(21):
       data[i][0] = 1
 
     nCols = 900
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_compute_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_compute_test.py	(refactored)
@@ -94,7 +94,7 @@
       'globalInhibition': True,
       "seed": int((time.time()%10000)*10),
     }
-    print "testBasicCompute1, SP seed set to:",params['seed']
+    print("testBasicCompute1, SP seed set to:",params['seed'])
     self.basicComputeLoop('py', params, inputSize, columnDimensions)
     self.basicComputeLoop('cpp', params, inputSize, columnDimensions)
 
@@ -119,7 +119,7 @@
       "synPermInactiveDec": 0.0,
       "seed": int((time.time()%10000)*10),
     }
-    print "testBasicCompute2, SP seed set to:",params['seed']
+    print("testBasicCompute2, SP seed set to:",params['seed'])
     self.basicComputeLoop('py', params, inputSize, columnDimensions)
     self.basicComputeLoop('cpp', params, inputSize, columnDimensions)
 
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_cpp_api_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_cpp_api_test.py	(refactored)
@@ -22,7 +22,7 @@
 import unittest2 as unittest
 from nupic.bindings.algorithms import SpatialPooler as CPPSpatialPooler
 
-import spatial_pooler_py_api_test
+from . import spatial_pooler_py_api_test
 
 spatial_pooler_py_api_test.SpatialPooler = CPPSpatialPooler
 SpatialPoolerCPPAPITest = spatial_pooler_py_api_test.SpatialPoolerAPITest
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_cpp_unit_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_cpp_unit_test.py	(refactored)
@@ -127,7 +127,7 @@
 
     expectedConnectedCounts = [2, 2, 2, 2, 3]
 
-    for i in xrange(5):
+    for i in range(5):
       permanences = np.array(permanencesList[i], dtype=realDType)
       expectedPermanences = np.array(expectedPermanencesList[i],
                                      dtype=realDType)
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_py_api_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_py_api_test.py	(refactored)
@@ -21,7 +21,7 @@
 
 from mock import Mock, patch, ANY, call
 import numpy
-import cPickle as pickle
+import pickle as pickle
 import unittest2 as unittest
 
 from nupic.bindings.math import GetNTAReal
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_unit_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\spatial_pooler_unit_test.py	(refactored)
@@ -100,14 +100,14 @@
 
     sp._potentialPools = BinaryCorticalColumns(numpy.ones([sp._numColumns,
                                                            sp._numInputs]))
-    sp._inhibitColumns = Mock(return_value = numpy.array(range(5)))
+    sp._inhibitColumns = Mock(return_value = numpy.array(list(range(5))))
 
     inputVector = numpy.array([1, 0, 1, 0, 1, 0, 0, 1, 1])
     activeArray = numpy.zeros(5)
-    for i in xrange(20):
+    for i in range(20):
       sp.compute(inputVector, True, activeArray)
 
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       perm = sp._permanences.getRow(i)
       self.assertEqual(list(perm), list(inputVector))
 
@@ -134,14 +134,14 @@
         seed=getSeed(),
         spVerbosity=0)
 
-    sp._inhibitColumns = Mock(return_value = numpy.array(range(5)))
+    sp._inhibitColumns = Mock(return_value = numpy.array(list(range(5))))
 
     inputVector = numpy.ones(sp._numInputs)
     activeArray = numpy.zeros(5)
-    for i in xrange(20):
+    for i in range(20):
       sp.compute(inputVector, True, activeArray)
 
-    for columnIndex in xrange(sp._numColumns):
+    for columnIndex in range(sp._numColumns):
       potential = sp._potentialPools[columnIndex]
       perm = sp._permanences.getRow(columnIndex)
       self.assertEqual(list(perm), list(potential))
@@ -351,7 +351,7 @@
     activeArray = numpy.ones(6)
     sp.stripUnlearnedColumns(activeArray)
     stripped = numpy.where(activeArray == 1)[0]
-    trueStripped = range(6)
+    trueStripped = list(range(6))
     self.assertListEqual(trueStripped, list(stripped))
 
 
@@ -740,7 +740,7 @@
                              [1, 1, 1, 1, 1, 1, 1, 1]]))
 
     trueAvgConnectedSpan = [7, 5, 1, 5, 0, 2, 3, 3, 8]
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       connectedSpan = sp._avgConnectedSpanForColumn1D(i)
       self.assertEqual(trueAvgConnectedSpan[i], connectedSpan)
 
@@ -763,7 +763,7 @@
                              [1, 1, 1, 1, 1, 1, 1, 1]]))
 
     trueAvgConnectedSpan = [7, 5, 1, 5, 0, 2, 3, 3, 8]
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       connectedSpan = sp._avgConnectedSpanForColumn1D(i)
       self.assertEqual(trueAvgConnectedSpan[i], connectedSpan)
 
@@ -827,12 +827,12 @@
       ])
 
     trueAvgConnectedSpan = [3, 3, 4.5, 3, 2.5, 2, 0]
-    for columnIndex in xrange(sp._numColumns):
+    for columnIndex in range(sp._numColumns):
       sp._connectedSynapses.replace(
         columnIndex, connected[columnIndex].reshape(-1).nonzero()[0]
       )
 
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       connectedSpan = sp._avgConnectedSpanForColumn2D(i)
       self.assertEqual(trueAvgConnectedSpan[i], connectedSpan)
 
@@ -886,7 +886,7 @@
 
     trueAvgConnectedSpan = [11.0/4, 6.0/4, 14.0/4, 15.0/4, 0]
 
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       connectedSpan = sp._avgConnectedSpanForColumnND(i)
       self.assertAlmostEqual(trueAvgConnectedSpan[i], connectedSpan)
 
@@ -926,9 +926,9 @@
       [0.110, 0.748, 0.055, 0.000, 0.060, 0.000, 0.218, 0.000]]
 
     sp._bumpUpWeakColumns()
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       perm = list(sp._permanences.getRow(i))
-      for j in xrange(sp._numInputs):
+      for j in range(sp._numInputs):
         self.assertAlmostEqual(truePermanences[i][j], perm[j])
 
 
@@ -978,7 +978,7 @@
     sp._activeDutyCycles = numpy.array([0.6, 0.07, 0.5, 0.4, 0.3])
     sp._updateMinDutyCyclesGlobal()
     trueMinOverlapDutyCycles = sp._numColumns*[0.01*6]
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       self.assertAlmostEqual(trueMinOverlapDutyCycles[i],
                              sp._minOverlapDutyCycles[i])
 
@@ -988,7 +988,7 @@
     sp._activeDutyCycles = numpy.array([0.16, 0.007, 0.15, 0.54, 0.13])
     sp._updateMinDutyCyclesGlobal()
     trueMinOverlapDutyCycles = sp._numColumns*[0.015*2.4]
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       self.assertAlmostEqual(trueMinOverlapDutyCycles[i],
                              sp._minOverlapDutyCycles[i])
 
@@ -998,7 +998,7 @@
     sp._activeDutyCycles = numpy.zeros(5)
     sp._updateMinDutyCyclesGlobal()
     trueMinOverlapDutyCycles = sp._numColumns * [0]
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       self.assertAlmostEqual(trueMinOverlapDutyCycles[i],
                              sp._minOverlapDutyCycles[i])
 
@@ -1065,9 +1065,9 @@
       #    -      -      -      -      -      -      -       -
 
     sp._adaptSynapses(inputVector, activeColumns)
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       perm = list(sp._permanences.getRow(i))
-      for j in xrange(sp._numInputs):
+      for j in range(sp._numInputs):
         self.assertAlmostEqual(truePermanences[i][j], perm[j])
 
     sp._potentialPools = BinaryCorticalColumns(
@@ -1096,9 +1096,9 @@
         #  -    -      -      -      -       -       -     -
 
     sp._adaptSynapses(inputVector, activeColumns)
-    for i in xrange(sp._numColumns):
+    for i in range(sp._numColumns):
       perm = list(sp._permanences.getRow(i))
-      for j in xrange(sp._numInputs):
+      for j in range(sp._numInputs):
         self.assertAlmostEqual(truePermanences[i][j], perm[j])
 
 
@@ -1131,11 +1131,11 @@
         [0.22, 0.1001, 0.15, 0.051, 0.07],  # increment four times
         [0.101, 0.101, 0.101, 0.101, 0.101]]  #increment 9 times
 
-    maskPP = numpy.array(range(5))
-    for i in xrange(sp._numColumns):
+    maskPP = numpy.array(list(range(5)))
+    for i in range(sp._numColumns):
       perm = sp._permanences.getRow(i)
       sp._raisePermanenceToThreshold(perm, maskPP)
-      for j in xrange(sp._numInputs):
+      for j in range(sp._numInputs):
         self.assertAlmostEqual(truePermanences[i][j], perm[j])
 
 
@@ -1172,7 +1172,7 @@
       [1, 1, 0, 0, 1]]
 
     trueConnectedCounts = [2, 2, 2, 2, 3]
-    for columnIndex in xrange(sp._numColumns):
+    for columnIndex in range(sp._numColumns):
       sp._updatePermanencesForColumn(permanences[columnIndex], columnIndex)
       self.assertListEqual(
         trueConnectedSynapses[columnIndex],
@@ -1242,7 +1242,7 @@
                              [0, 0, 0, 0, 1, 0, 0, 0, 0, 1]]))
     sp._connectedCounts = numpy.array([2.0, 2.0, 2.0, 2.0, 2.0])
     inputVector = numpy.zeros(sp._numInputs, dtype='float32')
-    inputVector[range(0, 10, 2)] = 1
+    inputVector[list(range(0, 10, 2))] = 1
     overlaps = sp._calculateOverlap(inputVector)
     overlapsPct = sp._calculateOverlapPct(overlaps)
     trueOverlaps = list(numpy.array([1, 1, 1, 1, 1], dtype=realDType))
@@ -1387,10 +1387,10 @@
 
     density = 0.5
     sp._numColumns = 10
-    overlaps = numpy.array(range(10), dtype=realDType)
+    overlaps = numpy.array(list(range(10)), dtype=realDType)
     active = list(sp._inhibitColumnsGlobal(overlaps, density))
     trueActive = numpy.zeros(sp._numColumns)
-    trueActive = range(5, 10)
+    trueActive = list(range(5, 10))
     self.assertListEqual(trueActive, sorted(active))
 
 
@@ -1493,7 +1493,7 @@
 
     # Check that the two spatial poolers have the same attributes
     self.assertSetEqual(set(sp1.__dict__.keys()), set(sp2.__dict__.keys()))
-    for k, v1 in sp1.__dict__.iteritems():
+    for k, v1 in sp1.__dict__.items():
       v2 = getattr(sp2, k)
       if k in ephemeral:
         continue
@@ -1507,7 +1507,7 @@
       elif isinstance(v1, float):
         self.assertAlmostEqual(v1, v2)
       elif isinstance(v1, numbers.Integral):
-        self.assertEqual(long(v1), long(v2), k)
+        self.assertEqual(int(v1), int(v2), k)
       else:
         self.assertEqual(type(v1), type(v2), k)
         self.assertEqual(v1, v2, k)
--- d:\nupic\src\python\python27\tests\unit\nupic\algorithms\temporal_memory_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\algorithms\temporal_memory_test.py	(refactored)
@@ -801,7 +801,7 @@
   def testAddSegmentToCellWithFewestSegments(self):
     grewOnCell1 = False
     grewOnCell2 = False
-    for seed in xrange(100):
+    for seed in range(100):
       tm = TemporalMemory(
         columnDimensions=[32],
         cellsPerColumn=4,
--- d:\nupic\src\python\python27\tests\unit\nupic\data\file_record_stream_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\data\file_record_stream_test.py	(refactored)
@@ -66,7 +66,7 @@
     fieldNames = ['name', 'timestamp', 'integer', 'real', 'reset', 'sid',
                   'categoryField']
 
-    print 'Creating temp file:', filename
+    print('Creating temp file:', filename)
 
     with FileRecordStream(streamID=filename, write=True, fields=fields) as s:
 
@@ -81,9 +81,9 @@
       self.assertEqual(fields, s.getFields())
       self.assertEqual(0, s.getNextRecordIdx())
 
-      print 'Writing records ...'
+      print('Writing records ...')
       for r in records:
-        print list(r)
+        print(list(r))
         s.appendRecord(list(r))
 
       self.assertEqual(3, s.getDataRowCount())
@@ -94,9 +94,9 @@
         ['rec_6', datetime(day=6, month=3, year=2010), 11, 11.5, 0, 'seq-1', 15]
       )
 
-      print 'Adding batch of records...'
+      print('Adding batch of records...')
       for rec in recordsBatch:
-        print rec
+        print(rec)
       s.appendRecords(recordsBatch)
       self.assertEqual(6, s.getDataRowCount())
 
@@ -110,7 +110,7 @@
       self.assertEqual(0, s.getNextRecordIdx())
 
       readStats = s.getStats()
-      print 'Got stats:', readStats
+      print('Got stats:', readStats)
       expectedStats = {
                        'max': [None, None, 12, 11.5, 1, None, 15],
                        'min': [None, None, 2, 6.5, 0, None, 10]
@@ -118,10 +118,10 @@
       self.assertEqual(expectedStats, readStats)
 
       readRecords = []
-      print 'Reading records ...'
+      print('Reading records ...')
       while True:
         r = s.getNextRecord()
-        print r
+        print(r)
         if r is None:
           break
 
@@ -155,7 +155,7 @@
     fieldNames = ['name', 'timestamp', 'integer', 'real', 'reset', 'sid',
                   'categories']
 
-    print 'Creating temp file:', filename
+    print('Creating temp file:', filename)
 
     with FileRecordStream(streamID=filename, write=True, fields=fields) as s:
 
@@ -173,9 +173,9 @@
       self.assertEqual(fields, s.getFields())
       self.assertEqual(0, s.getNextRecordIdx())
 
-      print 'Writing records ...'
+      print('Writing records ...')
       for r in records:
-        print r
+        print(r)
         s.appendRecord(r)
 
       self.assertEqual(3, s.getDataRowCount())
@@ -188,9 +188,9 @@
         ['rec_6', datetime(day=6, month=3, year=2010), 11, 11.5, 0, 'seq-1',
          [4, 5, 6]])
 
-      print 'Adding batch of records...'
+      print('Adding batch of records...')
       for rec in recordsBatch:
-        print rec
+        print(rec)
       s.appendRecords(recordsBatch)
       self.assertEqual(6, s.getDataRowCount())
 
@@ -204,7 +204,7 @@
       self.assertEqual(0, s.getNextRecordIdx())
 
       readStats = s.getStats()
-      print 'Got stats:', readStats
+      print('Got stats:', readStats)
       expectedStats = {
                        'max': [None, None, 11, 11.5, 1, None, None],
                        'min': [None, None, 2, 6.5, 0, None, None]
@@ -212,10 +212,10 @@
       self.assertEqual(expectedStats, readStats)
 
       readRecords = []
-      print 'Reading records ...'
+      print('Reading records ...')
       while True:
         r = s.getNextRecord()
-        print r
+        print(r)
         if r is None:
           break
 
@@ -258,7 +258,7 @@
 
     filename = _getTempFileName()
 
-    print 'Creating tempfile:', filename
+    print('Creating tempfile:', filename)
 
     # Write bad dataset with records going backwards in time
     fields = [FieldMetaInfo('timestamp', FieldMetaType.datetime,
@@ -288,14 +288,14 @@
 
   def testMissingValues(self):
 
-    print "Beginning Missing Data test..."
+    print("Beginning Missing Data test...")
     filename = _getTempFileName()
 
     # Some values missing of each type
     # read dataset from disk, retrieve values
     # string should return empty string, numeric types sentinelValue
 
-    print 'Creating tempfile:', filename
+    print('Creating tempfile:', filename)
 
     # write dataset to disk with float, int, and string fields
     fields = [FieldMetaInfo('timestamp', FieldMetaType.datetime,
@@ -334,8 +334,8 @@
       r = s.getNextRecord()
       if r is None:
         break
-      print 'Reading record ...'
-      print r
+      print('Reading record ...')
+      print(r)
       recordsRead.append(r)
 
     # sort the records by date, so we know for sure which is which
--- d:\nupic\src\python\python27\tests\unit\nupic\data\generators\anomalyzer_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\data\generators\anomalyzer_test.py	(refactored)
@@ -23,7 +23,7 @@
 
 import csv
 from mock import MagicMock, patch
-from StringIO import StringIO
+from io import StringIO
 
 import unittest2 as unittest
 
@@ -334,13 +334,13 @@
                                       write=True)
         anomalyzer.sample(inputFile, outputFile, 1)
     result = StringIO(output.getvalue())
-    result.next()
-    result.next()
-    result.next()
+    next(result)
+    next(result)
+    next(result)
     reader = csv.reader(result)
-    _, value = reader.next()
+    _, value = next(reader)
     self.assertIn(int(value), (1, 2, 3, 4, 5, 6))
-    self.assertRaises(StopIteration, result.next)
+    self.assertRaises(StopIteration, result.__next__)
 
 
 
--- d:\nupic\src\python\python27\tests\unit\nupic\data\generators\pattern_machine_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\data\generators\pattern_machine_test.py	(refactored)
@@ -77,7 +77,7 @@
     pattern = self.patternMachine.get(49)
     numberMap = self.patternMachine.numberMapForBits(pattern)
 
-    self.assertEqual(numberMap.keys(), [49])
+    self.assertEqual(list(numberMap.keys()), [49])
     self.assertEqual(numberMap[49], pattern)
 
 
--- d:\nupic\src\python\python27\tests\unit\nupic\data\generators\sequence_machine_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\data\generators\sequence_machine_test.py	(refactored)
@@ -35,7 +35,7 @@
 
 
   def testGenerateFromNumbers(self):
-    numbers = range(0, 10) + [None] + range(10, 19)
+    numbers = list(range(0, 10)) + [None] + list(range(10, 19))
     sequence = self.sequenceMachine.generateFromNumbers(numbers)
     self.assertEqual(len(sequence), 20)
     self.assertEqual(sequence[0], self.patternMachine.get(0))
@@ -46,7 +46,7 @@
   def testAddSpatialNoise(self):
     patternMachine = PatternMachine(10000, 1000, num=100)
     sequenceMachine = SequenceMachine(patternMachine)
-    numbers = range(0, 100)
+    numbers = list(range(0, 100))
     numbers.append(None)
 
     sequence = sequenceMachine.generateFromNumbers(numbers)
@@ -66,24 +66,24 @@
     numbers = self.sequenceMachine.generateNumbers(1, 100)
     self.assertEqual(numbers[-1], None)
     self.assertEqual(len(numbers), 101)
-    self.assertFalse(numbers[:-1] == range(0, 100))
-    self.assertEqual(sorted(numbers[:-1]), range(0, 100))
+    self.assertFalse(numbers[:-1] == list(range(0, 100)))
+    self.assertEqual(sorted(numbers[:-1]), list(range(0, 100)))
 
 
   def testGenerateNumbersMultipleSequences(self):
     numbers = self.sequenceMachine.generateNumbers(3, 100)
     self.assertEqual(len(numbers), 303)
 
-    self.assertEqual(sorted(numbers[0:100]), range(0, 100))
-    self.assertEqual(sorted(numbers[101:201]), range(100, 200))
-    self.assertEqual(sorted(numbers[202:302]), range(200, 300))
+    self.assertEqual(sorted(numbers[0:100]), list(range(0, 100)))
+    self.assertEqual(sorted(numbers[101:201]), list(range(100, 200)))
+    self.assertEqual(sorted(numbers[202:302]), list(range(200, 300)))
 
 
   def testGenerateNumbersWithShared(self):
     numbers = self.sequenceMachine.generateNumbers(3, 100, (20, 35))
     self.assertEqual(len(numbers), 303)
 
-    shared = range(300, 315)
+    shared = list(range(300, 315))
     self.assertEqual(numbers[20:35], shared)
     self.assertEqual(numbers[20+101:35+101], shared)
     self.assertEqual(numbers[20+202:35+202], shared)
--- d:\nupic\src\python\python27\tests\unit\nupic\docs\examples_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\docs\examples_test.py	(refactored)
@@ -95,9 +95,9 @@
   def testNumberOfOneStepPredictions(self):
     """Make sure all examples output the same number of oneStepPredictions."""
 
-    self.assertEquals(len(ExamplesTest.oneStepPredictions["opf"]),
+    self.assertEqual(len(ExamplesTest.oneStepPredictions["opf"]),
                       len(ExamplesTest.oneStepPredictions["algo"]))
-    self.assertEquals(len(ExamplesTest.oneStepPredictions["opf"]),
+    self.assertEqual(len(ExamplesTest.oneStepPredictions["opf"]),
                       len(ExamplesTest.oneStepPredictions["network"]))
 
   @unittest.expectedFailure
--- d:\nupic\src\python\python27\tests\unit\nupic\encoders\adaptivescalar_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\encoders\adaptivescalar_test.py	(refactored)
@@ -86,7 +86,7 @@
       (fieldsDict, _) = decoded
       self.assertEqual(len(fieldsDict), 1)
 
-      (ranges, _) = fieldsDict.values()[0]
+      (ranges, _) = list(fieldsDict.values())[0]
       self.assertEqual(len(ranges), 1)
 
       (rangeMin, rangeMax) = ranges[0]
@@ -115,14 +115,14 @@
     (fieldsDict, _) = decoded
     self.assertEqual(len(fieldsDict), 1)
 
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertSequenceEqual(ranges[0], [10, 10])
 
     decoded = l.decode(numpy.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]))
     (fieldsDict, _) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertSequenceEqual(ranges[0], [10, 10])
 
--- d:\nupic\src\python\python27\tests\unit\nupic\encoders\category_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\encoders\category_test.py	(refactored)
@@ -58,8 +58,8 @@
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldNames), 1)
     self.assertEqual(len(fieldsDict), 1)
-    self.assertEqual(fieldNames[0], fieldsDict.keys()[0])
-    (ranges, desc) = fieldsDict.values()[0]
+    self.assertEqual(fieldNames[0], list(fieldsDict.keys())[0])
+    (ranges, desc) = list(fieldsDict.values())[0]
     self.assertEqual(desc, "US")
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [3, 3]))
@@ -91,8 +91,8 @@
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldNames), 1)
     self.assertEqual(len(fieldsDict), 1)
-    self.assertEqual(fieldNames[0], fieldsDict.keys()[0])
-    (ranges, desc) = fieldsDict.values()[0]
+    self.assertEqual(fieldNames[0], list(fieldsDict.keys())[0])
+    (ranges, desc) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [0, 0]))
 
@@ -117,8 +117,8 @@
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldNames), 1)
     self.assertEqual(len(fieldsDict), 1)
-    self.assertEqual(fieldNames[0], fieldsDict.keys()[0])
-    (ranges, desc) = fieldsDict.values()[0]
+    self.assertEqual(fieldNames[0], list(fieldsDict.keys())[0])
+    (ranges, desc) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [1, 1]))
 
@@ -137,8 +137,8 @@
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldNames), 1)
     self.assertEqual(len(fieldsDict), 1)
-    self.assertEqual(fieldNames[0], fieldsDict.keys()[0])
-    (ranges, desc) = fieldsDict.values()[0]
+    self.assertEqual(fieldNames[0], list(fieldsDict.keys())[0])
+    (ranges, desc) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [0, 3]))
 
--- d:\nupic\src\python\python27\tests\unit\nupic\encoders\coordinate_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\encoders\coordinate_test.py	(refactored)
@@ -271,11 +271,11 @@
     self.assertGreater(np.average(allOverlaps), avgThreshold)
 
     if verbose:
-      print ("===== Adjacent positions overlap "
-             "(n = {0}, w = {1}, radius = {2}) ===").format(n, w, radius)
-      print "Max: {0}".format(np.max(allOverlaps))
-      print "Min: {0}".format(np.min(allOverlaps))
-      print "Average: {0}".format(np.average(allOverlaps))
+      print(("===== Adjacent positions overlap "
+             "(n = {0}, w = {1}, radius = {2}) ===").format(n, w, radius))
+      print("Max: {0}".format(np.max(allOverlaps)))
+      print("Min: {0}".format(np.min(allOverlaps)))
+      print("Average: {0}".format(np.average(allOverlaps)))
 
 
   def assertDecreasingOverlaps(self, overlaps):
@@ -351,13 +351,13 @@
     overlaps[i] = overlap(outputA, outputB)
 
   if verbose:
-    print
-    print ("===== Relative encoding overlaps (n = {0}, w = {1}, "
+    print()
+    print(("===== Relative encoding overlaps (n = {0}, w = {1}, "
                            "initPosition = {2}, initRadius = {3}, "
                            "dPosition = {4}, dRadius = {5}) =====").format(
-      n, w, initPosition, initRadius, dPosition, dRadius)
-    print "Average: {0}".format(np.average(overlaps))
-    print "Max: {0}".format(np.max(overlaps))
+      n, w, initPosition, initRadius, dPosition, dRadius))
+    print("Average: {0}".format(np.average(overlaps)))
+    print("Max: {0}".format(np.max(overlaps)))
 
   return overlaps
 
--- d:\nupic\src\python\python27\tests\unit\nupic\encoders\logenc_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\encoders\logenc_test.py	(refactored)
@@ -102,7 +102,7 @@
     decoded = le.decode(output)
     (fieldsDict, _) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [1, 1]))
 
@@ -166,7 +166,7 @@
     decoded = le.decode(output)
     (fieldsDict, _) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [100, 100]))
 
@@ -180,7 +180,7 @@
     decoded = le.decode(output)
     (fieldsDict, _) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [10000, 10000]))
 
--- d:\nupic\src\python\python27\tests\unit\nupic\encoders\random_distributed_scalar_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\encoders\random_distributed_scalar_test.py	(refactored)
@@ -19,7 +19,7 @@
 # http://numenta.org/licenses/
 # ----------------------------------------------------------------------
 
-from cStringIO import StringIO
+from io import StringIO
 import sys
 import tempfile
 import unittest2 as unittest
@@ -316,12 +316,12 @@
     encoder = RandomDistributedScalarEncoder(name="encoder", resolution=1.0,
                                              w=5, n=5*20)
     midIdx = encoder._maxBuckets/2
-    encoder.bucketMap[midIdx-2] = numpy.array(range(3, 8))
-    encoder.bucketMap[midIdx-1] = numpy.array(range(4, 9))
-    encoder.bucketMap[midIdx]   = numpy.array(range(5, 10))
-    encoder.bucketMap[midIdx+1] = numpy.array(range(6, 11))
-    encoder.bucketMap[midIdx+2] = numpy.array(range(7, 12))
-    encoder.bucketMap[midIdx+3] = numpy.array(range(8, 13))
+    encoder.bucketMap[midIdx-2] = numpy.array(list(range(3, 8)))
+    encoder.bucketMap[midIdx-1] = numpy.array(list(range(4, 9)))
+    encoder.bucketMap[midIdx]   = numpy.array(list(range(5, 10)))
+    encoder.bucketMap[midIdx+1] = numpy.array(list(range(6, 11)))
+    encoder.bucketMap[midIdx+2] = numpy.array(list(range(7, 12)))
+    encoder.bucketMap[midIdx+3] = numpy.array(list(range(8, 13)))
     encoder.minIndex = midIdx - 2
     encoder.maxIndex = midIdx + 3
 
@@ -350,14 +350,14 @@
     encoder = RandomDistributedScalarEncoder(name="encoder", resolution=1.0,
                                              w=5, n=5*20)
     midIdx = encoder._maxBuckets/2
-    encoder.bucketMap[midIdx-3] = numpy.array(range(4, 9)) # Not ok with
+    encoder.bucketMap[midIdx-3] = numpy.array(list(range(4, 9))) # Not ok with
                                                            # midIdx-1
-    encoder.bucketMap[midIdx-2] = numpy.array(range(3, 8))
-    encoder.bucketMap[midIdx-1] = numpy.array(range(4, 9))
-    encoder.bucketMap[midIdx]   = numpy.array(range(5, 10))
-    encoder.bucketMap[midIdx+1] = numpy.array(range(6, 11))
-    encoder.bucketMap[midIdx+2] = numpy.array(range(7, 12))
-    encoder.bucketMap[midIdx+3] = numpy.array(range(8, 13))
+    encoder.bucketMap[midIdx-2] = numpy.array(list(range(3, 8)))
+    encoder.bucketMap[midIdx-1] = numpy.array(list(range(4, 9)))
+    encoder.bucketMap[midIdx]   = numpy.array(list(range(5, 10)))
+    encoder.bucketMap[midIdx+1] = numpy.array(list(range(6, 11)))
+    encoder.bucketMap[midIdx+2] = numpy.array(list(range(7, 12)))
+    encoder.bucketMap[midIdx+3] = numpy.array(list(range(8, 13)))
     encoder.minIndex = midIdx - 3
     encoder.maxIndex = midIdx + 3
 
@@ -488,7 +488,7 @@
                      encoder.decode(encodedFromOriginal))
     self.assertEqual(original.random.getSeed(), encoder.random.getSeed())
 
-    for key, value in original.bucketMap.items():
+    for key, value in list(original.bucketMap.items()):
       self.assertTrue(numpy.array_equal(value, encoder.bucketMap[key]))
 
 
--- d:\nupic\src\python\python27\tests\unit\nupic\encoders\scalar_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\encoders\scalar_test.py	(refactored)
@@ -144,8 +144,8 @@
       (fieldsDict, fieldNames) = decoded
       self.assertEqual(len(fieldsDict), 1)
       self.assertEqual(len(fieldNames), 1)
-      self.assertEqual(fieldNames, fieldsDict.keys())
-      (ranges, _) = fieldsDict.values()[0]
+      self.assertEqual(fieldNames, list(fieldsDict.keys()))
+      (ranges, _) = list(fieldsDict.values())[0]
       self.assertEqual(len(ranges), 1)
       (rangeMin, rangeMax) = ranges[0]
       self.assertEqual(rangeMin, rangeMax)
@@ -176,7 +176,7 @@
     decoded = l.decode(numpy.array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [7.5, 7.5]))
 
@@ -184,7 +184,7 @@
     decoded = l.decode(numpy.array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]))
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 2)
     self.assertTrue(numpy.array_equal(ranges[0], [7.5, 8]))
     self.assertTrue(numpy.array_equal(ranges[1], [1, 1]))
@@ -193,7 +193,7 @@
     decoded = l.decode(numpy.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [1.5, 2.5]))
 
@@ -201,7 +201,7 @@
     decoded = l.decode(numpy.array([1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]))
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 2)
     self.assertTrue(numpy.array_equal(ranges[0], [1.5, 1.5]))
     self.assertTrue(numpy.array_equal(ranges[1], [5.5, 6.0]))
@@ -210,7 +210,7 @@
     decoded = l.decode(numpy.array([0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0]))
     (fieldsDict, fieldNames) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertTrue(len(ranges), 2)
     self.assertTrue(numpy.array_equal(ranges[0], [1.5, 1.5]))
     self.assertTrue(numpy.array_equal(ranges[1], [5.5, 6.0]))
@@ -221,7 +221,7 @@
     encoder = ScalarEncoder(w=7, minval=0, maxval=7, radius=1, periodic=True,
                             name="day of week", forced=True)
     scores = encoder.closenessScores((2, 4, 7), (4, 2, 1), fractional=False)
-    for actual, score in itertools.izip((2, 2, 1), scores):
+    for actual, score in zip((2, 2, 1), scores):
       self.assertEqual(actual, score)
 
 
@@ -265,7 +265,7 @@
 
       (fieldsDict, _) = decoded
       self.assertEqual(len(fieldsDict), 1)
-      (ranges, _) = fieldsDict.values()[0]
+      (ranges, _) = list(fieldsDict.values())[0]
       self.assertEqual(len(ranges), 1)
       (rangeMin, rangeMax) = ranges[0]
       self.assertEqual(rangeMin, rangeMax)
@@ -290,14 +290,14 @@
     decoded = l.decode(numpy.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1]))
     (fieldsDict, _) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [10, 10]))
 
     decoded = l.decode(numpy.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1]))
     (fieldsDict, _) = decoded
     self.assertEqual(len(fieldsDict), 1)
-    (ranges, _) = fieldsDict.values()[0]
+    (ranges, _) = list(fieldsDict.values())[0]
     self.assertEqual(len(ranges), 1)
     self.assertTrue(numpy.array_equal(ranges[0], [10, 10]))
 
@@ -338,7 +338,7 @@
 
       (fieldsDict, _) = decoded
       self.assertEqual(len(fieldsDict), 1)
-      (ranges, _) = fieldsDict.values()[0]
+      (ranges, _) = list(fieldsDict.values())[0]
       self.assertEqual(len(ranges), 1)
       (rangeMin, rangeMax) = ranges[0]
       self.assertEqual(rangeMin, rangeMax)
@@ -361,7 +361,7 @@
 
       (fieldsDict, _) = decoded
       self.assertEqual(len(fieldsDict), 1)
-      (ranges, _) = fieldsDict.values()[0]
+      (ranges, _) = list(fieldsDict.values())[0]
       self.assertEqual(len(ranges), 1)
       (rangeMin, rangeMax) = ranges[0]
       self.assertEqual(rangeMin, rangeMax)
--- d:\nupic\src\python\python27\tests\unit\nupic\encoders\sdrcategory_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\encoders\sdrcategory_test.py	(refactored)
@@ -140,7 +140,7 @@
 
     # serialization
     # TODO: Remove pickle-based serialization tests -- issues #1419 and #1420
-    import cPickle as pickle
+    import pickle as pickle
     t = pickle.loads(pickle.dumps(s))
     self.assertTrue((t.encode("ES") == es).all())
     self.assertTrue((t.encode("GB") == s.encode("GB")).all())
@@ -191,7 +191,7 @@
       s = SDRCategoryEncoder(n=fieldWidth, w=bitsOn,
                              categoryList=newcategories, name="foo",
                              forced=True)
-    except RuntimeError, e:
+    except RuntimeError as e:
       caughtException = True
     finally:
       if not caughtException:
--- d:\nupic\src\python\python27\tests\unit\nupic\engine\network_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\engine\network_test.py	(refactored)
@@ -95,29 +95,29 @@
   def testOneRegionNetwork(self):
     n = engine.Network()
 
-    print "Number of regions in new network: %d" % len(n.regions)
+    print("Number of regions in new network: %d" % len(n.regions))
     self.assertEqual(len(n.regions), 0)
 
-    print "Adding level1SP"
+    print("Adding level1SP")
     level1SP = n.addRegion("level1SP", "TestNode", "")
-    print "Current dimensions are: %s" % level1SP.dimensions
-    print "Number of regions in network: %d" % len(n.regions)
+    print("Current dimensions are: %s" % level1SP.dimensions)
+    print("Number of regions in network: %d" % len(n.regions))
 
     self.assertEqual(len(n.regions), 1)
     self.assertEqual(len(n.regions), len(n.regions))
 
-    print 'Node type: ', level1SP.type
+    print('Node type: ', level1SP.type)
 
     print("Attempting to initialize net when "
            "one region has unspecified dimensions")
-    print "Current dimensions are: %s" % level1SP.dimensions
+    print("Current dimensions are: %s" % level1SP.dimensions)
 
     with self.assertRaises(Exception):
       n.initialize()
 
     # Test Dimensions
     level1SP.dimensions = engine.Dimensions([4, 4])
-    print "Set dimensions of level1SP to %s" % str(level1SP.dimensions)
+    print("Set dimensions of level1SP to %s" % str(level1SP.dimensions))
 
     n.initialize()
 
@@ -134,72 +134,72 @@
     for i in range(len(a)):
       self.assertEqual(type(a[i]), int)
       self.assertEqual(a[i], i)
-      print i,
-    print
+      print(i, end=' ')
+    print()
 
     # --- Test Numpy Array
-    print 'Testing Numpy Array'
+    print('Testing Numpy Array')
     a = engine.Array('Byte', 15)
-    print len(a)
+    print(len(a))
     for i in range(len(a)):
       a[i] = ord('A') + i
 
     for i in range(len(a)):
-      print a[i], ord('A') + i
+      print(a[i], ord('A') + i)
       self.assertEqual(ord(a[i]), ord('A') + i)
-    print
-
-    print 'before asNumpyarray()'
+    print()
+
+    print('before asNumpyarray()')
     na = a.asNumpyArray()
-    print 'after asNumpyarray()'
+    print('after asNumpyarray()')
 
     self.assertEqual(na.shape, (15,))
-    print 'na.shape:', na.shape
+    print('na.shape:', na.shape)
     na = na.reshape(5, 3)
     self.assertEqual(na.shape, (5, 3))
-    print 'na.shape:', na.shape
+    print('na.shape:', na.shape)
     for i in range(5):
       for j in range(3):
-        print chr(na[i, j]), ' ',
-      print
-    print
+        print(chr(na[i, j]), ' ', end=' ')
+      print()
+    print()
 
 
     # --- Test get/setParameter for Int64 and Real64
-    print '---'
-    print 'Testing get/setParameter for Int64/Real64'
+    print('---')
+    print('Testing get/setParameter for Int64/Real64')
     val = level1SP.getParameterInt64('int64Param')
     rval = level1SP.getParameterReal64('real64Param')
-    print 'level1SP.int64Param = ', val
-    print 'level1SP.real64Param = ', rval
+    print('level1SP.int64Param = ', val)
+    print('level1SP.real64Param = ', rval)
 
     val = 20
     level1SP.setParameterInt64('int64Param', val)
     val = 0
     val = level1SP.getParameterInt64('int64Param')
-    print 'level1SP.int64Param = ', val, ' after setting to 20'
+    print('level1SP.int64Param = ', val, ' after setting to 20')
 
     rval = 30.1
     level1SP.setParameterReal64('real64Param', rval)
     rval = 0.0
     rval = level1SP.getParameterReal64('real64Param')
-    print 'level1SP.real64Param = ', rval, ' after setting to 30.1'
+    print('level1SP.real64Param = ', rval, ' after setting to 30.1')
 
     # --- Test array parameter
     # Array a will be allocated inside getParameter
-    print '---'
-    print 'Testing get/setParameterArray'
+    print('---')
+    print('Testing get/setParameterArray')
     a = engine.Array('Int64', 4)
     level1SP.getParameterArray("int64ArrayParam", a)
-    print 'level1SP.int64ArrayParam size = ', len(a)
-    print 'level1SP.int64ArrayParam = [ ',
-    for i in range(len(a)):
-      print a[i],
-
-    print ']'
+    print('level1SP.int64ArrayParam size = ', len(a))
+    print('level1SP.int64ArrayParam = [ ', end=' ')
+    for i in range(len(a)):
+      print(a[i], end=' ')
+
+    print(']')
     #
     # --- test setParameter of an Int64 Array ---
-    print 'Setting level1SP.int64ArrayParam to [ 1 2 3 4 ]'
+    print('Setting level1SP.int64ArrayParam to [ 1 2 3 4 ]')
     a2 = engine.Array('Int64', 4)
     for i in range(4):
       a2[i] = i + 1
@@ -211,15 +211,15 @@
     # want, but the buffer should be reused if we just pass it again.
     #// a.releaseBuffer();
     level1SP.getParameterArray('int64ArrayParam', a)
-    print 'level1SP.int64ArrayParam size = ', len(a)
-    print 'level1SP.int64ArrayParam = [ ',
-    for i in range(len(a)):
-      print a[i],
-    print ']'
+    print('level1SP.int64ArrayParam size = ', len(a))
+    print('level1SP.int64ArrayParam = [ ', end=' ')
+    for i in range(len(a)):
+      print(a[i], end=' ')
+    print(']')
 
     level1SP.compute()
 
-    print "Running for 2 iteraitons"
+    print("Running for 2 iteraitons")
     n.run(2)
 
 
@@ -230,7 +230,7 @@
       level1SP.getOutputData('doesnotexist')
 
     output = level1SP.getOutputData('bottomUpOut')
-    print 'Element count in bottomUpOut is ', len(output)
+    print('Element count in bottomUpOut is ', len(output))
     # set the actual output
     output[11] = 7777
     output[12] = 54321
@@ -263,7 +263,7 @@
     #     print 'method Network.{0}(): "{1}"'.format(name, x.__doc__)
 
     # Typed methods should return correct type
-    print "real64Param: %.2f" % level1SP.getParameterReal64("real64Param")
+    print("real64Param: %.2f" % level1SP.getParameterReal64("real64Param"))
 
     # Uncomment to get performance for getParameter
 
@@ -271,12 +271,12 @@
       import time
       t1 = time.time()
       t1 = time.time()
-      for i in xrange(0, 1000000):
+      for i in range(0, 1000000):
         # x = level1SP.getParameterInt64("int64Param")   # buffered
         x = level1SP.getParameterReal64("real64Param")   # unbuffered
       t2 = time.time()
 
-      print "Time for 1M getParameter calls: %.2f seconds" % (t2 - t1)
+      print("Time for 1M getParameter calls: %.2f seconds" % (t2 - t1))
 
 
   def testTwoRegionNetwork(self):
@@ -287,21 +287,21 @@
 
     names = [region[0] for region in n.regions]
     self.assertEqual(names, ['region1', 'region2'])
-    print n.getPhases('region1')
+    print(n.getPhases('region1'))
     self.assertEqual(n.getPhases('region1'), (0,))
     self.assertEqual(n.getPhases('region2'), (1,))
 
     n.link("region1", "region2", "TestFanIn2", "")
 
-    print "Initialize should fail..."
+    print("Initialize should fail...")
     with self.assertRaises(Exception):
       n.initialize()
 
-    print "Setting region1 dims"
+    print("Setting region1 dims")
     r1dims = engine.Dimensions([6, 4])
     region1.setDimensions(r1dims)
 
-    print "Initialize should now succeed"
+    print("Initialize should now succeed")
     n.initialize()
 
     r2dims = region2.dimensions
@@ -335,7 +335,7 @@
     inputArrays = []
 
     iterations = propagationDelay + 2
-    for i in xrange(iterations):
+    for i in range(iterations):
       n.run(1)
 
       if i < iterations - propagationDelay:
@@ -362,20 +362,20 @@
     r1_output = region1.getOutputData("bottomUpOut")
 
     region1.compute()
-    print "Region 1 output after first iteration:"
-    print "r1_output:", r1_output
+    print("Region 1 output after first iteration:")
+    print("r1_output:", r1_output)
 
     region2.prepareInputs()
     r2_input = region2.getInputData("bottomUpIn")
-    print "Region 2 input after first iteration:"
-    print 'r2_input:', r2_input
+    print("Region 2 input after first iteration:")
+    print('r2_input:', r2_input)
 
 
   def testNodeSpec(self):
     n = engine.Network()
     r = n.addRegion("region", "TestNode", "")
 
-    print r.getSpec()
+    print(r.getSpec())
 
 
   @unittest.skipIf(sys.platform.lower().startswith("win"),
@@ -385,10 +385,10 @@
 
     r = n.addRegion("region", "py.TestNode", "")
 
-    print "Setting region1 dims"
+    print("Setting region1 dims")
     r.dimensions = engine.Dimensions([6, 4])
 
-    print "Initialize should now succeed"
+    print("Initialize should now succeed")
     n.initialize()
 
     result = r.getParameterReal64('real64Param')
@@ -407,10 +407,10 @@
 
     r = n.addRegion("region", "py.TestNode", "")
 
-    print "Setting region1 dims"
+    print("Setting region1 dims")
     r.setDimensions(engine.Dimensions([6, 4]))
 
-    print "Initialize should now succeed"
+    print("Initialize should now succeed")
     n.initialize()
 
     ns = r.spec
@@ -434,15 +434,15 @@
 
     n.link("region1", "region2", "TestFanIn2", "")
 
-    print "Initialize should fail..."
+    print("Initialize should fail...")
     with self.assertRaises(Exception):
       n.initialize()
 
-    print "Setting region1 dims"
+    print("Setting region1 dims")
     r1dims = engine.Dimensions([6, 4])
     region1.setDimensions(r1dims)
 
-    print "Initialize should now succeed"
+    print("Initialize should now succeed")
     n.initialize()
 
     r2dims = region2.dimensions
--- d:\nupic\src\python\python27\tests\unit\nupic\engine\syntactic_sugar_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\engine\syntactic_sugar_test.py	(refactored)
@@ -92,18 +92,18 @@
         self.fail("Expected i == 0 or i == 1")
 
     # test .keys()
-    keys = regions.keys()
+    keys = list(regions.keys())
     self.assertEqual(keys, list(['r1', 'r2']))
 
     # test .values()
-    values = regions.values()
+    values = list(regions.values())
     self.assertEqual(len(values), 2)
     v1 = values.pop()
     v2 = values.pop()
     self.assertTrue((v1, v2) == (r1, r2) or (v1, v2) == (r2, r1))
 
     # test .items()
-    items = regions.items()
+    items = list(regions.items())
     self.assertEqual(len(items), 2)
     i1 = items.pop()
     i2 = items.pop()
@@ -116,7 +116,7 @@
   def testRegion(self):
     r = net.Network().addRegion('r', 'py.TestNode', '')
 
-    print r.spec
+    print(r.spec)
     self.assertEqual(r.type, 'py.TestNode')
     self.assertEqual(r.name, 'r')
     self.assertTrue(r.dimensions.isUnspecified())
@@ -149,7 +149,7 @@
     t.start()
     # Dummy time
     _j = 0
-    for i in xrange(0, 1000):
+    for i in range(0, 1000):
       _j = i
     t.stop()
     self.assertTrue(t.elapsed > 0)
--- d:\nupic\src\python\python27\tests\unit\nupic\engine\unified_py_parameter_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\engine\unified_py_parameter_test.py	(refactored)
@@ -41,8 +41,8 @@
     scalars = [
       ("int32Param", 32, int, 35),
       ("uint32Param", 33, int, 36),
-      ("int64Param", 64, long, 74),
-      ("uint64Param", 65, long, 75),
+      ("int64Param", 64, int, 74),
+      ("uint64Param", 65, int, 75),
       ("real32Param", 32.1, float, 33.1),
       ("real64Param", 64.1, float, 65.1),
       ("stringParam", "nodespec value", str, "new value")]
@@ -90,10 +90,10 @@
       self.assertTrue(isinstance(x, nupic.bindings.engine_internal.Array))
       self.assertEqual(x.getType(), paramtype)
       self.assertEqual(len(x), len(initval))
-      for i in xrange(len(x)):
+      for i in range(len(x)):
         self.assertEqual(x[i], initval[i])
 
-      for i in xrange(len(x)):
+      for i in range(len(x)):
         x[i] = x[i] * 2
       l1.setParameter(paramName, x)
 
@@ -101,7 +101,7 @@
       self.assertTrue(isinstance(x, nupic.bindings.engine_internal.Array))
       self.assertEqual(x.getType(), paramtype)
       self.assertEqual(len(x), len(initval))
-      for i in xrange(len(x)):
+      for i in range(len(x)):
         self.assertEqual(x[i], 2 * initval[i])
 
 
--- d:\nupic\src\python\python27\tests\unit\nupic\frameworks\opf\htmpredictionmodel_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\frameworks\opf\htmpredictionmodel_test.py	(refactored)
@@ -82,81 +82,81 @@
     inferences
     """
     modelConfig = (
-      {u'aggregationInfo': {u'days': 0,
-                            u'fields': [],
-                            u'hours': 0,
-                            u'microseconds': 0,
-                            u'milliseconds': 0,
-                            u'minutes': 0,
-                            u'months': 0,
-                            u'seconds': 0,
-                            u'weeks': 0,
-                            u'years': 0},
-       u'model': u'HTMPrediction',
-       u'modelParams': {u'anomalyParams': {u'anomalyCacheRecords': None,
-                                           u'autoDetectThreshold': None,
-                                           u'autoDetectWaitRecords': 5030},
-                        u'clEnable': False,
-                        u'clParams': {u'alpha': 0.035828933612158,
-                                      u'verbosity': 0,
-                                      u'regionName': u'SDRClassifierRegion',
-                                      u'steps': u'1'},
-                        u'inferenceType': u'TemporalAnomaly',
-                        u'sensorParams': {u'encoders': {u'c0_dayOfWeek': None,
-                                                        u'c0_timeOfDay': {u'fieldname': u'c0',
-                                                                          u'name': u'c0',
-                                                                          u'timeOfDay': [21,
+      {'aggregationInfo': {'days': 0,
+                            'fields': [],
+                            'hours': 0,
+                            'microseconds': 0,
+                            'milliseconds': 0,
+                            'minutes': 0,
+                            'months': 0,
+                            'seconds': 0,
+                            'weeks': 0,
+                            'years': 0},
+       'model': 'HTMPrediction',
+       'modelParams': {'anomalyParams': {'anomalyCacheRecords': None,
+                                           'autoDetectThreshold': None,
+                                           'autoDetectWaitRecords': 5030},
+                        'clEnable': False,
+                        'clParams': {'alpha': 0.035828933612158,
+                                      'verbosity': 0,
+                                      'regionName': 'SDRClassifierRegion',
+                                      'steps': '1'},
+                        'inferenceType': 'TemporalAnomaly',
+                        'sensorParams': {'encoders': {'c0_dayOfWeek': None,
+                                                        'c0_timeOfDay': {'fieldname': 'c0',
+                                                                          'name': 'c0',
+                                                                          'timeOfDay': [21,
                                                                                          9.49122334747737],
-                                                                          u'type': u'DateEncoder'},
-                                                        u'c0_weekend': None,
-                                                        u'c1': {u'fieldname': u'c1',
-                                                                u'name': u'c1',
-                                                                u'resolution': 0.8771929824561403,
-                                                                u'seed': 42,
-                                                                u'type': u'RandomDistributedScalarEncoder'}},
-                                          u'sensorAutoReset': None,
-                                          u'verbosity': 0},
-                        u'spEnable': True,
-                        u'spParams': {u'potentialPct': 0.8,
-                                      u'columnCount': 2048,
-                                      u'globalInhibition': 1,
-                                      u'inputWidth': 0,
-                                      u'boostStrength': 0.0,
-                                      u'numActiveColumnsPerInhArea': 40,
-                                      u'seed': 1956,
-                                      u'spVerbosity': 0,
-                                      u'spatialImp': u'cpp',
-                                      u'synPermActiveInc': 0.0015,
-                                      u'synPermConnected': 0.1,
-                                      u'synPermInactiveDec': 0.0005,
+                                                                          'type': 'DateEncoder'},
+                                                        'c0_weekend': None,
+                                                        'c1': {'fieldname': 'c1',
+                                                                'name': 'c1',
+                                                                'resolution': 0.8771929824561403,
+                                                                'seed': 42,
+                                                                'type': 'RandomDistributedScalarEncoder'}},
+                                          'sensorAutoReset': None,
+                                          'verbosity': 0},
+                        'spEnable': True,
+                        'spParams': {'potentialPct': 0.8,
+                                      'columnCount': 2048,
+                                      'globalInhibition': 1,
+                                      'inputWidth': 0,
+                                      'boostStrength': 0.0,
+                                      'numActiveColumnsPerInhArea': 40,
+                                      'seed': 1956,
+                                      'spVerbosity': 0,
+                                      'spatialImp': 'cpp',
+                                      'synPermActiveInc': 0.0015,
+                                      'synPermConnected': 0.1,
+                                      'synPermInactiveDec': 0.0005,
                                       },
-                        u'tmEnable': True,
-                        u'tmParams': {u'activationThreshold': 13,
-                                      u'cellsPerColumn': 32,
-                                      u'columnCount': 2048,
-                                      u'globalDecay': 0.0,
-                                      u'initialPerm': 0.21,
-                                      u'inputWidth': 2048,
-                                      u'maxAge': 0,
-                                      u'maxSegmentsPerCell': 128,
-                                      u'maxSynapsesPerSegment': 32,
-                                      u'minThreshold': 10,
-                                      u'newSynapseCount': 20,
-                                      u'outputType': u'normal',
-                                      u'pamLength': 3,
-                                      u'permanenceDec': 0.1,
-                                      u'permanenceInc': 0.1,
-                                      u'seed': 1960,
-                                      u'temporalImp': u'cpp',
-                                      u'verbosity': 0},
-                        u'trainSPNetOnlyIfRequested': False},
-       u'predictAheadTime': None,
-       u'version': 1}
+                        'tmEnable': True,
+                        'tmParams': {'activationThreshold': 13,
+                                      'cellsPerColumn': 32,
+                                      'columnCount': 2048,
+                                      'globalDecay': 0.0,
+                                      'initialPerm': 0.21,
+                                      'inputWidth': 2048,
+                                      'maxAge': 0,
+                                      'maxSegmentsPerCell': 128,
+                                      'maxSynapsesPerSegment': 32,
+                                      'minThreshold': 10,
+                                      'newSynapseCount': 20,
+                                      'outputType': 'normal',
+                                      'pamLength': 3,
+                                      'permanenceDec': 0.1,
+                                      'permanenceInc': 0.1,
+                                      'seed': 1960,
+                                      'temporalImp': 'cpp',
+                                      'verbosity': 0},
+                        'trainSPNetOnlyIfRequested': False},
+       'predictAheadTime': None,
+       'version': 1}
     )
 
-    inferenceArgs = {u'inputPredictedField': u'auto',
-                     u'predictedField': u'c1',
-                     u'predictionSteps': [1]}
+    inferenceArgs = {'inputPredictedField': 'auto',
+                     'predictedField': 'c1',
+                     'predictionSteps': [1]}
 
     data = [
       {'_category': [None],
@@ -164,22 +164,22 @@
        '_sequenceId': 0,
        '_timestamp': datetime.datetime(2013, 12, 5, 0, 0),
        '_timestampRecordIdx': None,
-       u'c0': datetime.datetime(2013, 12, 5, 0, 0),
-       u'c1': 5.0},
+       'c0': datetime.datetime(2013, 12, 5, 0, 0),
+       'c1': 5.0},
       {'_category': [None],
        '_reset': 0,
        '_sequenceId': 0,
        '_timestamp': datetime.datetime(2013, 12, 6, 0, 0),
        '_timestampRecordIdx': None,
-       u'c0': datetime.datetime(2013, 12, 6, 0, 0),
-       u'c1': 6.0},
+       'c0': datetime.datetime(2013, 12, 6, 0, 0),
+       'c1': 6.0},
       {'_category': [None],
        '_reset': 0,
        '_sequenceId': 0,
        '_timestamp': datetime.datetime(2013, 12, 7, 0, 0),
        '_timestampRecordIdx': None,
-       u'c0': datetime.datetime(2013, 12, 7, 0, 0),
-       u'c1': 7.0}
+       'c0': datetime.datetime(2013, 12, 7, 0, 0),
+       'c1': 7.0}
     ]
 
     model = ModelFactory.create(modelConfig=modelConfig)
--- d:\nupic\src\python\python27\tests\unit\nupic\frameworks\opf\opf_metrics_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\frameworks\opf\opf_metrics_test.py	(refactored)
@@ -38,7 +38,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY}))
     gt = [9, 4, 5, 6]
     p = [0, 13, 8, 3]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       rmse.addInstance(gt[i], p[i])
     target = 6.71
 
@@ -51,7 +51,7 @@
                                  {"verbosity" : OPFMetricsTest.VERBOSITY}))
     gt = [9, 4, 5, 6]
     p = [0, 13, 8, 3]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       nrmse.addInstance(gt[i], p[i])
     target = 3.5856858280031814
 
@@ -76,7 +76,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY}))
     gt = [9, 4, 5, 6]
     p = [0, 13, 8, 3]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       aae.addInstance(gt[i], p[i])
     target = 6.0
     self.assertTrue(abs(aae.getMetric()["value"]-target) < OPFMetricsTest.DELTA)
@@ -87,7 +87,7 @@
                 {"verbosity" : OPFMetricsTest.VERBOSITY,"errorMetric":"aae"}))
     gt = [i/4+1 for i in range(100)]
     p = [i for i in range(100)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       trivialaae.addInstance(gt[i], p[i])
     target = .25
     self.assertTrue(abs(trivialaae.getMetric()["value"]-target) \
@@ -99,7 +99,7 @@
                 {"verbosity" : OPFMetricsTest.VERBOSITY,"errorMetric":"acc"}))
     gt = [str(i/4+1) for i in range(100)]
     p = [str(i) for i in range(100)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       trivialaccuracy.addInstance(gt[i], p[i])
     target = .75
     self.assertTrue(abs(trivialaccuracy.getMetric()["value"]-target) \
@@ -112,7 +112,7 @@
             {"verbosity" : OPFMetricsTest.VERBOSITY,"errorMetric":"avg_err"}))
     gt = [str(i/4+1) for i in range(100)]
     p = [str(i) for i in range(100)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       trivialAveErr.addInstance(gt[i], p[i])
     target = .25
     self.assertTrue(abs(trivialAveErr.getMetric()["value"]-target)\
@@ -125,7 +125,7 @@
     {"verbosity" : OPFMetricsTest.VERBOSITY, "window":100,"errorMetric":"aae"}))
     gt = [i/4+1 for i in range(1000)]
     p = [i for i in range(1000)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       trivialaae.addInstance(gt[i], p[i])
     target = .25
     self.assertTrue(abs(trivialaae.getMetric()["value"]-target) \
@@ -138,7 +138,7 @@
  {"verbosity" : OPFMetricsTest.VERBOSITY, "window":100,"errorMetric":"acc"}))
     gt = [str(i/4+1) for i in range(1000)]
     p = [str(i) for i in range(1000)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       trivialaccuracy.addInstance(gt[i], p[i])
     target = .75
     self.assertTrue(abs(trivialaccuracy.getMetric()["value"]-target)\
@@ -151,7 +151,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY, "window":100,"errorMetric":"avg_err"}))
     gt = [str(i/4+1) for i in range(500, 1000)]
     p = [str(i) for i in range(1000)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       trivialAveErr.addInstance(gt[i], p[i])
     target = .25
     self.assertTrue(abs(trivialAveErr.getMetric()["value"]-target)\
@@ -168,7 +168,7 @@
     gt = [i+1 for i in range(100)]
     p = [{3: {i: .7, 5: 0.3}} for i in range(100)]
 
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       msp.addInstance(gt[i], p[i])
     target = 1
     self.assertTrue(abs(msp.getMetric()["value"]-target) < OPFMetricsTest.DELTA)
@@ -186,7 +186,7 @@
     p = [{3: {i+1: .7, 5: 0.3},
           6: {i+0.5: .7, 5: 0.3}} for i in range(100)]
 
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       msp.addInstance(gt[i], p[i])
     target = 0.75  # average of +1 error and 0.5 error
     self.assertTrue(abs(msp.getMetric()["value"]-target) < OPFMetricsTest.DELTA)
@@ -199,7 +199,7 @@
            "steps":3}))
     gt = [5 for i in range(1000)]
     p = [{3: {i: .3, 5: .7}} for i in range(1000)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       msp.addInstance(gt[i], p[i])
     #((999-5)(1000-5)/2-(899-5)(900-5)/2)*.3/100
     target = 283.35
@@ -215,7 +215,7 @@
     gt = [5 for i in range(1000)]
     p = [{3: {i: .3, 5: .7},
           1: {5: 1.0}} for i in range(1000)]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       msp.addInstance(gt[i], p[i])
     #(((999-5)(1000-5)/2-(899-5)(900-5)/2)*.3/100) / 2
     #  / 2 because the 1-step prediction is 100% accurate
@@ -232,7 +232,7 @@
     gt.extend([2*i for i in range(110)])
     p = [i for i in range(1000)]
     res = []
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       movingMeanAAE.addInstance(gt[i], p[i])
       res.append(movingMeanAAE.getMetric()["value"])
     self.assertTrue(max(res[1:890]) == 2.0)
@@ -251,7 +251,7 @@
     gt.extend([2*i for i in range(110)])
     p = [i for i in range(1000)]
     res = []
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       movingMeanRMSE.addInstance(gt[i], p[i])
       res.append(movingMeanRMSE.getMetric()["value"])
     self.assertTrue(max(res[1:890]) == 2.0)
@@ -272,7 +272,7 @@
     gt.extend([2*i/4 for i in range(100)])
     p = [i for i in range(1000)]
     res = []
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       movingModeAvgErr.addInstance(gt[i], p[i])
       res.append(movingModeAvgErr.getMetric()["value"])
     #Make sure that there is no point where the average error is >.5
@@ -298,7 +298,7 @@
     gt.extend([2*i/4 for i in range(100)])
     p = [i for i in range(1000)]
     res = []
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       movingModeACC.addInstance(gt[i], p[i])
       res.append(movingModeACC.getMetric()["value"])
     #Make sure that there is no point where the average acc is <.5
@@ -327,7 +327,7 @@
       encoding[i] = 1
     gt = [i%5 for i in range(1000)]
     res = []
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       if i == 20:
         # Make sure we don"t barf with missing values
         oneGram.addInstance(np.zeros(10), prediction=None,
@@ -356,7 +356,7 @@
       encoding[i] = 1
     gt = [i%5 for i in range(1000)]
     res = []
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       if i == 20:
         # Make sure we don"t barf with missing values
         oneGram.addInstance(np.zeros(10), prediction=None,
@@ -391,7 +391,7 @@
       newElem += 20
 
     res = []
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       if i==20:
         # Make sure we don"t barf with missing values
         oneGram.addInstance(np.zeros(10), prediction=None,
@@ -411,7 +411,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY, "window":1}))
     gt = [9, 4, 5, 6]
     p = [0, 13, 8, 3]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       waae.addInstance(gt[i], p[i])
     target = 3.0
     self.assertTrue( abs(waae.getMetric()["value"]-target) \
@@ -424,7 +424,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY}))
     gt = [0, 1, 2, 3, 4, 5]
     p = [0, 1, 2, 4, 5, 6]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       acc.addInstance(gt[i], p[i])
     target = 0.5
     self.assertTrue(abs(acc.getMetric()["value"]-target) < OPFMetricsTest.DELTA)
@@ -436,7 +436,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY,  "window":2}))
     gt = [0, 1, 2, 3, 4, 5]
     p = [0, 1, 2, 4, 5, 6]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       acc.addInstance(gt[i], p[i])
     target = 0.0
 
@@ -449,7 +449,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY}))
     gt = [1, 1, 2, 3, 4, 5]
     p = [0, 1, 2, 4, 5, 6]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       err.addInstance(gt[i], p[i])
     target = (2.0/3.0)
     self.assertTrue(abs(err.getMetric()["value"]-target) < OPFMetricsTest.DELTA)
@@ -461,7 +461,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY, "window":2}))
     gt = [0, 1, 2, 3, 4, 5]
     p = [0, 1, 2, 4, 5, 6]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       err.addInstance(gt[i], p[i])
     target = 1.0
 
@@ -474,7 +474,7 @@
 {"verbosity" : OPFMetricsTest.VERBOSITY, "window":100}))
     gt = [9, 4, 5, 6]
     p = [0, 13, 8, 3]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       rmse.addInstance(gt[i], p[i])
     target = 6.71
 
@@ -525,7 +525,7 @@
     gt_bucketIdx = [0, 2, 1, 3]
     negLL = getModule(MetricSpec("negativeLogLikelihood", None, None,
                                 {"verbosity" : OPFMetricsTest.VERBOSITY}))
-    for i in xrange(len(bucketLL)):
+    for i in range(len(bucketLL)):
       negLL.addInstance(0, 0, record = None,
                         result=MockModelResult(bucketLL[i], gt_bucketIdx[i]))
     target = 5.756462
@@ -569,7 +569,7 @@
                                 {"verbosity" : OPFMetricsTest.VERBOSITY}))
     negLL_mean = getModule(MetricSpec("negativeLogLikelihood", None, None,
                                 {"verbosity" : OPFMetricsTest.VERBOSITY}))
-    for i in xrange(len(gt_bucketIdx)):
+    for i in range(len(gt_bucketIdx)):
       negLL_gt.addInstance(0, 0, record = None,
                         result=MockModelResult(prediction_gt, gt_bucketIdx[i]))
       negLL_ml.addInstance(0, 0, record = None,
@@ -589,7 +589,7 @@
 {"customFuncSource":customFunc, "errorWindow":3}))
     gt = [9, 4, 5, 6]
     p = [0, 13, 8, 3]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       aggErr = customEM.addInstance(gt[i], p[i])
     target = 5.0
     delta = 0.001
@@ -609,7 +609,7 @@
 {"customFuncSource":customFunc}))
     gt = [9, 4, 5, 6]
     p = [0, 13, 8, 3]
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       customEM.addInstance(gt[i], p[i])
     target = 5.0
     delta = 0.001
@@ -631,7 +631,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if i < lookBack:
           try:
@@ -655,7 +655,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if lookBack>=storeWindow-1:
           pass
@@ -683,7 +683,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if i < lookBack:
           try:
@@ -707,7 +707,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if i < lookBack or lookBack>=storeWindow:
           try:
@@ -733,7 +733,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if i < lookBack:
           try:
@@ -757,7 +757,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if i < lookBack or lookBack>=storeWindow:
           try:
@@ -781,7 +781,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if i < lookBack:
           try:
@@ -806,7 +806,7 @@
       t1 = [3*i for i in range(100)]
       t2 = [str(4*i) for i in range(100)]
 
-      for i in xrange(len(gt)):
+      for i in range(len(gt)):
         curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
         if i < lookBack or lookBack>=storeWindow:
           try:
@@ -830,7 +830,7 @@
     t1 = [3*i for i in range(100)]
     t2 = [str(4*i) for i in range(100)]
 
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
       customEM.addInstance(gt[i], p[i], curRecord)
       self.assertTrue (customEM.getMetric()["value"] == i+1)
@@ -845,7 +845,7 @@
     t1 = [3*i for i in range(100)]
     t2 = [str(4*i) for i in range(100)]
 
-    for i in xrange(len(gt)):
+    for i in range(len(gt)):
       curRecord = {"pred":p[i], "ground":gt[i], "test1":t1[i], "test2":t2[i]}
       customEM.addInstance(gt[i], p[i], curRecord)
       self.assertTrue (customEM.getMetric()["value"] == min(i+1, 4))
@@ -881,10 +881,10 @@
     metric10ref = getModule(ms2)
 
 
-    gt = range(500, 1000)
-    p = range(500)
-
-    for i in xrange(len(gt)):
+    gt = list(range(500, 1000))
+    p = list(range(500))
+
+    for i in range(len(gt)):
       v10=metric10ref.addInstance(gt[i], p[i])
       v1000=metric1000ref.addInstance(gt[i], p[i])
       if v10 is None or v1000 is None:
--- d:\nupic\src\python\python27\tests\unit\nupic\frameworks\opf\two_gram_model_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\frameworks\opf\two_gram_model_test.py	(refactored)
@@ -66,7 +66,7 @@
 
 
   def testSequenceReset(self):
-    encoders = {"a": {"fieldname": u"a",
+    encoders = {"a": {"fieldname": "a",
                       "maxval": 9,
                       "minval": 0,
                       "n": 10,
@@ -94,7 +94,7 @@
 
 
   def testMultipleFields(self):
-    encoders = {"a": {"fieldname": u"a",
+    encoders = {"a": {"fieldname": "a",
                       "maxval": 9,
                       "minval": 0,
                       "n": 10,
@@ -102,7 +102,7 @@
                       "clipInput": True,
                       "forced": True,
                       "type": "ScalarEncoder"},
-                "b": {"fieldname": u"b",
+                "b": {"fieldname": "b",
                       "maxval": 9,
                       "minval": 0,
                       "n": 10,
@@ -127,7 +127,7 @@
 
 
   def testCategoryPredictions(self):
-    encoders = {"a": {"fieldname": u"a",
+    encoders = {"a": {"fieldname": "a",
                       "n": 10,
                       "w": 3,
                       "forced": True,
@@ -149,7 +149,7 @@
 
 
   def testBucketedScalars(self):
-    encoders = {"a": {"fieldname": u"a",
+    encoders = {"a": {"fieldname": "a",
                       "maxval": 9,
                       "minval": 0,
                       "n": 2,
@@ -179,7 +179,7 @@
   @unittest.skipUnless(
     capnp, "pycapnp is not installed, skipping serialization test.")
   def testWriteRead(self):
-    encoders = {"a": {"fieldname": u"a",
+    encoders = {"a": {"fieldname": "a",
                       "maxval": 9,
                       "minval": 0,
                       "n": 10,
@@ -187,7 +187,7 @@
                       "clipInput": True,
                       "forced": True,
                       "type": "ScalarEncoder"},
-                "b": {"fieldname": u"b",
+                "b": {"fieldname": "b",
                       "maxval": 9,
                       "minval": 0,
                       "n": 10,
--- d:\nupic\src\python\python27\tests\unit\nupic\math\lgamma_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\math\lgamma_test.py	(refactored)
@@ -66,7 +66,7 @@
       (3.0,  0.69314718),
     )
     for v, lg in items:
-      print v, lg, lgamma(v)
+      print(v, lg, lgamma(v))
       self.assertLessEqual(abs(lgamma(v) - lg), 1.0e-8,
                            "log Gamma(%f) = %f; lgamma(%f) -> %f" % (
                                v, lg, v, lgamma(v)))
--- d:\nupic\src\python\python27\tests\unit\nupic\math\topology_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\math\topology_test.py	(refactored)
@@ -31,39 +31,39 @@
 class TestTopology(unittest.TestCase):
 
   def testIndexFromCoordinates(self):
-    self.assertEquals(0, indexFromCoordinates((0,), (100,)))
-    self.assertEquals(50, indexFromCoordinates((50,), (100,)))
-    self.assertEquals(99, indexFromCoordinates((99,), (100,)))
-
-    self.assertEquals(0, indexFromCoordinates((0, 0), (100, 80)))
-    self.assertEquals(10, indexFromCoordinates((0, 10), (100, 80)))
-    self.assertEquals(80, indexFromCoordinates((1, 0), (100, 80)))
-    self.assertEquals(90, indexFromCoordinates((1, 10), (100, 80)))
-
-    self.assertEquals(0, indexFromCoordinates((0, 0, 0), (100, 10, 8)))
-    self.assertEquals(7, indexFromCoordinates((0, 0, 7), (100, 10, 8)))
-    self.assertEquals(8, indexFromCoordinates((0, 1, 0), (100, 10, 8)))
-    self.assertEquals(80, indexFromCoordinates((1, 0, 0), (100, 10, 8)))
-    self.assertEquals(88, indexFromCoordinates((1, 1, 0), (100, 10, 8)))
-    self.assertEquals(89, indexFromCoordinates((1, 1, 1), (100, 10, 8)))
+    self.assertEqual(0, indexFromCoordinates((0,), (100,)))
+    self.assertEqual(50, indexFromCoordinates((50,), (100,)))
+    self.assertEqual(99, indexFromCoordinates((99,), (100,)))
+
+    self.assertEqual(0, indexFromCoordinates((0, 0), (100, 80)))
+    self.assertEqual(10, indexFromCoordinates((0, 10), (100, 80)))
+    self.assertEqual(80, indexFromCoordinates((1, 0), (100, 80)))
+    self.assertEqual(90, indexFromCoordinates((1, 10), (100, 80)))
+
+    self.assertEqual(0, indexFromCoordinates((0, 0, 0), (100, 10, 8)))
+    self.assertEqual(7, indexFromCoordinates((0, 0, 7), (100, 10, 8)))
+    self.assertEqual(8, indexFromCoordinates((0, 1, 0), (100, 10, 8)))
+    self.assertEqual(80, indexFromCoordinates((1, 0, 0), (100, 10, 8)))
+    self.assertEqual(88, indexFromCoordinates((1, 1, 0), (100, 10, 8)))
+    self.assertEqual(89, indexFromCoordinates((1, 1, 1), (100, 10, 8)))
 
 
   def testCoordinatesFromIndex(self):
-    self.assertEquals([0], coordinatesFromIndex(0, [100]));
-    self.assertEquals([50], coordinatesFromIndex(50, [100]));
-    self.assertEquals([99], coordinatesFromIndex(99, [100]));
-
-    self.assertEquals([0, 0], coordinatesFromIndex(0, [100, 80]));
-    self.assertEquals([0, 10], coordinatesFromIndex(10, [100, 80]));
-    self.assertEquals([1, 0], coordinatesFromIndex(80, [100, 80]));
-    self.assertEquals([1, 10], coordinatesFromIndex(90, [100, 80]));
-
-    self.assertEquals([0, 0, 0], coordinatesFromIndex(0, [100, 10, 8]));
-    self.assertEquals([0, 0, 7], coordinatesFromIndex(7, [100, 10, 8]));
-    self.assertEquals([0, 1, 0], coordinatesFromIndex(8, [100, 10, 8]));
-    self.assertEquals([1, 0, 0], coordinatesFromIndex(80, [100, 10, 8]));
-    self.assertEquals([1, 1, 0], coordinatesFromIndex(88, [100, 10, 8]));
-    self.assertEquals([1, 1, 1], coordinatesFromIndex(89, [100, 10, 8]));
+    self.assertEqual([0], coordinatesFromIndex(0, [100]));
+    self.assertEqual([50], coordinatesFromIndex(50, [100]));
+    self.assertEqual([99], coordinatesFromIndex(99, [100]));
+
+    self.assertEqual([0, 0], coordinatesFromIndex(0, [100, 80]));
+    self.assertEqual([0, 10], coordinatesFromIndex(10, [100, 80]));
+    self.assertEqual([1, 0], coordinatesFromIndex(80, [100, 80]));
+    self.assertEqual([1, 10], coordinatesFromIndex(90, [100, 80]));
+
+    self.assertEqual([0, 0, 0], coordinatesFromIndex(0, [100, 10, 8]));
+    self.assertEqual([0, 0, 7], coordinatesFromIndex(7, [100, 10, 8]));
+    self.assertEqual([0, 1, 0], coordinatesFromIndex(8, [100, 10, 8]));
+    self.assertEqual([1, 0, 0], coordinatesFromIndex(80, [100, 10, 8]));
+    self.assertEqual([1, 1, 0], coordinatesFromIndex(88, [100, 10, 8]));
+    self.assertEqual([1, 1, 1], coordinatesFromIndex(89, [100, 10, 8]));
 
 
   # ===========================================================================
@@ -80,9 +80,9 @@
                                                  dimensions),
                                     expected):
       numIndices += 1
-      self.assertEquals(index, expectedIndex)
-
-    self.assertEquals(numIndices, len(expected))
+      self.assertEqual(index, expectedIndex)
+
+    self.assertEqual(numIndices, len(expected))
 
 
   def expectNeighborhoodCoords(self, centerCoords, radius, dimensions, expected):
@@ -94,9 +94,9 @@
                                                  dimensions),
                                     expected):
       numIndices += 1
-      self.assertEquals(index, indexFromCoordinates(expectedIndex, dimensions))
-
-    self.assertEquals(numIndices, len(expected))
+      self.assertEqual(index, indexFromCoordinates(expectedIndex, dimensions))
+
+    self.assertEqual(numIndices, len(expected))
 
 
   def testNeighborhoodOfOrigin1D(self):
@@ -214,9 +214,9 @@
                                                          dimensions),
                                     expected):
       numIndices += 1
-      self.assertEquals(index, expectedIndex)
-
-    self.assertEquals(numIndices, len(expected))
+      self.assertEqual(index, expectedIndex)
+
+    self.assertEqual(numIndices, len(expected))
 
 
   def expectWrappingNeighborhoodCoords(self, centerCoords, radius, dimensions,
@@ -229,9 +229,9 @@
                                                          dimensions),
                                     expected):
       numIndices += 1
-      self.assertEquals(index, indexFromCoordinates(expectedIndex, dimensions))
-
-    self.assertEquals(numIndices, len(expected))
+      self.assertEqual(index, indexFromCoordinates(expectedIndex, dimensions))
+
+    self.assertEqual(numIndices, len(expected))
 
 
   def testWrappingNeighborhoodOfOrigin1D(self):
--- d:\nupic\src\python\python27\tests\unit\nupic\regions\anomaly_likelihood_region_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\regions\anomaly_likelihood_region_test.py	(refactored)
@@ -67,7 +67,7 @@
     outputs = AnomalyLikelihoodRegion.getSpec()['outputs']
     with open (_INPUT_DATA_FILE) as f:
       reader = csv.reader(f)
-      reader.next()
+      next(reader)
       for record in reader:
         consumption = float(record[1])
         anomalyScore = float(record[2])
@@ -90,7 +90,7 @@
     inputs = AnomalyLikelihoodRegion.getSpec()['inputs']
     outputs = AnomalyLikelihoodRegion.getSpec()['outputs']
 
-    for _ in xrange(0, 6):
+    for _ in range(0, 6):
       inputs['rawAnomalyScore'] = numpy.array([random.random()])
       inputs['metricValue'] = numpy.array([random.random()])
       anomalyLikelihoodRegion1.compute(inputs, outputs)
@@ -110,7 +110,7 @@
     anomalyLikelihoodRegion2 = AnomalyLikelihoodRegion.read(proto2)
     self.assertEqual(anomalyLikelihoodRegion1, anomalyLikelihoodRegion2)
 
-    for _ in xrange(6, 500):
+    for _ in range(6, 500):
       inputs['rawAnomalyScore'] = numpy.array([random.random()])
       inputs['metricValue'] = numpy.array([random.random()])
       anomalyLikelihoodRegion1.compute(inputs, outputs)
--- d:\nupic\src\python\python27\tests\unit\nupic\regions\anomaly_region_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\regions\anomaly_region_test.py	(refactored)
@@ -68,7 +68,7 @@
     anomalyRegion1 = AnomalyRegion()
     inputs = AnomalyRegion.getSpec()['inputs']
     outputs = AnomalyRegion.getSpec()['outputs']
-    for i in xrange(0, 6):
+    for i in range(0, 6):
       inputs['predictedColumns'] = numpy.array(predictedColumns[i])
       inputs['activeColumns'] = numpy.array(activeColumns[i])
       anomalyRegion1.compute(inputs, outputs)
@@ -87,7 +87,7 @@
 
     self.assertEqual(anomalyRegion1, anomalyRegion2)
 
-    for i in xrange(6, 10):
+    for i in range(6, 10):
       inputs['predictedColumns'] = numpy.array(predictedColumns[i])
       inputs['activeColumns'] = numpy.array(activeColumns[i])
       anomalyRegion1.compute(inputs, outputs)
--- d:\nupic\src\python\python27\tests\unit\nupic\regions\knn_anomaly_classifier_region_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\regions\knn_anomaly_classifier_region_test.py	(refactored)
@@ -839,13 +839,13 @@
 
     self.maxDiff = None
     records = []
-    for i in xrange(self.helper.trainRecords):
+    for i in range(self.helper.trainRecords):
       spBottomUpOut = numpy.zeros(1000)
       tpTopDownOut = numpy.zeros(1000)
       tpLrnActiveStateT = numpy.zeros(1000)
-      spBottomUpOut[random.sample(xrange(1000), 20)] = 1
-      tpTopDownOut[random.sample(xrange(1000), 20)] = 1
-      tpLrnActiveStateT[random.sample(xrange(1000), 20)] = 1
+      spBottomUpOut[random.sample(range(1000), 20)] = 1
+      tpTopDownOut[random.sample(range(1000), 20)] = 1
+      tpLrnActiveStateT[random.sample(range(1000), 20)] = 1
       records.append({
         'spBottomUpOut': spBottomUpOut,
         'tpTopDownOut': tpTopDownOut,
@@ -853,10 +853,10 @@
       })
 
     self.helper.setParameter('anomalyThreshold', None, 0.5)
-    for i in xrange(self.helper.trainRecords):
+    for i in range(self.helper.trainRecords):
       self.helper.compute(records[i], None)
 
-    for _ in xrange(10):
+    for _ in range(10):
       self.helper.compute(random.choice(records), None)
 
     proto = KNNAnomalyClassifierRegionProto.new_message()
@@ -870,33 +870,33 @@
     knnDeserialized = KNNAnomalyClassifierRegion.readFromProto(
       protoDeserialized)
 
-    self.assertEquals(self.helper._maxLabelOutputs,
+    self.assertEqual(self.helper._maxLabelOutputs,
                       knnDeserialized._maxLabelOutputs)
-    self.assertEquals(self.helper._activeColumnCount,
+    self.assertEqual(self.helper._activeColumnCount,
                       knnDeserialized._activeColumnCount)
     self.assertTrue((self.helper._prevPredictedColumns ==
                              knnDeserialized._prevPredictedColumns).all())
-    self.assertEquals(self.helper._anomalyVectorLength,
+    self.assertEqual(self.helper._anomalyVectorLength,
                       knnDeserialized._anomalyVectorLength)
-    self.assertAlmostEquals(self.helper._classificationMaxDist,
+    self.assertAlmostEqual(self.helper._classificationMaxDist,
                       knnDeserialized._classificationMaxDist)
-    self.assertEquals(self.helper._iteration, knnDeserialized._iteration)
-    self.assertEquals(self.helper.trainRecords, knnDeserialized.trainRecords)
-    self.assertEquals(self.helper.anomalyThreshold,
+    self.assertEqual(self.helper._iteration, knnDeserialized._iteration)
+    self.assertEqual(self.helper.trainRecords, knnDeserialized.trainRecords)
+    self.assertEqual(self.helper.anomalyThreshold,
                       knnDeserialized.anomalyThreshold)
-    self.assertEquals(self.helper.cacheSize, knnDeserialized.cacheSize)
-    self.assertEquals(self.helper.classificationVectorType,
+    self.assertEqual(self.helper.cacheSize, knnDeserialized.cacheSize)
+    self.assertEqual(self.helper.classificationVectorType,
                       knnDeserialized.classificationVectorType)
     self.assertListEqual(self.helper.getLabelResults(),
                          knnDeserialized.getLabelResults())
 
     for i, expected in enumerate(self.helper._recordsCache):
       actual = knnDeserialized._recordsCache[i]
-      self.assertEquals(expected.ROWID, actual.ROWID)
-      self.assertAlmostEquals(expected.anomalyScore, actual.anomalyScore)
+      self.assertEqual(expected.ROWID, actual.ROWID)
+      self.assertAlmostEqual(expected.anomalyScore, actual.anomalyScore)
       self.assertListEqual(expected.anomalyVector, actual.anomalyVector)
       self.assertListEqual(expected.anomalyLabel, actual.anomalyLabel)
-      self.assertEquals(expected.setByUser, actual.setByUser)
+      self.assertEqual(expected.setByUser, actual.setByUser)
 
 
 if __name__ == '__main__':
--- d:\nupic\src\python\python27\tests\unit\nupic\regions\record_sensor_region_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\regions\record_sensor_region_test.py	(refactored)
@@ -110,14 +110,14 @@
     network = _createNetwork()
     network.run(1)  # Process 1 row of data
     bucketIdxOut = network.regions['sensor'].getOutputData('bucketIdxOut')[0]
-    self.assertEquals(bucketIdxOut, 500)
+    self.assertEqual(bucketIdxOut, 500)
 
 
   def testActValueOut(self):
     network = _createNetwork()
     network.run(1)  # Process 1 row of data
     actValueOut = network.regions['sensor'].getOutputData('actValueOut')[0]
-    self.assertEquals(round(actValueOut, 1), 21.2)  # only 1 precision digit
+    self.assertEqual(round(actValueOut, 1), 21.2)  # only 1 precision digit
 
 
 
--- d:\nupic\src\python\python27\tests\unit\nupic\regions\sdr_classifier_region_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\regions\sdr_classifier_region_test.py	(refactored)
@@ -88,13 +88,13 @@
   def testActValueIn(self):
     self.network.run(1)  # Process 1 row of data
     actValueIn = self.classifierRegion.getInputData('actValueIn')[0]
-    self.assertEquals(round(actValueIn, 1), 21.2)  # only 1 precision digit
+    self.assertEqual(round(actValueIn, 1), 21.2)  # only 1 precision digit
 
 
   def testBucketIdxIn(self):
     self.network.run(1)  # Process 1 row of data
     bucketIdxIn = self.classifierRegion.getInputData('bucketIdxIn')[0]
-    self.assertEquals(bucketIdxIn, 500)
+    self.assertEqual(bucketIdxIn, 500)
 
 
 
--- d:\nupic\src\python\python27\tests\unit\nupic\support\configuration_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\support\configuration_test.py	(refactored)
@@ -21,7 +21,7 @@
 
 import os
 import shutil
-from StringIO import StringIO
+from io import StringIO
 import sys
 import tempfile
 import unittest2 as unittest
--- d:\nupic\src\python\python27\tests\unit\nupic\support\custom_configuration_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\support\custom_configuration_test.py	(refactored)
@@ -22,7 +22,7 @@
 from copy import copy
 import os
 import shutil
-from StringIO import StringIO
+from io import StringIO
 import sys
 import tempfile
 import unittest2 as unittest
@@ -41,7 +41,7 @@
 import nupic
 import nupic.support.configuration_custom as configuration
 
-import configuration_test
+from . import configuration_test
 
 
 
@@ -219,7 +219,7 @@
     environ.get.return_value = None
     findConfigFile.side_effect = self.files.get
     configuration.Configuration.clear()
-    paramNames = configuration.Configuration.dict().keys()
+    paramNames = list(configuration.Configuration.dict().keys())
     customValue = 'NewValue'
     with open(self.files['nupic-custom.xml'], 'w') as fp:
       fp.write('\n'.join((
--- d:\nupic\src\python\python27\tests\unit\nupic\support\group_by_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\support\group_by_test.py	(refactored)
@@ -37,7 +37,7 @@
     i = 0
     for data in groupby2(sequence0, identity):
       self.assertEqual(data[0], expectedValues[i][0])
-      for j in xrange(1, len(data)):
+      for j in range(1, len(data)):
         temp = list(data[j]) if data[j] else data[j]
         self.assertEqual(temp, expectedValues[i][j])
       i += 1
@@ -60,7 +60,7 @@
     for data in groupby2(sequence0, identity,
                          sequence1, times3):
       self.assertEqual(data[0], expectedValues[i][0])
-      for j in xrange(1, len(data)):
+      for j in range(1, len(data)):
         temp = list(data[j]) if data[j] else data[j]
         self.assertEqual(temp, expectedValues[i][j])
       i += 1
@@ -87,7 +87,7 @@
                          sequence1, times3,
                          sequence2, times4):
       self.assertEqual(data[0], expectedValues[i][0])
-      for j in xrange(1, len(data)):
+      for j in range(1, len(data)):
         temp = list(data[j]) if data[j] else data[j]
         self.assertEqual(temp, expectedValues[i][j])
       i += 1
@@ -118,7 +118,7 @@
                          sequence2, times4,
                          sequence3, times5):
       self.assertEqual(data[0], expectedValues[i][0])
-      for j in xrange(1, len(data)):
+      for j in range(1, len(data)):
         temp = list(data[j]) if data[j] else data[j]
         self.assertEqual(temp, expectedValues[i][j])
       i += 1
@@ -154,7 +154,7 @@
                          sequence3, times5,
                          sequence4, times6):
       self.assertEqual(data[0], expectedValues[i][0])
-      for j in xrange(1, len(data)):
+      for j in range(1, len(data)):
         temp = list(data[j]) if data[j] else data[j]
         self.assertEqual(temp, expectedValues[i][j])
       i += 1
@@ -186,7 +186,7 @@
                          sequence3, times5,
                          sequence4, times6):
       self.assertEqual(data[0], expectedValues[i][0])
-      for j in xrange(1, len(data)):
+      for j in range(1, len(data)):
         temp = list(data[j]) if data[j] else data[j]
         self.assertEqual(temp, expectedValues[i][j])
       i += 1
--- d:\nupic\src\python\python27\tests\unit\nupic\support\object_json_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\support\object_json_test.py	(refactored)
@@ -22,7 +22,7 @@
 """Unit tests for object_json module."""
 
 import datetime
-import StringIO
+import io
 
 from nupic.data.inference_shifter import InferenceShifter
 from nupic.swarming.hypersearch import object_json as json
@@ -42,8 +42,8 @@
     self.assertEqual(json.loads(json.dumps(5)), 5)
     self.assertEqual(json.loads(json.dumps(7.7)), 7.7)
     self.assertEqual(json.loads(json.dumps('hello')), 'hello')
-    self.assertEqual(json.loads(json.dumps(5L)), 5L)
-    self.assertEqual(json.loads(json.dumps(u'hello')), u'hello')
+    self.assertEqual(json.loads(json.dumps(5)), 5)
+    self.assertEqual(json.loads(json.dumps('hello')), 'hello')
     self.assertEqual(json.loads(json.dumps([5, 6, 7])), [5, 6, 7])
     self.assertEqual(json.loads(json.dumps({'5': 6, '7': 8})), {'5': 6, '7': 8})
 
@@ -89,7 +89,7 @@
 
   def testDump(self):
     d = {'a': 1, 'b': {'c': 2}}
-    f = StringIO.StringIO()
+    f = io.StringIO()
     json.dump(d, f)
     self.assertEqual(f.getvalue(), '{"a": 1, "b": {"c": 2}}')
 
@@ -104,7 +104,7 @@
     self.assertDictEqual(d, {'a': 1, 'b': {'c': 2}})
 
   def testLoad(self):
-    f = StringIO.StringIO('{"a": 1, "b": {"c": 2}}')
+    f = io.StringIO('{"a": 1, "b": {"c": 2}}')
     d = json.load(f)
     self.assertDictEqual(d, {'a': 1, 'b': {'c': 2}})
 
@@ -137,8 +137,8 @@
     decoded = json.loads(encoded)
     self.assertEqual(decoded.a, 5)
     self.assertEqual(type(decoded.b), dict)
-    self.assertEqual(len(decoded.b.keys()), 1)
-    self.assertTupleEqual(decoded.b.keys()[0], (4, 5))
+    self.assertEqual(len(list(decoded.b.keys())), 1)
+    self.assertTupleEqual(list(decoded.b.keys())[0], (4, 5))
     self.assertTupleEqual(decoded.b[(4, 5)], (17,))
 
 
--- d:\nupic\src\python\python27\tests\unit\nupic\support\consoleprinter_test\consoleprinter_test.py	(original)
+++ d:\nupic\src\python\python27\tests\unit\nupic\support\consoleprinter_test\consoleprinter_test.py	(refactored)
@@ -36,7 +36,7 @@
 
 
   def run(self):
-    for i in xrange(0, 4):
+    for i in range(0, 4):
       self.cPrint(i, "message at level %d", i)
 
 
@@ -54,19 +54,19 @@
     # Capture output to a file so that we can compare it
     with Tee(filename):
       c1 = MyClass()
-      print "Running with default verbosity"
+      print("Running with default verbosity")
       c1.run()
-      print
+      print()
 
-      print "Running with verbosity 2"
+      print("Running with verbosity 2")
       c1.consolePrinterVerbosity = 2
       c1.run()
-      print
+      print()
 
-      print "Running with verbosity 0"
+      print("Running with verbosity 0")
       c1.consolePrinterVerbosity = 0
       c1.run()
-      print
+      print()
 
       c1.cPrint(0, "Message %s two %s", "with", "args")
       c1.cPrint(0, "Message with no newline", newline=False)
@@ -75,7 +75,7 @@
                 "no newline", "args", newline=False)
       c1.cPrint(0, " Message with %s and %s", "newline", "args")
 
-      print "Done"
+      print("Done")
 
     with self.assertRaises(KeyError):
       c1.cPrint(0, "Message", badkw="badvalue")
@@ -84,12 +84,12 @@
     expected = open(referenceFilename).readlines()
     actual = open(filename).readlines()
 
-    print ("Comparing files '%s'" % referenceFilename)
-    print ("and             '%s'" % filename)
+    print(("Comparing files '%s'" % referenceFilename))
+    print(("and             '%s'" % filename))
 
     self.assertEqual(len(expected), len(actual))
 
-    for i in xrange(len(expected)):
+    for i in range(len(expected)):
       self.assertEqual(expected[i].strip(), actual[i].strip())
 
     # Clean up
